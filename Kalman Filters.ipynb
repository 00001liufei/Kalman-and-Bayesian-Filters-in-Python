{
 "metadata": {
  "name": "",
  "signature": "sha256:3a7b44bd3874a8149fcf6a20285e8a0855662094c0cb4dbb11f3ec3a2e1e7a3c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Kalman Filters\n",
      "\n",
      "\n",
      "Now that we understand the histogram filter and Gaussians we are prepared to implement a 1D Kalman filter. We will do this exactly as we did the histogram filter - rather than going into the theory we will just develop the code step by step. \n",
      "\n",
      "#Tracking A Dog\n",
      "\n",
      "As in the histogram chapter we will be tracking a dog in a long hallway at work. However, in our latest hackathon someone created an RFID tracker that provides a reasonable accurate position for our dog. Suppose the hallway is 100m long. The sensor returns the distance of the dog from the left end of the hallway. So, 23.4 would mean the dog is 23.4 meters from the left end of the hallway.\n",
      "\n",
      "Naturally, the sensor is not perfect. A reading of 23.4 could correspond to a real position of 23.7, or 23.0. However, it is very unlikely to correspond to a real position of say 47.6. Testing during the hackathon confirmed this result - the sensor is accurate, and while it had errors, the errors are small.\n",
      "\n",
      "Implementing and/or robustly modelling an RFID system is beyond the scope of this book, so we will write a very simple model. We will start with a simulation of the dog moving from left to right at a constant speed with some random noise added. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function, division\n",
      "\n",
      "import numpy.random as random\n",
      "import math\n",
      "\n",
      "class DogSensor(object):\n",
      "    \n",
      "    def __init__(self, x0=0, velocity=1, noise=0.0):\n",
      "        \"\"\" x0 - initial position\n",
      "            velocity - (+=right, -=left)\n",
      "            noise - scaling factor for noise, 0== no noise\n",
      "        \"\"\"\n",
      "        self.x = x0\n",
      "        self.velocity = velocity\n",
      "        self.noise = math.sqrt(noise)\n",
      "\n",
      "    def sense(self):\n",
      "        self.x = self.x + self.velocity\n",
      "        return self.x + random.randn() * self.noise\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The constructor (__init__()) initializes the DogSensor class with an initial position (x0), velocity (vel), and an noise scaling factor. The *sense()* function has the dog move by the set velocity and returns its new position, with noise added. If you look at the code for *sense()* you will see a call to *numpy.random.randn()*. This returns a number sampled from a normal distribution with a mean of 0.0. Let's look at some example output for that.\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function\n",
      "for i in range(20):\n",
      "    print(\"%.4f\" % random.randn(),end='\\t')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You should see a sequence of numbers near 0, some negative and some positive. Most are probably between -1 and 1, but a few might lie somewhat outside that range. This is what we expect from a normal distribution - values are clustered around the mean, and there are fewer values the further you get from the mean.\n",
      "\n",
      "Okay, so lets look at the output of the *DogSensor* class. We will start by setting the noise to 0 to check that the class does what we think it does"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.pylab as pylab\n",
      "pylab.rcParams['figure.figsize'] = 10,6\n",
      "\n",
      "dog = DogSensor (noise=0.0)\n",
      "xs = []\n",
      "for i in range(10):\n",
      "    x = dog.sense()\n",
      "    xs.append(x)\n",
      "    print(\"%.4f\" % x, end=' '),\n",
      "plt.plot(xs)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The constructor initialized the dog at position 0 with a velocity of 1 (move 1.0 to the right). So we would expect to see an output of 1..10, and indeed that is what we see. If you thought the correct answer should have been 0..9 recall that *sense()* returns the dog's position *after* updating his position, so the first postion is 0.0 + 1, or 1.0.\n",
      "\n",
      "Now let's inject some noise in the signal."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_sensor(noise_scale):\n",
      "    dog = DogSensor(noise=noise_scale)\n",
      "\n",
      "    xs = []\n",
      "    for i in range(100):\n",
      "        x = dog.sense()\n",
      "        xs.append(x)\n",
      "    p1, = plt.plot(xs, c='b')\n",
      "    p2, = plt.plot([0,99],[1,100], 'r--')\n",
      "    plt.xlabel('time')\n",
      "    plt.ylabel('pos')\n",
      "    plt.ylim([0,100])\n",
      "    plt.title('noise = ' + str(noise_scale))\n",
      "    plt.legend([p1, p2], ['sensor', 'actual'], loc=2)\n",
      "    plt.show()\n",
      "    \n",
      "test_sensor(4.0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> **Note**: numpy uses a random number generator to generate the normal distribution samples. The numbers I see as I write this are unlikely to be the ones that you see. If you run the cell above multiple times, you should get a slightly different result each time. I could use *numpy.random.seed(some_value)* to force the results to be the same each time. This would simplify my explanations in some cases, but would ruin the interactive nature of this chapter. To get a real feel for how normal distributions and Kalman filters work you will probably want to run cells several times, observing what changes, and what stays roughly the same.\n",
      "\n",
      "So the output of the sensor should be a wavering blue line drawn over a dotted red line. The dotted red line shows the actual position of the dog, and the blue line is the noise signal produced by the simulated RFID sensor. Please note that the red dotted line was manually plotted - we do not yet have a filter that recovers that information! \n",
      "\n",
      "If you are running this in an interactive IPython Notebook, I strongly urge you to run the script several times in a row. You can do this by putting the cursor in the cell containing the Python code and pressing Ctrl+Enter. Each time it runs you should see a different jagged blue line wavering over the top of the dotted red line.\n",
      "\n",
      "I also urge you to adjust the noise setting to see the result of various values. However, since you may be reading this in a read only notebook, I will show two extreme examples. The first plot shows the noise set to 100.0, and the second shows noise set to 0.5."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_sensor(100.0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_sensor(0.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You may not have a full understanding of the exact *meaning* of a noise value of 100.0, but as it turns out if you multiply *randn()* with a number $n$, the result is just a normal distribution with $\\sigma = \\sqrt{n}$. So the example with noise = 100 is using the normal distribution  $\\mathcal{N}(0,100)$. Recall the notation for a normal distribution is $\\mathcal{N}(\\mu,\\sigma^2)$. If the square root is confusing, recall that normal distributions use $\\sigma^2$ for the variance, and $\\sigma$ is the standard deviation, which we do not use in this book. *dog_sensor.__init__()* takes the square root of the noise setting so that the *noise &ast; randn()* call properly computes the normal distribution. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Math with Gaussians\n",
      "\n",
      "Let's say we believe that our dog is at 23m, and the variance is 5, or $pos_{dog}=\\mathcal{N}(23,5)$). We can represent that in a plot:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import stats\n",
      "stats.norm_plot(23, 5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This corresponds to a fairly inexact belief. While we believe that the dog is at 23, note that roughly 21 to 25 are quite likely as well. Let's assume for the moment our dog is standing still, and we query the sensor again. This time it returns 23.2 as the position. Can we use this additional information to improve our estimate of the dog's position?\n",
      "\n",
      "Intuition suggests 'yes'. Consider: if we read the sensor 100 times and each time it returned a value between 21 and 25, all centered around 23, we should be very confident that the dog is somewhere very near 23. Of course, a different physical interpertation is possible. Perhaps our dog was randomly wandering back and forth in a way that exactly emulated a normal distribution. But that seems extremely unlikely - I certainly have never seen a dog do that. So the only reasonable assumption is that the dog was mostly standing still at 23.0.\n",
      "\n",
      "Let's look at 100 sensor readings in a plot:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dog = DogSensor(23, 0, 5)\n",
      "xs = range(100)\n",
      "ys = []\n",
      "for i in xs:\n",
      "    ys.append(dog.sense())\n",
      "               \n",
      "plt.plot(xs,ys)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Eyeballing this confirms our intuition - no dog moves like this. However, noisy sensor data certainly looks like this. So let's proceed and try to solve this mathematically. But how?\n",
      "\n",
      "\n",
      "Recall the histogram code for adding a measurement to a pre-existing belief:\n",
      "\n",
      "    def sense(pos, measure, p_hit, p_miss):\n",
      "        q = array(pos, dtype=float)\n",
      "        for i in range(len(hallway)):\n",
      "            if hallway[i] == measure:\n",
      "                q[i] = pos[i] * p_hit\n",
      "            else:\n",
      "                q[i] = pos[i] * p_miss\n",
      "         normalize(q)\n",
      "         return q\n",
      "         \n",
      "Note that the algorithm is essentially computing:\n",
      "\n",
      "    new_belief = old_belief * measurement * sensor_error\n",
      "    \n",
      "The measurement term might not be obvious, but recall that measurement in this case was always 1 or 0, and so it was left out for convience. \n",
      "    \n",
      "If we are implementing this with gaussians, we might expect it to be implemented as:\n",
      "\n",
      "    new_gaussian = measurement * old_gaussian\n",
      "    \n",
      "where measurement is a Gaussian returned from the sensor. But does that make sense? Can we multiply gaussians? If we multiply a Gaussian with a Gaussian is the result another Gaussian, or something else?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is not particularly difficult to perform the algebra to derive the equation for multiplying two gaussians, but I will just present the result:\n",
      "$$\n",
      "N(\\mu_1, \\sigma_1^2)*N(\\mu_2, \\sigma_2^2) = N(\\frac{\\sigma_1^2 \\mu_2 + \\sigma_2^2 \\mu_1}{\\sigma_1^2 + \\sigma_2^2},\\frac{1}{\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}}) $$ \n",
      "\n",
      "In other words the result is a Gaussian with \n",
      "\n",
      "$$\\begin{align*}\n",
      "\\mu &=\\frac{\\sigma_1^2 \\mu_2 + \\sigma_2^2 \\mu_1} {\\sigma_1^2 + \\sigma_2^2}, \\\\\n",
      "\\sigma &= \\frac{1}{\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}}\n",
      "\\end{align*}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Without doing a deep analysis we can immediately infer some things. First and most importantly the result of multiplying two Gaussians is another Gaussian. The expression for the mean is not particularly illuminating, except that it is a combination of the means and variances of the input. But the variance of the result is merely some combination of the variances of the variances of the input. We conclude from this that the variances are completely unaffected by the values of the mean!\n",
      "\n",
      "Let's immediately look at some plots of this. First, let's look at the result of multiplying $N(23,5)$ to itself. This corresponds to getting 23.0 as the sensor value twice in a row. But before you look at the result, what do you think the result will look like? What should the new mean be? Will the variance by wider, narrower, or the same?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "def multiply(mu1, sig1, mu2, sig2):\n",
      "    m = (sig1*mu2 + sig2*mu1) / (sig1+sig2)\n",
      "    s = 1. / (1./sig1 + 1./ sig2)\n",
      "    return (m,s)\n",
      "\n",
      "xs = np.arange(16, 30, 0.1)\n",
      "\n",
      "\n",
      "m1,s1 = 23, 5\n",
      "m, s = multiply(m1,s1,m1,s1)\n",
      "\n",
      "ys = [stats.gaussian(x,m1,s1) for x in xs]\n",
      "p1, = plt.plot (xs,ys)\n",
      "\n",
      "ys = [stats.gaussian(x,m,s) for x in xs]\n",
      "p2, = plt.plot (xs,ys)\n",
      "\n",
      "plt.legend([p1,p2],['original', 'multiply'])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The result is either amazing or what you would expect, depending on your state of mind. I must admit I vacillate freely between the two! Note that the result of the multiplation is taller and narrow than the original Gaussian but the mean is the same. Does this match your intuition of what the result should have been?\n",
      "\n",
      "If we think of the Gaussians as two measurements, this makes sense. If I measure twice and get the same value, I should be more confident in my answer than if I just measured once. If I measure twice and get $23m$ each time, I should conclude that the length is close to $23m$. So the mean should be $23$. I am more confident with two measurements than with one, so the variance of the result should be smaller. \n",
      "\n",
      "\"Measure twice, cut once\" is a useful saying and practice due to this fact!  The Gaussian is just a mathematical model of this physical fact, so we should expect the math to follow our physical process. \n",
      "\n",
      "Now let's multiply two gaussians (or equivelently, two measurements) that are partially separated. In other words, their means will be different, but their variances will be the same. What do you think the result will be? Think about it, and then look at the graph."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xs = np.arange(16, 30, 0.1)\n",
      "\n",
      "m1,s1 = 23, 5\n",
      "m2,s2 = 25, 5\n",
      "m, s = multiply(m1,s1,m2,s2)\n",
      "\n",
      "ys = [stats.gaussian(x,m1,s1) for x in xs]\n",
      "p1, = plt.plot (xs,ys)\n",
      "\n",
      "ys = [stats.gaussian(x,m2,s2) for x in xs]\n",
      "p2, = plt.plot (xs,ys)\n",
      "\n",
      "ys = [stats.gaussian(x,m,s) for x in xs]\n",
      "p3, = plt.plot(xs,ys)\n",
      "plt.legend([p1,p2,p3],['measure 1', 'measure 2', 'multiply'])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another beautiful result! If I handed you a measuring tape and asked you to measure the distance from table to a wall, and you got 23m, and then a friend make the same measurement and got 25m, your best guess must be 24m. \n",
      "\n",
      "That is fairly counter-intuitive, so let's consider it further. Perhaps a more reasonable assumption would be that either you or your coworker just made a mistake, and the true distance is either 23 or 25, but certainly not 24. Surely that is possible. However, suppose the two measurements you reported as 24.01 and 23.99. In that case you would agree that in this case the best guess for the correct value is 24?  Which interpretation we choose depends on the properties of the sensors we are using. Humans make galling mistakes, physical sensors do not. \n",
      "\n",
      "This topic is fairly deep, and I will explore it once we have completed our Kalman filter. For now I will merely say that the Kalman filter requires the interpretation that measurements are accurate, with Gaussian noise, and that a large error caused by misreading a measuring tape is not Gaussian noise.\n",
      "\n",
      "For now I ask that you trust me. The math is correct, so we have no choice but to accept it and use it. We will see how the Kalman filter deals with movements vs error very soon. In the meantime, accept that 24 is the correct answer to this problem.\n",
      "\n",
      "One final test of your intuition. What if the two measurements are widely separated? "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xs = np.arange(0, 60, 0.1)\n",
      "\n",
      "m1,s1 = 10, 5\n",
      "m2,s2 = 50, 5\n",
      "m, s = multiply(m1,s1,m2,s2)\n",
      "\n",
      "ys = [stats.gaussian(x,m1,s1) for x in xs]\n",
      "p1, = plt.plot (xs,ys)\n",
      "\n",
      "ys = [stats.gaussian(x,m2,s2) for x in xs]\n",
      "p2, = plt.plot (xs,ys)\n",
      "\n",
      "ys = [stats.gaussian(x,m,s) for x in xs]\n",
      "p3, = plt.plot(xs,ys)\n",
      "plt.legend([p1,p2,p3],['measure 1', 'measure 2', 'multiply'])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This result bothered me quite a bit when I first learned it. If my first measurement was 10, and the next one was 50, why would I choose 30 as a result? And why would I be *more* confident? Doesn't it make sense that either one of the measurements is wrong, or that I am measuring a moving object? Shouldn't the result be nearer 50? And, shouldn't the variance be larger, not smaller?\n",
      "\n",
      "Well, no. It will become clearer soon, but recall our discrete Bayesian filter. It had two steps: *sense*, which incorporated the new measurement, and then *update*, which incorporated the movement. In the chart above we don't have any movement information. For now, trust me that very shortly we will learn how to incorporate that information. In the meantime, reflect on the fact that if we have 2 measurements with known variance, this is the only possible result, because we do not have any information (yet) as to whether the object is moving or that one sensor might be malfunctioning. As with the discrete Bayes filter, this result just reflects our current knowledge, and this is all we know. Trust the math!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Implementing the Sensing Step\n",
      "\n",
      "Recall the histogram filter uses a numpy array to encode our belief about the position of our dog at any time. That array stored our belief of our dog's position in the hallway using 10 discrete positions. This was very crude, because with a 100m hallway that corresponded to positions 10m apart. It would have been trivial to expand the number of positions to say 1,000, and that is what we would do if using it for a real problem. But the problem remains that the distribution is discrete and multimodal - it can express strong belief that the dog is in two positions at the same time.\n",
      "\n",
      "Therefore, we will use a single Gaussian to reflect our current belief of the dog's position. In other words, we will use $dog_{pos} = \\mathcal{N}(\\mu,\\sigma^2)$. Gaussians extend to infinity on both sides of the mean, so the single Gaussian will cover the entire hallway. They are unimodal, and seem to reflect the behavior of real-world sensors - most errors are small and clustered around the mean. Here is the entire implementation of the sense function for a Kalman filter:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sense(mu, sigma, measurement, measurement_sigma):\n",
      "    return multiply(mu, sigma, measurement, measurement_sigma)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Kalman filters are supposed to be hard! But this is very short and straightforward. All we are doing is multiplying the Gaussian that reflects our belief of where the dog was with the new measurement. Perhaps this would be clearer if we used more specific names:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sense_dog(dog_pos, dog_sigma, measurement, measurement_sigma):\n",
      "    return multiply(dog_pos, dog_sigma, measurement, measurement_sigma)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That is less abstract, which perhaps helps with comprehension, but it is poor coding practice. We are writing a Kalman filter that works for any problem, not just tracking dogs in a hallway, so we don't use variable names with 'dog' in them. Still, the *sense_dog()* function should make what we are doing very clear. \n",
      "\n",
      "Let's look at an example. We will suppose that our current belief for the dog's position is $N(2,5)$. Don't worry about where that number came from. It may appear that we have a chicken and egg problem, in that how do we know the position before we sense it, but we will resolve that shortly. We will create a *DogSensor* object initialized to be at position 0.0, and with no velocity, and modest noise. This corresponds to the dog standing still at the far left side of the hallway. Note that we mistakenly believe the dog is at postion 2.0, not 0.0."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dog = DogSensor(velocity=0, noise=1)\n",
      "\n",
      "pos,s = 2, 5\n",
      "for i in range(20):\n",
      "    pos,s = sense(pos, s, dog.sense(), 5)\n",
      "    print('time:', i, '\\tposition =', \"%.3f\" % pos, '\\tvariance =', \"%.3f\" % s)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Because of the random numbers I do not know the exact values that you see, but the position should have converged very quickly to almost 0 despite the initial error of believing that the position was 2.0. Furthermore, the variance should have quickly converged from the intial value of 5.0 to 0.238.\n",
      "\n",
      "By now the fact that we converged to a position of 0.0 should not be terribly suprising. All we are doing is computing $new\\_position = old\\_position * measurement$, and the measurement is a normal distribution around 0, so we should get very close to 0 after 20 iterations. But the truly amazing part of this code is how the variance became 0.238 despite every measurement having a variance of 5.0. \n",
      "\n",
      "If we think about the physical interpretation of this is should be clear that this is what should happen. If you sent 20 people into the hall with a tape measure to physically measure the position of the dog you would be very confident in the result after 20 measurements - more confident than after 1 or 2 measurements. So it makes sense that as we make more measurements the variance gets smaller.\n",
      "\n",
      "Mathematically it makes sense as well. Recall the computation for the variance after the multiplication: $\\sigma^2 = 1/(\\frac{1}{{\\sigma}_1} + \\frac{1}{{\\sigma}_2})$. We take the reciprocals of the sigma from the measurement and prior belief, add them, and take the reciprocal of the result. Think about that for a moment, and you will see that this will always result in smaller numbers as we proceed."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Implementing Updates\n",
      "\n",
      "That is a beautiful result, but it is not yet a filter. We assumed that the dog was sitting still, an extremely dubious assumption. Certainly it is a useless one - who would need to write a filter to track nonmoving objects? The histogram used a loop of sense and update functions, and we must do the same to accomodate movement.\n",
      "\n",
      "How how do we perform the update function with gaussians? Recall the histogram method:\n",
      "\n",
      "    def update(pos, move, p_correct, p_under, p_over):\n",
      "        n = len(pos)\n",
      "        result = array(pos, dtype=float)\n",
      "        for i in range(n):\n",
      "                result[i] =  \\\n",
      "                pos[(i-move) % n]   * p_correct + \\\n",
      "                pos[(i-move-1) % n] * p_over + \\\n",
      "                pos[(i-move+1) % n] * p_under          \n",
      "        return result\n",
      "        \n",
      "                \n",
      "In a nutshell, we shift the probability vector by the amount we believe the animal moved, and adjust the probability. How do we do that with gaussians?\n",
      "\n",
      "It turns out that we just add gaussians. Think of the case without gaussians. I think my dog is at 7.3m, and he moves 2.6m to right, where is he now? Obviously, $7.3+2.6=9.9$. He is at 9.9m. Abstractly, the algorithm is $new\\_pos = old\\_pos + dist\\_moved$. It does not matter if we use floating point numbers or gaussians for these values, the algorithm must be the same. \n",
      "\n",
      "How is addition for gaussians performed? It turns out to be very simple:\n",
      "$$ N({\\mu}_1, {{\\sigma}_1}^2)+N({\\mu}_2, {{\\sigma}_2}^2) = N({\\mu}_1 + {\\mu}_2, {{\\sigma}_1}^2 + {{\\sigma}_2}^2)$$\n",
      "\n",
      "All we do is add the means and the variance separately! Does that make sense? Think of the physical representation of this abstract equation.\n",
      "${\\mu}_1$ is the old position, and ${\\mu}_2$ is the distance moved. Surely it makes sense that our new position is ${\\mu}_1 + {\\mu}_2$. What about the variance? It is perhaps harder to form an intuition about this. However, recall that with the *update()* function for the histogram filter we always lost information - our confidence after the update was lower than our confidence before the update. Perhaps this makes sense - we don't really know where the dog is moving, so perhaps the confidence should get smaller (variance gets larger). I assure you that the equation for gaussian addition is correct, and derived by basic algebra. Therefore it is reasonable to expect that if we are using gaussians to model physical events, the results must correctly describe those events.\n",
      "\n",
      "I recognize the amount of hand waving in that argument. Now is a good time to either work through the algebra to convince yourself of the mathematical correctness of the algorithm, or to work through some examples and see that it behaves reasonably. This book will do the latter.\n",
      "\n",
      "So, here is our implementation of the update function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def update(pos, sigma, movement, movement_sigma):\n",
      "    return (pos + movement, sigma + movement_sigma)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What is left? Just calling these functions. The histogram did nothing more than loop over the *sense()* and *update()* functions, so let's do the same. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# assume dog is always moving 1m to the right\n",
      "movement = 1\n",
      "movement_error = 2\n",
      "sensor_error = 10\n",
      "pos = (0, 500)   # gaussian N(0,50)\n",
      "\n",
      "dog = DogSensor(pos[0], velocity=movement, noise=sensor_error)\n",
      "\n",
      "zs = []\n",
      "ps = []\n",
      "\n",
      "for i in range(10):\n",
      "    pos = update(pos[0], pos[1], movement, movement_error)\n",
      "    print('UPDATE: %.4f,\\t%.4f' % (pos[0], pos[1]))\n",
      "    \n",
      "    Z = dog.sense()\n",
      "    zs.append(Z)\n",
      "    \n",
      "    pos = sense(pos[0], pos[1], Z, sensor_error)\n",
      "    ps.append(pos[0])\n",
      "    \n",
      "    print('SENSE:  %.4f,\\t%.4f' % (pos[0], pos[1]))\n",
      "    print()\n",
      "    \n",
      "p1, = plt.plot(zs,c='r', linestyle='dashed')\n",
      "p2, = plt.plot(ps, c='b')\n",
      "plt.legend([p1,p2], ['measurement', 'filter'], 2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There is a fair bit of arbitrary constants code above, but don't worry about it. What does require explanation are the first few lines:\n",
      "\n",
      "    movement = 1 \n",
      "    movement_error = 2\n",
      "    \n",
      "For the moment we are assuming that we have some other sensor that detects how the dog is moving. For example, there could be an inertial sensor clipped onto the dog's collar, and it reports how far the dog moved each time it is triggered. The details don't matter. The upshot is that we have a sensor, it has noise, and so we represent it with a Gaussian. Later we will learn what to do if we do not have a sensor for the *update()* step.\n",
      "\n",
      "For now let's walk through the code and output bit by bit.\n",
      "\n",
      "    movement = 1\n",
      "    movement_error = 2\n",
      "    sensor_error = 10\n",
      "    pos = (0, 500)   # gaussian N(0,500)\n",
      "    \n",
      "  \n",
      "The first lines just set up the initial conditions for our filter. We are assuming that the dog moves steadily to the right 1m at a time. We have a relatively low error of 2 for the movement sensor, and a higher error of 10 for the RFID position sensor. Finally, we set our belief of the dog's initial position as $N(0,500)$. Why those numbers. Well, 0 is as good as any number if we don't know where the dog is. But we set the variance to 500 to denote that we have no confidence in this value at all. 100m is almost as likely as 0 with this value for the variance. \n",
      "\n",
      "Next we initialize the RFID simulator with\n",
      "    dog = DogSensor(pos[0], velocity=movement, noise=sensor_error)\n",
      "\n",
      "It may seem very 'convienent' to set the simulator to the same position as our guess, and it is. Do not fret. In the next example we will see the effect of a wildly inaccurate guess for the dog's initial position.\n",
      "\n",
      "The next code allocates an array to store the output of the measurements and filtered positions. \n",
      "\n",
      "    zs = []\n",
      "    ps = []\n",
      "    \n",
      "This is the first time that I am introducing standard nomenclature used by the Kalman filtering literature. It is traditional to call our measurement $Z$, and so I follow that convention here. As an aside, I find the nomenclature used by the literature very obscure. However, if you wish to read the literature you will have to become used to it, so I will not use a much more readable variable name such as $m$ or $measure$.\n",
      "    \n",
      "    \n",
      "Now we just enter our *sense()->update()* loop.\n",
      "\n",
      "    for i in range(10):\n",
      "        pos = update(pos[0], pos[1], movement, sensor_error)\n",
      "        print 'UPDATE:', \"%.4f\" %pos[0], \", %.4f\" %pos[1]\n",
      "\n",
      "Wait, why *update()* before sense? It turns out the order does not matter once, but the first call to DogSensor.sense() assumes that the dog has already moved, so we start with the update step. In practice you will order these calls based on the details of your sensor, and you will very typically do the *sense()* first.\n",
      "\n",
      "So we call the update function with the gaussian representing our current belief about our position, the another gaussian representing our belief as to where the dog is moving, and then print the output. Your output will differ, but when writing this I get this as output:\n",
      "\n",
      "    UPDATE: 1.000 502.000\n",
      "\n",
      "What is this saying? After the update, we believe that we are at 1.0, and the variance is now 502.0. Recall we started at 500.0. The variance got worse, which is always what happens during the update step.\n",
      "\n",
      "    Z = dog.sense()\n",
      "    zs.append(Z)\n",
      "    \n",
      "Here we sense the dog's position, and store it in our array so we can plot the results later.\n",
      "\n",
      "Finally we call the sense function of our filter, save the result in our *ps* array, and print the updated position belief:\n",
      "\n",
      "    pos = sense(pos[0], pos[1], Z, movement_error)\n",
      "    ps.append(pos[0])\n",
      "    print 'SENSE:', \"%.4f\" %pos[0], \", %.4f\" %pos[1]\n",
      "    \n",
      "Your result will be different, but I get\n",
      "\n",
      "    SENSE: 1.6279 , 9.8047\n",
      "    \n",
      "as the result. What is happening? Well, at this point the dog is really at 1.0, however the predicted position is 1.6279. What is happening is the RFID sensor has a fair amount of noise, and so we compute the position as 1.6279. That is pretty far off from 1, but this is just are first time through the loop. Intuition tells us that the results will get better as we make more measurements, so let's hope that this is true for our filter as well. Now look at the variance: 9.8047. It has dropped tremendously from 502.0. Why? Well, the RFID has a reasonably small variance of 2.0, so we trust it far more than our previous belief. At this point there is no way to know for sure that the RFID is outputting reliable data, so the variance is not 2.0, but is has gotten much better.\n",
      "\n",
      "Now the software just loops, calling *update()* and *sense()* in turn. Because of the random sampling I do not know exactly what numbers you are seeing, but the final position is probably between 9 and 11, and the final variance is probably around 3.5. After several runs I did see the final position nearer 7, which would have been the result of several measurements with relatively large errors.\n",
      "\n",
      "Now look at the plot. The noisy measurements are plotted in with a dotted red line, and the filter results are in the solid blue line. Both are quite noisy, but notice how much noisier the measurements (red line) are. This is your first Kalman filter shown to work!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this example I only plotted 10 data points so the output from the print statements would not overwhelm us. Now let's look at the filter's performance with more data. This time we will plot both the output of the filter and the variance."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%precision 2\n",
      "# assume dog is always moving 1m to the right\n",
      "movement = 1\n",
      "movement_error = 2\n",
      "sensor_error = 4.5\n",
      "pos = (0, 100)   # gaussian N(0,50)\n",
      "\n",
      "dog = DogSensor(pos[0], velocity=movement, noise=sensor_error)\n",
      "\n",
      "zs = []\n",
      "ps = []\n",
      "vs = []\n",
      "\n",
      "for i in range(50):\n",
      "    pos = update(pos[0], pos[1], movement, movement_error)    \n",
      "    Z = dog.sense()\n",
      "    zs.append(Z)\n",
      "    vs.append(pos[1])\n",
      "    \n",
      "    pos = sense(pos[0], pos[1], Z, sensor_error)\n",
      "    ps.append(pos[0])\n",
      "    \n",
      "#plt.subplot(121)    \n",
      "p1, = plt.plot(zs,c='r', linestyle='dashed')\n",
      "p2, = plt.plot(ps, c='b')\n",
      "plt.legend([p1,p2], ['measurement', 'filter'], 2)\n",
      "plt.show()\n",
      "\n",
      "plt.plot(vs)\n",
      "plt.title('Variance')\n",
      "plt.show()\n",
      "print ([float(\"%0.4f\" % v) for v in vs])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we can see that the variance converges very quickly to roughly 4.1623 in 10 steps. We interpret this as meaning that we become very confident in our position estimate very quickly. The first few measurements are unsure due to our uncertainty in our guess at the initial position, but the filter is able to quickly determine an accurate estimate."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "> Before I go on, I want to emphasize that this code fully implements a 1D Kalman filter. If you have tried to read the literatue, you are perhaps surprised, because this looks nothing like the complex, endless pages of math in those books. To be fair, the math gets a bit more complicated in multiple dimensions, but not by much. So long as we worry about *using* the equations rather than *deriving* them we can create Kalman filters without a lot of effort. Moreover, I hope you'll agree that you have a decent intuitive grasp of what is happening. We represent our beliefs with Gaussians, and our beliefs get better over time because more measurement means more data to work with. \"Measure twice, cut once!\"\n",
      "\n",
      "\n",
      "#####Excercise:\n",
      "Modify the values of *movement_error* and *sensor_error* and note the effect on the filter and on the variance. Which has a larger effect on the value that variance converges to. For example, which results in a smaller variance:\n",
      "\n",
      "    movement_error = 40\n",
      "    sensor_error = 2\n",
      "    \n",
      "or:\n",
      "\n",
      "    movement_error = 2\n",
      "    sensor_error = 40  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Introduction to Designing a Filter\n",
      "So far we have developed our filter based on the dog sensors introduced in the Discrete Bayesian filter chapter. We are used to this problem by now, and may feel ill-equiped to implement a Kalman filter for a different problem. To be honest, there is still quite a bit of information missing from this presentation. The next chapter will fill in the gaps. Still, lets get a feel for it by designing and implementing a Kalman filter for a thermometer. The sensor for the thermometer outputs a voltage that corresponds to the temperature that is being measured. We have read the manufacturer's specifications for the sensor, and it tells us that the sensor exhibits white noise with a standard deviation of 2.13.\n",
      "\n",
      "We do not have a real sensor to read, so we will simulate the sensor with the following function. We have hard-coded the voltage to 16.3 - obviously the voltage will differ based on the temperature, but that is not important to our filter design."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "temp_variance = 2.13**2\n",
      "def volt():\n",
      "    return random.randn()*temp_variance + 16.3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We generate white noise with a given variance using the equation *random.randn() &ast; variance*. The specification gives us the standard deviation of the noise, not the variance, but recall that variance is just the square of the standard deviation. Hence we raise 2.13 to the second power.\n",
      "> **Sidebar**: spec sheets are just what they sound like - specificiations. Any individual sensor will exhibit different performance based on normal manufacturing variations. Numbers given are often maximums - the spec is a guarantee that the performace will be at least that good. So, our sensor might have standard deviation of 1.8. If you buy an expensive piece of equipment it often comes with a sheet of paper displaying the test results of your specific item; this is usually very trustworthy. On the other hand, if this is a cheap sensor it is likely it received little to no testing prior to being sold. Manufacturers typically test a small subset of their output to verify that everything falls within the desired performance range. If you have a critical application you will need to read the specification sheet carefully to figure out exactly what they mean by their ranges. Do they guarantee their number is a maximum, or is it, say, the $3\\sigma$ error rate? Is every item tested? Is the variance normal, or some other distribution. Finally, manufacturing is not perfect. Your part might be defective and not match the performance on the sheet.\n",
      "\n",
      "> For example, I just randomly looked up a data sheet for an airflow sensor. There is a field \"Repeatability\", with the value \"$\\pm0.50\\%$ Reading\". Is this a Gaussian? Is there a bias? For example, perhaps the repeatibility is nearly $0.0\\%$ at low temperatures, and always nearly  $+0.50$ at high temperatures. Data sheets for electrical components often contain a section of \"Typical Performance Characteristics\". These are used to capture information that cannot be easily conveyed in a table. For example, I am looking at a chart showing output voltage vs current for a LM555 timer. There are three curves showing the performance at different temperatures. The response is ideally linear, but all three lines are curved. This clarifies that errors in voltage outputs are probably not Gaussian - in this chip's case higher temperatures leads to lower voltage output, and the voltage output is quite nonlinear if the input current is very high. \n",
      "\n",
      "> As you might guess, modeling the performance of your sensors is one of the harder parts of creating good Kalman filter. \n",
      "\n",
      "Now we need to write the Kalman filter processing loop. As with our previous problem, we need to perform a cycle of sensing and updating. The sensing step probably seems clear - call *volt()* to get the measurement, pass the result into *sense()* function, but what about the update step? We do not have a sensor to detect 'movement' in the voltage, and for any small duration we expect the voltage to remain constant. How shall we handle this?\n",
      "\n",
      "As always, we will trust in the math. We have no movement, and no error associated with them, so we will just set both to zero. Let's see what happens. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sensor_error = temp_variance\n",
      "movement_error = 0\n",
      "movement = 0\n",
      "voltage = (25,1000) #who knows what the first value is?\n",
      "\n",
      "zs = []\n",
      "ps = []\n",
      "vs = []\n",
      "N=50\n",
      "\n",
      "for i in range(N):\n",
      "    Z = volt()\n",
      "    zs.append(Z)\n",
      "    \n",
      "    voltage = sense(voltage[0], voltage[1], Z, sensor_error)\n",
      "    ps.append(voltage[0])\n",
      "    vs.append(voltage[1])\n",
      "\n",
      "    voltage = update(voltage[0], voltage[1], movement, movement_error)\n",
      "\n",
      "plt.scatter(range(N), zs, marker='+')\n",
      "p1, = plt.plot(ps, c='g')\n",
      "plt.legend([p1], ['filter'], 3)\n",
      "plt.xlim((0,N));plt.ylim((0,30))\n",
      "plt.show()\n",
      "plt.plot(vs)\n",
      "plt.title('Variance')\n",
      "plt.show()\n",
      "print('Variance converges to',vs[-1])\n",
      "print('Last voltage is',voltage[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first plot shows the individual sensor measurements marked with '+'s vs the filter output. Despite a lot of noise in the sensor we quickly discover the approximate voltage of the sensor. In the run I just completed at the time of authorship, the last voltage output from the filter is $16.213$, which is quite close to the $16.4$ used by the *volt()* function. On other runs I have gotten up to around $16.9$ as an output and also as low as 15.5 or so.\n",
      "\n",
      "The second plot shows how the variance converges over time. Compare this plot to the variance plot for the dog sensor. While this does converge to a very small value, it is much slower than the dog problem. The next section **Explaining the Results - Multi-Sensor Fusion** explains why this happens.\n",
      "\n",
      "##### Exercise(optional):\n",
      "Write a function that runs the Kalman filter many times and record what value the voltage converges to each time. Plot this as a histogram. After 10,000 runs do the results look normally distributed? Does this match your intuition of what should happen?\n",
      "\n",
      "> use plt.hist(data,bins=100) to plot the histogram. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "######Solution\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sensor_error = temp_variance\n",
      "\n",
      "def VKF():\n",
      "    voltage=(14,1000)\n",
      "    for i in range(N):\n",
      "        Z = volt()\n",
      "        voltage = sense(voltage[0], voltage[1], Z, sensor_error)\n",
      "    return voltage[0]\n",
      "\n",
      "vs = []\n",
      "for i in range (10000):\n",
      "    vs.append (VKF())\n",
      "plt.hist(vs, bins=100)   \n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "######Discussion\n",
      "The results do in fact look like a normal distribution. Each voltage is Gaussian, and the **Central Limit Theorem** guarantees that a large number of Gaussians is normally distributed. We will discuss this more in a subsequent math chapter."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Explaining the Results - Multi-Sensor Fusion\n",
      "\n",
      "So how does the Kalman filter do so well? I have glossed over one aspect of the filter as it becomes confusing to address too many points at the same time. We will return to the dog tracking problem. We used two sensors to track the dog - the RFID sensor that detects position, and the inertial tracker that tracked movement. However, we have focussed all of our attention on the position sensor. Let's change focus and see how the filter performs if the intertial tracker is also noisy. This will provide us with an vital insight into the performance of Kalman filters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sensor_error = 30\n",
      "movement_sensor = 30\n",
      "pos = (0,500)\n",
      "\n",
      "dog = DogSensor(0, velocity=movement, noise=sensor_error)\n",
      "\n",
      "zs = []\n",
      "ps = []\n",
      "vs = []\n",
      "\n",
      "for i in range(100):\n",
      "    Z = dog.sense()\n",
      "    zs.append(Z)\n",
      "    \n",
      "    pos = sense(pos[0], pos[1], Z, sensor_error)\n",
      "    ps.append(pos[0])\n",
      "    vs.append(pos[1])\n",
      "\n",
      "    pos = update(pos[0], pos[1], movement+ random.randn(), movement_error)\n",
      "\n",
      "p1, = plt.plot(zs,c='r', linestyle='dashed')\n",
      "p2, = plt.plot(ps, c='b')\n",
      "plt.legend([p1,p2], ['measurement', 'filter'], 2)\n",
      "plt.show()\n",
      "plt.plot(vs)\n",
      "plt.title('Variance')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This result is worse than the example where only the measurement sensor was noisy. Instead of being mostly straight, this time the filter's output is distintly jagged. But, it still mostly tracks the dog. What is happening here?\n",
      "\n",
      "This illustrates the effects of *multi-sensor fusion*. Suppose we get a position reading of -28.78 followed by 31.43.  From that information alone it is impossible to tell if the dog is standing still during very noisy measurements, or perhaps sprinting from -29 to 31 and being accurately measured. But we have a second source of information, his velocity. Even when the velocity is also noisy, it constrains what our beliefs might be. For example, suppose that with the 31.43 position reading we get a velocity reading of 59. That matches the difference between the two positions quite well, so this will lead us to believe the RFID sensor and the velocity sensor. Now suppose we got a velocity reading of 1.7. This doesn't match our RFID reading very well - it suggests that the dog is standing still or moving slowly.\n",
      "\n",
      "When sensors measure different aspects of the system and they all agree we have strong evidence that the sensors are accurate. And when they do not agree it is a strong indication that one or more of them are inaccurate. \n",
      "\n",
      "We will formalize this mathematically in the next chapter; for now trust this intuitive explanation. We use this sort of reasoning every day in our lives. If one person tells us something that seems far fetched we are inclined to doubt them. But if several people independently relay the same information we attach higher credence to the data. If one person disagrees with several other people, we tend to distrust the outlier. If we know the people that might alter our belief. If a friend is inclined to practical jokes and tall tales we may put very little trust in what they say. If one lawyer and three lay people opine on some fact of law, and the lawyer disagees with the three you'll probably lend more credence to what the lawyer says because of her expertise. In the next chapter we will learn how to mathematicall model this sort of reasoning."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### More examples\n",
      "\n",
      "##### Example: Extreme Amounts of Noise\n",
      "So I didn't put a lot of noise in the signal, and I also 'correctly guessed' that the dog was at position 0. How does the filter perform in real world conditions? Let's explore and find out. I will start by injecting a lot of noise in the RFID sensor. I will inject an extreme amount of noise - noise that apparently swamps the actual measurement. What does your intution tell about how the filter will perform if the noise is allowed to be anywhere from -300 or 300. In other words, an actual position of 1.0 might be reported as 287.9, or -189.6, or any other number in that range. Think about it before you scroll down."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sensor_error = 30000\n",
      "movement_error = 2\n",
      "pos = (0,500)\n",
      "\n",
      "dog = DogSensor(pos[0], velocity=movement, noise=sensor_error)\n",
      "\n",
      "zs = []\n",
      "ps = []\n",
      "\n",
      "for i in range(1000):\n",
      "    pos = update(pos[0], pos[1], movement, movement_error)\n",
      "    \n",
      "    Z = dog.sense()\n",
      "    zs.append(Z)\n",
      "    \n",
      "    pos = sense(pos[0], pos[1], Z, sensor_error)\n",
      "    ps.append(pos[0])\n",
      "\n",
      "\n",
      "p1, = plt.plot(zs,c='r', linestyle='dashed')\n",
      "p2, = plt.plot(ps, c='b')\n",
      "plt.legend([p1,p2], ['measurement', 'filter'], 2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this example the noise is extreme yet the filter still outputs a nearly straight line! This is an astonishing result! What do you think might be the cause of this performance? If you are not sure, don't worry, we will discuss it latter."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#####Example: Bad Initial Estimate\n",
      "Now let's lets look at the results when we make a bad initial estimate of position. To avoid obscuring the results I'll reduce the sensor variance to 30, but set the initial position to 1000m. Can the filter recover from a 1000m initial error?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sensor_error = 30\n",
      "movement_error = 2\n",
      "pos = (1000,500)\n",
      "\n",
      "dog = DogSensor(0, velocity=movement, noise=sensor_error)\n",
      "\n",
      "zs = []\n",
      "ps = []\n",
      "\n",
      "for i in range(100):\n",
      "    pos = update(pos[0], pos[1], movement, movement_error)\n",
      "    \n",
      "    Z = dog.sense()\n",
      "    zs.append(Z)\n",
      "    \n",
      "    pos = sense(pos[0], pos[1], Z, sensor_error)\n",
      "    ps.append(pos[0])\n",
      "\n",
      "\n",
      "p1, = plt.plot(zs,c='r', linestyle='dashed')\n",
      "p2, = plt.plot(ps, c='b')\n",
      "plt.legend([p1,p2], ['measurement', 'filter'], 2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Again the answer is yes! Because we are relatively sure about our belief in the sensor ($\\sigma=30$) even after the first step we have changed our belief in the first position from 1000 to somewhere around 60.0 or so. After another 5-10 measurements we have converged to the correct value! So this is how we get around the chicken and egg problem of initial guesses. In practice we would probably just assign the first measurement from the sensor as the initial value, but you can see it doesn't matter much if we wildly guess at the initial conditions - the Kalman filter still converges very quickly."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#####Example: Large Noise and Bad Initial Estimate\n",
      "What about the worst of both worlds, large noise and a bad initial estimate?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sensor_error = 30000\n",
      "movement_error = 2\n",
      "pos = (1000,500)\n",
      "\n",
      "\n",
      "dog = DogSensor(0, velocity=movement, noise=sensor_error) \n",
      "zs = []\n",
      "ps = []\n",
      "\n",
      "for i in range(1000):\n",
      "    pos = update (pos[0], pos[1], movement, movement_error)\n",
      "    \n",
      "    Z = dog.sense()\n",
      "    zs.append(Z)\n",
      "    \n",
      "    pos = sense (pos[0], pos[1], Z, sensor_error)\n",
      "    ps.append(pos[0])\n",
      "\n",
      "\n",
      "p1, = plt.plot(zs,c='r', linestyle='dashed')\n",
      "p2, = plt.plot(ps, c='b')\n",
      "plt.legend([p1,p2], ['measurement', 'filter'], 2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This time the filter does struggle. Notice that the previous example only computed 100 updates, whereas this example uses 1000. By my eye it takes the filter 400 or so iterations to become reasonable accurate, but maybe over 600 before the results are good. Kalman filters are good, but we cannot expect miracles. If we have extremely noisy data and extremely bad initial conditions, this is as good as it gets.\n",
      "\n",
      "Finally, let's make the suggest change of making our initial position guess just be the first sensor measurement."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sensor_error = 30000\n",
      "movement_error = 2\n",
      "pos = None\n",
      "\n",
      "dog = DogSensor(0, velocity=movement, noise=sensor_error)\n",
      "\n",
      "zs = []\n",
      "ps = []\n",
      "\n",
      "for i in range(1000):\n",
      "    Z = dog.sense()\n",
      "    zs.append(Z)\n",
      "    if pos == None:\n",
      "        pos = (Z, 500)\n",
      "    \n",
      "    pos = sense (pos[0], pos[1], Z, sensor_error)\n",
      "    ps.append(pos[0])\n",
      "\n",
      "    pos = update (pos[0], pos[1], movement, movement_error)\n",
      "\n",
      "p1, = plt.plot(zs,c='r', linestyle='dashed')\n",
      "p2, = plt.plot(ps, c='b')\n",
      "plt.legend([p1,p2], ['measurement', 'filter'], 2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This simple change significantly improves the results. On some runs it takes 200 iterations or so to settle to a good solution, but other runs it converges very rapidly. This all depends on whether the initial measurement $Z$ had a small amount or large amount of noise. \n",
      "\n",
      "200 iterations may seem like a lot, but the amount of noise we are injecting is truly huge. In the real world we use sensors like thermometers,  laser rangefinders, GPS satellites, computer vision, and so on. None have the enormous error as shown here. A reasonable value for the variance for a cheap thermometer might be 10, for example, and our code is using 30,000 for the variance. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#####Exercise:\n",
      "Implement the Kalman filter using IPython Notebook's animation features to allow you to modify the various constants in real time using sliders. Refer to the section **Interactive Gaussians** in the Gaussian chapter to see how to do this. You will use the *interact()* function to call a calculation and plotting function. Each parameter passed into *interact()* automatically gets a slider created for it. I have built the boilerplate for this; just fill in the required code."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.html.widgets import interact, interactive, fixed\n",
      "import IPython.html.widgets as widgets\n",
      "def plot_kalman_filter(start_pos, sensor_noise, movement, movement_noise, noise_scale):\n",
      "    # your code goes here\n",
      "    pass\n",
      "\n",
      "interact(plot_kalman_filter,\n",
      "         start_pos=(-10,10), \n",
      "         sensor_noise=widgets.IntSliderWidget(value=5,min=0,max=100), \n",
      "         movement=widgets.FloatSliderWidget(value=1,min=-2.,max=2.), \n",
      "         movement_noise=widgets.FloatSliderWidget(value=5,min=0,max=100.),\n",
      "         noise_scale=widgets.FloatSliderWidget(value=1,min=0,max=2.))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "######Solution\n",
      "One possible solution follows."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "zs = np.zeros(100)\n",
      "ps = np.zeros(100)\n",
      "def plot_kalman_filter(start_pos, sensor_noise, movement, movement_noise,noise_scale):\n",
      "    dog = DogSensor(start_pos, velocity=movement, noise=sensor_noise)\n",
      "    random.seed(303)\n",
      "    pos = (0,100)\n",
      "\n",
      "    for i in range(100):\n",
      "        Z = dog.sense() + random.randn()*noise_scale\n",
      "        zs[i] = Z\n",
      "\n",
      "        pos = sense(pos[0], pos[1], Z, sensor_error)\n",
      "        ps[i] = pos[0]\n",
      "\n",
      "        pos = update(pos[0], pos[1], movement + random.randn()*movement_noise, movement_noise)\n",
      "\n",
      "    p1, = plt.plot(zs,c='r', linestyle='dashed')\n",
      "    p2, = plt.plot(ps, c='b')\n",
      "    plt.legend([p1,p2], ['measurement', 'filter'], 2)\n",
      "    plt.show()\n",
      "\n",
      "interact(plot_kalman_filter,\n",
      "         start_pos=(-10,10), \n",
      "         sensor_noise=widgets.IntSliderWidget(value=5,min=0,max=100), \n",
      "         movement=widgets.FloatSliderWidget(value=1,min=-2.,max=2.), \n",
      "         movement_noise=widgets.FloatSliderWidget(value=2,min=0,max=100.),\n",
      "         noise_scale=widgets.FloatSliderWidget(value=1,min=0,max=20.))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#####Exercise - Nonlinear Systems\n",
      "\n",
      "Our equations are linear: \n",
      "$$\\begin{align*}new\\_pos&=old\\_pos+dist\\_moved\\\\\n",
      "new\\_position&=old\\_position*measurement\\end{align*}$$\n",
      "\n",
      "Do you suppose that this filter works well or poorly with nonlinear systems?\n",
      "\n",
      "Implement a Kalman filter that uses the following *sin()* to generate the measurement value for i in range(100):\n",
      "\n",
      "    Z = math.sin(i/3.) * 2\n",
      "    \n",
      "Adust the variance and initial positions to see the effect. What is, for example, the result of a very bad initial guess?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#enter your code here."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###### Solution:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sensor_error = 30\n",
      "movement_error = 2\n",
      "pos = (100,500)\n",
      "\n",
      "zs = []\n",
      "ps = []\n",
      "\n",
      "\n",
      "for i in range(100):\n",
      "    pos = update(pos[0], pos[1], movement, movement_error)\n",
      "\n",
      "    Z = math.sin(i/3.)*2\n",
      "    zs.append(Z)\n",
      "    \n",
      "    pos = sense(pos[0], pos[1], Z, sensor_error)\n",
      "    ps.append(pos[0])\n",
      "\n",
      "\n",
      "p1, = plt.plot(zs,c='r', linestyle='dashed')\n",
      "p2, = plt.plot(ps, c='b')\n",
      "plt.legend([p1,p2], ['measurement', 'filter'], 2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "######Discussion\n",
      "\n",
      "Here we set a bad initial guess of 100. We can see that the filter never 'acquires' the signal. Note now the peak of the filter output always lags the peak of the signal by a small amount. More clearely we can see the large gap in height between the measurement and filter. \n",
      "**REWriTE - not seeing heigh gap now**\n",
      "\n",
      "Maybe we just didn't adjust things 'quite right'. After all, the output looks like a sin wave, it is just offset in $x$ and $y$. Let's test this assumption.\n",
      "\n",
      "#####Exercise - Noisy Nonlinear Systems\n",
      "Implement the same system, but add noise to the measurement."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#enter your code here"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "######Solution"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sensor_error = 30\n",
      "movement_error = 2\n",
      "pos = (100,500)\n",
      "\n",
      "zs = []\n",
      "ps = []\n",
      "\n",
      "\n",
      "for i in range(100):\n",
      "    pos = update(pos[0], pos[1], movement, movement_error)\n",
      "\n",
      "    Z = math.sin(i/3.)*2 + random.randn()*1.2\n",
      "    zs.append(Z)\n",
      "    \n",
      "    pos = sense(pos[0], pos[1], Z, sensor_error)\n",
      "    ps.append(pos[0])\n",
      "\n",
      "\n",
      "p1, = plt.plot(zs,c='r', linestyle='dashed')\n",
      "p2, = plt.plot(ps, c='b')\n",
      "plt.legend([p1,p2], ['measurement', 'filter'], 3)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###### Discussion\n",
      "This is terrible! The output is not at all like a sin wave, except in the grossest way. With linear systems we could add extreme amounts of noise to our signal and still extract a very accurate result, but here even modest noise creates a very bad result.\n",
      "\n",
      "Very shortly after practioners began implementing Kalman filters they realized the poor performance of them for nonlinear systems and began devising ways of dealing with it. Much of this book is devoted to this problem and its various solutions."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Summary\n",
      "This information in this chapter takes some time to assimulate. To truly understand this you will probably have to work through this chapter several times. I encourage you to change the various constants and observe the results. Convince yourself that Gaussians are a good representation of a unimodal belief of something like the position of a dog in a hallway. Then convince yourself that multiplying Gaussians truly does compute a new belief from your prior belief and the new measurement. Finally, convince yourself that if you are measuring movement, that adding the Gaussians correctly updates your belief. That is all the Kalman filter does. Even now I alternate between complacency and amazement at the results. \n",
      "\n",
      "If you understand this, you will be able to understand multidimensional Kalman filters and the various extensions that have been make on them. If you do not fully understand this, I strongly suggest rereading this chapter. Try implementing the filter from scratch, just by looking at the equations and reading the text. Change the constants. Maybe try to implement a different tracking problem, like tracking stock prices. Experimentation will build your intuition and understanding of how these marvelous filters work."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def volt():\n",
      "    return random.randn()*4. + 14.4\n",
      "\n",
      "sensor_error = 30\n",
      "movement_error = 2\n",
      "pos = (12,500)\n",
      "\n",
      "zs = []\n",
      "ps = []\n",
      "vs = []\n",
      "\n",
      "\n",
      "for i in range(100):\n",
      "    Z = volt()\n",
      "    zs.append(Z)\n",
      "    \n",
      "    pos = sense(pos[0], pos[1], Z, sensor_error)\n",
      "    ps.append(pos[0])\n",
      "    vs.append(pos[1])\n",
      "    #pos = update(pos[0], pos[1], 0, movement_error)\n",
      "\n",
      "\n",
      "p1, = plt.plot(zs,c='r', linestyle='dashed')\n",
      "p2, = plt.plot(ps, c='b')\n",
      "plt.legend([p1,p2], ['measurement', 'filter'], 3)\n",
      "plt.show()\n",
      "plt.plot(vs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "author notes:\n",
      "    clean up the code - same stuff duplicated over and over - write a 'clean implemntation' at the end.\n",
      "    \n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#format the book\n",
      "from IPython.core.display import HTML\n",
      "def css_styling():\n",
      "    styles = open(\"./styles/custom2.css\", \"r\").read()\n",
      "    return HTML(styles)\n",
      "css_styling()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}