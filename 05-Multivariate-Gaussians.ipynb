{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Table of Contents](http://nbviewer.ipython.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/table_of_contents.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Gaussians  - Modeling Uncertainty in Multiple Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('http://fonts.googleapis.com/css?family=Source+Code+Pro');\n",
       "@import url('http://fonts.googleapis.com/css?family=Vollkorn');\n",
       "@import url('http://fonts.googleapis.com/css?family=Arimo');\n",
       "@import url('http://fonts.googleapis.com/css?family=Fira_sans');\n",
       "\n",
       "    div.cell{\n",
       "        width: 900px;\n",
       "        margin-left: 0% !important;\n",
       "        margin-right: auto;\n",
       "    }\n",
       "    div.text_cell code {\n",
       "        background: transparent;\n",
       "        color: #000000;\n",
       "        font-weight: 600;\n",
       "        font-size: 11pt;\n",
       "        font-style: bold;\n",
       "        font-family:  'Source Code Pro', Consolas, monocco, monospace;\n",
       "   }\n",
       "    h1 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "\t}\n",
       "\t\n",
       "    div.input_area {\n",
       "        background: #F6F6F9;\n",
       "        border: 1px solid #586e75;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 30pt;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h2 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "        text-align: left;\n",
       "    }\n",
       "    .text_cell_render h2 {\n",
       "        font-weight: 200;\n",
       "        font-size: 16pt;\n",
       "        font-style: italic;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1.5em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h3 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h3 {\n",
       "        font-weight: 200;\n",
       "        font-size: 14pt;\n",
       "        line-height: 100%;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 2em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    }\n",
       "    h4 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h4 {\n",
       "        font-weight: 100;\n",
       "        font-size: 14pt;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    h5 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 200;\n",
       "        font-style: normal;\n",
       "        color: #1d3b84;\n",
       "        font-size: 16pt;\n",
       "        margin-bottom: 0em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    div.text_cell_render{\n",
       "        font-family: 'Fira sans', verdana,arial,sans-serif;\n",
       "        line-height: 150%;\n",
       "        font-size: 110%;\n",
       "        font-weight: 400;\n",
       "        text-align:justify;\n",
       "        text-justify:inter-word;\n",
       "    }\n",
       "    div.output_subarea.output_text.output_pyout {\n",
       "        overflow-x: auto;\n",
       "        overflow-y: scroll;\n",
       "        max-height: 50000px;\n",
       "    }\n",
       "    div.output_subarea.output_stream.output_stdout.output_text {\n",
       "        overflow-x: auto;\n",
       "        overflow-y: scroll;\n",
       "        max-height: 50000px;\n",
       "    }\n",
       "    div.output_wrapper{\n",
       "        margin-top:0.2em;\n",
       "        margin-bottom:0.2em;\n",
       "}\n",
       "\n",
       "    code{\n",
       "      font-size: 70%;\n",
       "    }\n",
       "    .rendered_html code{\n",
       "    background-color: transparent;\n",
       "    }\n",
       "    ul{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li li{\n",
       "        padding-left: 0.2em; \n",
       "        margin-bottom: 0.2em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    ol{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ol li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    a:link{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:visited{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:hover{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:focus{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:active{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    .rendered_html :link {\n",
       "       text-decoration: underline; \n",
       "    }\n",
       "    .rendered_html :hover {\n",
       "       text-decoration: none; \n",
       "    }\n",
       "    .rendered_html :visited {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :focus {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :active {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "    } \n",
       "    hr {\n",
       "      color: #f3f3f3;\n",
       "      background-color: #f3f3f3;\n",
       "      height: 1px;\n",
       "    }\n",
       "    blockquote{\n",
       "      display:block;\n",
       "      background: #fcfcfc;\n",
       "      border-left: 5px solid #c76c0c;\n",
       "      font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "      width:680px;\n",
       "      padding: 10px 10px 10px 10px;\n",
       "      text-align:justify;\n",
       "      text-justify:inter-word;\n",
       "      }\n",
       "      blockquote p {\n",
       "        margin-bottom: 0;\n",
       "        line-height: 125%;\n",
       "        font-size: 100%;\n",
       "      }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\", \"autobold.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    scale:100,\n",
       "                        availableFonts: [],\n",
       "                        preferredFont:null,\n",
       "                        webFont: \"TeX\",\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#format the book\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from __future__ import division, print_function\n",
    "from book_format import load_style\n",
    "load_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The techniques in the last chapter are very powerful, but they only work with one variable or dimension. Gaussians represent a mean and variance that are scalars - real numbers. They provide no way to represent multidimensional data, such as the position of a dog in a field. You may retort that you could use two Kalman filters from the last chapter. One would track the x coordinate and the other  the y coordinate. That does work, but suppose we want to track position, velocity, acceleration, and attitude. These values are related to each other, and as we learned in the g-h chapter we should never throw away information. Through one key insight we will achieve markedly better filter performance than was possible with the equations from the last chapter.\n",
    "\n",
    "In this chapter I will introduce you to multivariate Gaussians - Gaussians for more than one variable, and the key insight I mention above. Then, in the next chapter we will use the math from this chapter to write a complete filter in just a few lines of code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Normal Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last two chapters we used Gaussians for a scalar (one dimensional) variable, expressed as $\\mathcal{N}(\\mu, \\sigma^2)$. A more formal term for this is *univariate normal*, where univariate means 'one variable'. The probability distribution of the Gaussian is known as the *univariate normal distribution*.\n",
    "\n",
    "What might a *multivariate normal distribution* be? *Multivariate* means multiple variables. Our goal is to be able to represent a normal distribution across multiple dimensions. I don't necessarily mean spatial dimensions - it could be position, velocity, and acceleration. Consider a two dimensional case. Let's say we believe that $x = 2$ and $y = 17$. This might be the *x* and *y* coordinates for the position of our dog, it might be the  position and velocity of our dog on the x-axis, or the temperature and wind speed at our weather station. It doesn't really matter. We can see that for $N$ dimensions, we need $N$ means, which we will arrange in a column matrix (vector) like so:\n",
    "\n",
    "$$\n",
    "\\mu = \\begin{bmatrix}\\mu_1\\\\\\mu_2\\\\ \\vdots \\\\\\mu_n\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Therefore for this example we would have\n",
    "\n",
    "$$\n",
    "\\mu = \\begin{bmatrix}2\\\\17\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "The next step is representing our variances. At first blush we might think we would also need N variances for N dimensions. We might want to say the variance for x is 10 and the variance for y is 4, like so. \n",
    "\n",
    "$$\\sigma^2 = \\begin{bmatrix}10\\\\4\\end{bmatrix}$$ \n",
    "\n",
    "This is incomplete because it does not consider the more general case. In the **Gaussians** chapter we computed the variance in the heights of students. We can do the same for their weights. But there is also a relationship between height and weight. In general, a taller person weighs more than a shorter person.  We say they are *correlated*. We want a way to express not only what we think the variance is in the height and the weight, but also the degree to which they are correlated. \n",
    "\n",
    "Before we can understand multivariate normal distributions we need to understand the mathematics behind correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation and Covariance\n",
    "\n",
    "The *covariance* describes how much two variables vary together. Covariance is short for *correlated variances*. In other words, *variance* is a measure for how a population vary amongst themselves, and *covariance* is a measure for how much two variables change in relation to each other.  Generally speaking as height increases weight also increases. These variables are *correlated*. It is also called *positively correlated* because as one variable gets larger so does the other. As the outdoor temperature decreases home heating bills increase. These are *inversely correlated* because as one variable gets larger the other variable lowers. This is also called *negatively correlated*. The price of tea and the number of tail wags my dog makes have no relation to each other, and we say they are *uncorrelated* - each can change independent of the other.\n",
    "\n",
    "Correlation implies *prediction*. If you are significantly taller than me I can predict that you also weigh more. As winter comes I predict that I will be spending more to heat my house. If my dog wags his tail more I don't conclude that tea prices will be increasing.\n",
    "\n",
    "For example, here is a plot of height and weight of students on the school's track team. If a student is 67 inches tall I can predict they weigh roughly 140 pounds. Since the correlation is not perfect neither is my prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvoAAAEtCAYAAAB5zMTfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbPSJiIiIiKzQ/wD2BGqVYpeoNAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x4aee668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gaussian_internal import plot_correlated_data\n",
    "\n",
    "height = [60, 62, 63, 65, 65.1, 68, 69, 70, 72, 74]\n",
    "weight = [95, 120, 127, 119, 151, 143, 173, 171, 180, 210]\n",
    "\n",
    "plot_correlated_data(height, weight, 'Height (in)', 'Weight (lbs)', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this book we will always be using *linear correlation*. We assume that the relationship between variables is linear. That is, a straight line is a good fit for the data. I've fit a straight line through the data in the above chart. The concept of *nonlinear correlation* exists, but we will not be using it.\n",
    "\n",
    "\n",
    "The equation for the covariance between $X$ and $Y$ is\n",
    "\n",
    "$$ COV(X, Y) = \\sigma_{xy} = E\\big[(X-\\mu_x)(Y-\\mu_y)\\big]$$\n",
    "\n",
    "Where $E[X]$ is the *expected value* of X defined as\n",
    "\n",
    "$$E[X] =  \\begin{cases} \\sum_{i=1}^n p_ix_i & \\mbox{discrete}\\\\ \\int_{-\\infty}^\\infty x\\, f(x) & \\mbox{continuous}\\end{cases}$$\n",
    "\n",
    "\n",
    "Compare the covariance equation to the equation for the variance:\n",
    "\n",
    "$$VAR(X) = \\sigma_x^2 = E[(X - \\mu)^2]$$\n",
    "\n",
    "As you can see they are very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a *covariance matrix* to denote covariances of a multivariate normal distribution, and it looks like this:\n",
    "$$\n",
    "\\Sigma = \\begin{bmatrix}\n",
    "  \\sigma_1^2 & \\sigma_{12} & \\cdots & \\sigma_{1n} \\\\\n",
    "  \\sigma_{21} &\\sigma_2^2 & \\cdots & \\sigma_{2n} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  \\sigma_{n1} & \\sigma_{n2} & \\cdots & \\sigma_n^2\n",
    " \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If you haven't seen this before it is probably a bit confusing. Instead of continuing with the mathematical definitions I will build your intuition with thought experiments. At this point, note that the diagonal contains the variance for each variable, and that all off-diagonal elements (covariances) represent how much the $i$th (horizontal row) and $j$th (vertical column) variables are linearly correlated to each other. So $\\sigma_3^2$ is the variance of the third variable, and $\\sigma_{13}$ is the covariance between the first and third variables.\n",
    "\n",
    "A covariance of 0 indicates no correlation. If the variance for $x$ is 10, the variance for $y$ is 4, and there is no linear correlation between $x$ and $y$, then we would write\n",
    "\n",
    "$$\\Sigma = \\begin{bmatrix}10&0\\\\0&4\\end{bmatrix}$$\n",
    "\n",
    "If there was a small amount of positive correlation between $x$ and $y$ we might have\n",
    "\n",
    "$$\\Sigma = \\begin{bmatrix}10&1.2\\\\1.2&4\\end{bmatrix}$$\n",
    "\n",
    "where 1.2 is the covariance between $x$ and $y$. I say the correlation is \"small\" because the covariance of 1.2 is small relative to the variances of 10. \n",
    "\n",
    "If there was a large amount of negative correlation between  between $x$ and $y$ we might have\n",
    "$$\\Sigma = \\begin{bmatrix}10&-9.7\\\\-9.7&4\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "Note that the matrix always symmetric. Logically the covariance between $x$ and $y$ is always equal to the covariance between $y$ and $x$. That is, $\\sigma_{xy}=\\sigma_{yx}$ for any $x$ and $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work through a few examples. `numpy.cov` computes the covariance matrix. Lets create data with perfectly correlated data. By this I mean that the data perfectly fits on a line - there is no variance from the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.linspace(1, 10, 100)\n",
    "Y = np.linspace(1, 10, 100)\n",
    "print(np.cov(X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the covariance matrix that the covariance is equal to the variance in x and in y. \n",
    "\n",
    "Now let's add some noise to one of the variables so that they are no longer perfectly correlated. I will make $Y$ negative to create a negative correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.linspace(1, 10, 100)\n",
    "Y = -(np.linspace(1, 5, 100) + np.sin(X)*.2)\n",
    "plot_correlated_data(X, Y)\n",
    "print(np.cov(X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data no longer forms a straight line. The covariance is $\\sigma_{xy}=-3.08$. The absolute value is smaller than the variance of $X$ or $Y$. It is not close to zero, and so we know there is still a high degree of correlation. We can verify this by looking at the chart. The data forms nearly a straight line.\n",
    "\n",
    "Now I will add random noise to a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.random import randn\n",
    "X = np.linspace(1, 10, 1000) + randn(1000)\n",
    "Y = np.linspace(1, 5, 1000) + randn(1000)\n",
    "plot_correlated_data(X, Y)\n",
    "print(np.cov(X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the variance is smaller in relation to the variances, reflecting the lower correlation between $X$ and $Y$. We can still fit a straight line through this data, but there is much greater variation in the data.\n",
    "\n",
    "Finally, here is the covariance between completely random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = randn(100000)\n",
    "Y = randn(100000)\n",
    "plot_correlated_data(X, Y)\n",
    "print(np.cov(X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the covariances are very near zero. As you can see with the plot, there is no clear way to draw a line to fit the data. A vertical line would be as convincing as the horizontal line shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Normal Distribution Equation\n",
    "\n",
    "Here is the multivariate normal distribution in $n$ dimensions.\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x},\\, \\mu,\\,\\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^n|\\Sigma|}}\\, \\exp  \\Big [{ -\\frac{1}{2}(\\mathbf{x}-\\mu)^\\mathsf{T}\\Sigma^{-1}(\\mathbf{x}-\\mu) \\Big ]}\n",
    "$$\n",
    "\n",
    "I urge you to not try to remember this equation. We will program it in a Python function and then call it if we need to compute a specific value. However, note that it has the same form as the univariate normal distribution:\n",
    "\n",
    "$$ \n",
    "f(x, \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\Big [{-\\frac{1}{2}}{(x-\\mu)^2}/\\sigma^2 \\Big ]\n",
    "$$\n",
    "\n",
    "The multivariate version merely replaces the scalars of the univariate equations with matrices. If you are reasonably well-versed in linear algebra this equation should look quite manageable; if not, don't worry, we have code to compute it for us! Let's plot it and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mkf_internal\n",
    "mean = (2, 17)\n",
    "cov = [[10., 0], \n",
    "       [0, 4.]]\n",
    "\n",
    "mkf_internal.plot_3d_covariance(mean, cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a plot of multivariate Gaussian with a mean of $\\mu=[\\begin{smallmatrix}2\\\\17\\end{smallmatrix}]$ and a covariance of $\\Sigma=[\\begin{smallmatrix}10&0\\\\0&4\\end{smallmatrix}]$. The three dimensional shape shows the probability density of for any value of ($X, Y$) in the z-axis. I have projected the variance for x and y onto the walls of the chart - you can see that they take on the normal Gaussian bell curve shape. The curve for $X$ is wider than the curve for $Y$, which is explained by $\\sigma_x^2=10$ and $\\sigma_y^2=4$. The highest point of the curve is at the the means for $X$ and $Y$. \n",
    "\n",
    "All multivariate Gaussians have this shape. If we think of this as a the Gaussian for the position of a dog, the z-value at each point of ($X, Y$) is the probability density of it being at that position. Strictly speaking this is the *joint probability density function*, which I will define soon. So, the dog has the highest probability of being near (2, 17), a modest probability of being near (5, 14), and a very low probability of being near (10, 10). As with the univariate case this is a *probability density*, not a *probability*. Continuous distributions have an infinite range, and so the probability of being exactly at (2, 17), or any point, is 0%. We can compute the probability of being within a given range by computing the area under the curve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FilterPy [2] implements the equation with the function `filterpy.stats.multivariate_gaussian`. SciPy's `stats` module implements the multivariate normal equation with `multivariate_normal()`. It implements a 'frozen' form where you set the mean and covariance once, and then calculate the probability for any number of values for x over any arbitrary number of calls. This is much more efficient then recomputing everything in each call. I named my function `multivariate_gaussian()` to ensure it is never confused with the SciPy version. I will say that for a single call, where the frozen variables do not matter, mine consistently runs faster as measured by the `timeit` function.\n",
    "\n",
    "> The <a href=\"http://docs.scipy.org/doc/scipy/reference/tutorial/stats.html\">tutorial</a>[1] for the `scipy.stats` module explains 'freezing' distributions and other very useful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from filterpy.stats import gaussian, multivariate_gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll demonstrate using it, and then move on to more interesting things.\n",
    "\n",
    "First, let's find the probability density for our dog being at (2.5, 7.3) if we believe he is at (2, 7) with a variance of 8 for $x$ and a variance of 3 for $y$.\n",
    "\n",
    "Start by setting $x$ to (2.5, 7.3). You can use a tuple, list, or NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [2.5, 7.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set the mean of our belief:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mu = [2.0, 7.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have to define our covariance matrix. In the problem statement we did not mention any correlation between $x$ and $y$, and we will assume there is none. This makes sense; a dog can choose to independently wander in either the $x$ direction or $y$ direction without affecting the other. If there is no correlation between the values place the variances in the diagonal, and set off-diagonal elements to zero. I will use name `P`. Kalman filters use the name $\\textbf{P}$ for the covariance matrix, and we need to become familiar with the conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P = [[8., 0.], \n",
    "     [0., 3.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('{:.4}'.format(multivariate_gaussian(x, mu, P)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers are not easy to interpret. Let's view a plot of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mkf_internal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = mkf_internal.plot_3d_covariance(mu, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is clearly a 3D bell shaped curve. We can see that the Gaussian is centered around (2,7), and that the probability density quickly drops away in all directions. This plot shows what is known as the *joint probability*. On the sides of the plot I have drawn the *marginal probability* for $X$ and $Y$ (defined below).\n",
    "\n",
    "It's time to define some terms. The *joint probability*, denoted $P(x,y)$, is the probability of both $x$ and $y$ happening.  For example, if you have two die $P(2,5)$ is the probability of the first die rolling a 2 and the second die rolling a 5. Assuming the die are six sided and fair, the probability $P(2,5) = \\frac{1}{6}\\times \\frac{1}{6}=\\frac{1}{36}$. The 3D chart above shows the *joint probability density function*.\n",
    "\n",
    "The *marginal probability* is the probability of an event happening without regard of any other event. In the chart above the Gaussian curve drawn to the left is the marginal for $Y$. This is the probability for the dog being at any position in $Y$ disregarding the value for $X$. In back I've plotted the marginal for $X$, which is the probability of the dog being at any position in $X$ disregarding $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at this in a slightly different way. Instead of plotting a surface showing the probability distribution I will generate 1,000 points with the distribution of $[\\begin{smallmatrix}8&0\\\\0&3\\end{smallmatrix}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mkf_internal.plot_3d_sampled_covariance(mu, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of the sampled points as being possible locations for our dog given those particular mean and covariances. The contours on the side show the marginal probability for $X$ and $Y$. We can see that he is far more likely to be at (2, 7) where there are many points, than at (-5, 5) where there are few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As beautiful as these plots are, it is hard to get useful information from them. For example, it is not easy to tell if $X$ and $Y$ both have the same variance. In most of the book I'll display Gaussians using contour plots using helper functions from FilterPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from book_format import set_figsize, figsize\n",
    "\n",
    "with figsize(y=5):\n",
    "    mkf_internal.plot_3_covariances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those of you viewing this online or in Juptyer Notebook on your computer, here is an animation of varying the covariance while holding the variance constant.\n",
    "\n",
    "<img src='animations/multivariate_ellipse.gif'>\n",
    "\n",
    "(source: http://git.io/vqxLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contour plots display the range of values that the multivariate Gaussian takes for a specific standard deviation. This is like taking a horizontal slice out of the 3D plot. By default it displays one standard deviation, but you can use either the `variance` or `std` parameter to control what is displayed. For example, `variance=3**2` or `std=3` would display the 3rd standard deviation, and `variance=[1,4,9]` or `std=[1,2,3]` would display the 1st, 2nd, and 3rd standard deviations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from filterpy.stats import plot_covariance_ellipse\n",
    "P = [[2, 0], [0, 9]]\n",
    "plot_covariance_ellipse((2, 7), P, fc='g', alpha=0.2, \n",
    "                        std=[1, 2, 3],\n",
    "                        title='|2 0|\\n|0 9|')\n",
    "plt.gca().grid(b=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solid colors may suggest to you that the probability distribution is constant between the standard deviations. This is not true, as you can tell from the 3D plot of the Gaussian. Here is a 2D shaded representation of the probability distribution for the covariance ($\\begin{smallmatrix}2&1.2\\\\1.2&1.3\\end{smallmatrix})$. Darker gray corresponds to higher probability density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nonlinear_plots import plot_cov_ellipse_colormap\n",
    "plot_cov_ellipse_colormap(cov=[[2, 1.2], [1.2, 1.3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking about the physical interpretation of these plots clarifies their meaning. The mean and covariance of the first plot is\n",
    "\n",
    "$$\n",
    "\\mathbf{\\mu} =\\begin{bmatrix}2\\\\7\\end{bmatrix},\\, \\,\n",
    "\\Sigma = \\begin{bmatrix}2&0\\\\0&2 \\end{bmatrix}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [2, 7]\n",
    "P = [[2, 0], [0, 2]]\n",
    "plot_covariance_ellipse(x, P, fc='g', alpha=0.2, \n",
    "                        title='|2 0|\\n|0 2|')\n",
    "plt.gca().grid(b=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Bayesian way of thinking about this is that the ellipse shows us the amount of error in our belief. A tiny circle would indicate that we have a very small error, and a very large circle indicates a lot of error in our belief. The shape of the ellipse shows us the geometric relationship of the errors in $X$ and $Y$. Here we have a circle so errors in $X$ and $Y$ are equally likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second plot is for the mean and covariance\n",
    "\n",
    "$$\n",
    "\\mu =\\begin{bmatrix}2\\\\7\\end{bmatrix}, \\, \\, \\, \n",
    "\\Sigma = \\begin{bmatrix}2&0\\\\0&9\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [2, 7]\n",
    "P = [[2, 0], [0, 9]]\n",
    "plot_covariance_ellipse(x, P, fc='g', alpha=0.2, \n",
    "                        title='|2 0|\\n|0 9|')\n",
    "plt.gca().grid(b=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we use a different variance for $X$ ($\\sigma_x^2=2$) vs $Y$ ($\\sigma^2_y=9$). The result is a tall and narrow ellipse. We can see that a lot more uncertainty in $Y$ vs $X$. In both cases we believe the dog is at (2, 7), but the uncertainties are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third plot shows the mean and covariance\n",
    "\n",
    "$$\n",
    "\\mu =\\begin{bmatrix}2\\\\7\\end{bmatrix}, \\, \\, \\, \n",
    "\\Sigma = \\begin{bmatrix}2&1.2\\\\1.2&2\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = [2, 7]\n",
    "P = [[2, 1.2], [1.2, 2]]\n",
    "plot_covariance_ellipse(x, P, fc='g', alpha=0.2, \n",
    "                        title='|2 1.2|\\n|1.2 2|')\n",
    "plt.gca().grid(b=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first contour that has values in the off-diagonal elements of the covariance, and this is the first contour plot with a slanted ellipse. This is not a coincidence. The two facts are telling us the same thing. A slanted ellipse tells us that the $x$ and $y$ values are somehow correlated. The off-diagonal elements in the covariance matrix are non-zero, indicating that a correlation exists.\n",
    "\n",
    "Recall the plot for height versus weight. It formed a slanted grouping of points. We can use NumPy's `cov()` function to compute the covariance of two or more variables by placing them into a 2D array. Let's do that, then plot the $2\\sigma$ covariance ellipse on top of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cov_hw = np.cov(np.vstack((height, weight)))\n",
    "cov_hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(height, weight, s=120, marker='s')\n",
    "plt.title('Track Team Height vs. Weight')\n",
    "plt.xlabel('Height (in)'); plt.ylabel('Weight (lbs)')\n",
    "plot_covariance_ellipse((np.mean(height), np.mean(weight)), cov_hw, fc='g', \n",
    "                        alpha=0.2, axis_equal=False, std=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should help you form a strong intuition on the meaning and use of covariances. The covariance ellipse shows you how the data is 'scattered' in relation to each other. A narrow ellipse like this tells you that the data is very correlated. There is only a narrow range of weights for any given height. The ellipse leans towards the right, telling us there is a positive correlation - as x increases y also increases. If the ellipse leaned towards the left then the correlation would be negative - as x increases y decreases. We can see this in the following plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_temp = [200, 250, 300, 400, 450, 500]\n",
    "lifespan = [10, 9.7, 5, 5.4, 4.3, 0.3]\n",
    "plt.scatter(max_temp, lifespan, s=80)\n",
    "cov = np.cov(np.vstack((max_temp, lifespan)))\n",
    "plot_covariance_ellipse((np.mean(max_temp), np.mean(lifespan)), cov, fc='g', \n",
    "            alpha=0.2, axis_equal=False, std=2)\n",
    "plt.title('Engine Temperature vs Lifespan')\n",
    "plt.xlabel('Temperature (C)'); plt.ylabel('Years');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationships between variances and covariances can be hard to puzzle out by inspection, so here is an interactive plot. (If you are reading this in a static form instructions to run this online are here: http://git.io/vmv6e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.html.widgets import interact, interactive, fixed\n",
    "from IPython.html.widgets import FloatSlider\n",
    "\n",
    "def plot_covariance(var_x, var_y, cov_xy):\n",
    "    P1 = [[var_x, cov_xy], [cov_xy, var_y]]\n",
    "\n",
    "    plot_covariance_ellipse((10, 10), P1, axis_equal=False,\n",
    "                            show_semiaxis=True)\n",
    "\n",
    "    plt.xlim(4, 16)\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.ylim(4, 16)\n",
    "    plt.show()\n",
    "\n",
    "interact (plot_covariance,           \n",
    "          var_x=FloatSlider(value=5., min=0, max=20.), \n",
    "          var_y=FloatSlider(value=5., min=0., max=20.), \n",
    "          cov_xy=FloatSlider(value=1.5, min=0.0, max=50, step=.2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson's Correlation Coefficient\n",
    "\n",
    "The correlation between two variables can be given a numerical value with *Pearson's Correlation Coefficient*. It is defined as\n",
    "\n",
    "$$\\rho_{xy} = \\frac{COV(X, Y)}{\\sigma_x \\sigma_y}$$\n",
    "\n",
    "This value can range in value from -1 to 1. If the covariance is 0 than $\\rho=0$. A value greater than 0 indicates that the relationship is a positive correlation, and a negative value indicates that there is a negative correlation. Values near -1 or 1 indicate a very strong correlation, and values near 0 indicate a very weak correlation.\n",
    "\n",
    "Correlation and covariance are very closely related. Covariance has units associated with it, and correlation is a unitless ratio. For example, for our dog $\\sigma_{xy}$ has units of meters squared.\n",
    "\n",
    "We can use `scipy.stats.pearsonr` function to compute the Pearson coefficient. It returns a tuple of the Pearson coefficient and of the 2 tailed p-value. The latter is not used in this book. Here we compute $\\rho$ for height vs weight of student athletes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "pearsonr(height, weight)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the correlation between engine temperature and lifespan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pearsonr(max_temp, lifespan)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not be using this coefficient much in this book,but you may see it elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Correlations to Improve Estimates\n",
    "\n",
    "Suppose we believe our dog is at position (5, 10) with some given covariance. If the standard deviation in x and y is each 2 meters, but they are strongly correlated, the covariance contour would look something like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P = [[4, 3.9], [3.9, 4]]\n",
    "plot_covariance_ellipse((5, 10), P, ec='k', std=[1, 2, 3])\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose I were to tell you that we know that $x=7.5$. What can we infer about the value for $y$? The position is extremely likely to lie within the 3$\\sigma$ covariance ellipse. We can *infer* the position in *y* based on the covariance matrix because there is a correlation between *x* and *y*. I've illustrated the likely range of values for y as a blue filled circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mkf_internal.plot_correlation_covariance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A circle not mathematically correct, but it gets the idea across. We will tackle the mathematics in the next section. For now recognize that we can predict that $y$ is likely near 12. A value of $y=-10$ is extremely improbable.\n",
    "\n",
    "A word about *correlation* and *independence*. If variables are *independent* they can vary separately. If you walk in an open field, you can move in the $x$ direction (east-west), the $y$ direction(north-south), or any combination thereof. Independent variables are always also *uncorrelated*. Except in special cases, the reverse does not hold true. Variables can be uncorrelated, but dependent. For example, consider $y=x^2$. Correlation is a linear measurement, so $x$ and $y$ are uncorrelated. However, they are obviously dependent on each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplying Multidimensional Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapter we incorporated an uncertain measurement with an uncertain estimate by multiplying their Gaussians together. The result was another Gaussian with a smaller variance. If two pieces of uncertain information corroborate each other we should be more certain in our conclusion. The graphs look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mkf_internal.plot_gaussian_multiply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combination of measurements 1 and 2 yields more certainty, so the new Gaussian is taller and narrower - the variance became smaller. The same happens in multiple dimensions with multivariate Gaussians.\n",
    "\n",
    "Here are the equations for multiplying multivariate Gaussians. They are generated by plugging the Gaussians for the prior and the estimate into Bayes Theorem. I gave you the algebra for the univariate case in the last section of the **Univariate Kalman Filter** chapter. You will not need to remember these equations, as they are computed by Kalman filter equations that will be presented shortly. This computation is also available in FilterPy using the `multivariate_multiply()` method, which you can import from `filterpy.stats`. \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mu &= \\Sigma_2(\\Sigma_1 + \\Sigma_2)^{-1}\\mu_1 + \\Sigma_1(\\Sigma_1 + \\Sigma_2)^{-1}\\mu_2 \\\\\n",
    "\\Sigma &= \\Sigma_1(\\Sigma_1+\\Sigma_2)^{-1}\\Sigma_2\n",
    "\\end{aligned}$$\n",
    "\n",
    "To give you some intuition about this, recall the equations for multiplying univariate Gaussians:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mu &=\\frac{\\sigma_1^2 \\mu_2 + \\sigma_2^2 \\mu_1} {\\sigma_1^2 + \\sigma_2^2}, \\\\\n",
    "\\sigma^2 &= \\frac{1}{\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}} = \\frac{\\sigma_1^2\\sigma_2^2}{\\sigma_1^2+\\sigma_2^2}\n",
    "\\end{aligned}$$\n",
    "\n",
    "This looks similar to the equations for the multivariate equations. This will be more obvious if you recognize that matrix inversion, denoted by the -1 power, is *like* division since $AA^{-1} =I$. I will rewrite the inversions as divisions - this is not a mathematically correct thing to do but it does help us see what is going on.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mu &\\approx \\frac{\\Sigma_2\\mu_1 + \\Sigma_1\\mu_2}{\\Sigma_1 + \\Sigma_2} \\\\ \\\\\n",
    "\\Sigma &\\approx \\frac{\\Sigma_1\\Sigma_2}{(\\Sigma_1+\\Sigma_2)}\n",
    "\\end{aligned}$$\n",
    "\n",
    "In this form the relationship between the univariate and multivariate equations is clear.\n",
    "\n",
    "Now let's explore multivariate Gaussians in terms of a concrete example. Suppose that we are tracking an aircraft with two radar systems. I will ignore altitude so I can use two dimensional plots. Radars give us the range and bearing to a target. We start out being uncertain about the position of the aircraft, so the covariance, which is our uncertainty about the position, might look like this. In the language of Bayesian statistics this is our *prior*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "P0 = [[6, 0], [0, 6]]\n",
    "plot_covariance_ellipse((10, 10), P0, fc='y', alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that there is a radar to the lower left of the aircraft. Further suppose that the radar's bearing measurement is accurate, but the range measurement is inaccurate. The covariance for the error in the measurement might look like this (plotted in blue):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "P1 = [[2, 1.9], [1.9, 2]]\n",
    "plot_covariance_ellipse((10, 10), P0, fc='y', alpha=0.6)\n",
    "plot_covariance_ellipse((10, 10), P1, fc='b', alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that Bayesian statistics calls this the *evidence*. The ellipse points towards the radar. It is very long because the range measurement is inaccurate, and the aircraft could be within a considerable distance of the measured range. It is very narrow because the bearing estimate is very accurate and thus the aircraft must be very close to the bearing estimate.\n",
    "\n",
    "We want to find the *posterior* - the mean and covariance that results from incorporating the evidence into the prior. As in every other chapter we combine evidence by multiplying them together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from filterpy.stats import multivariate_multiply\n",
    "\n",
    "P2 = multivariate_multiply((10, 10), P0, (10, 10), P1)[1]\n",
    "\n",
    "plot_covariance_ellipse((10, 10), P0, fc='y', alpha=0.2)\n",
    "plot_covariance_ellipse((10, 10), P1, fc='b', alpha=0.6)\n",
    "plot_covariance_ellipse((10, 10), P2, fc='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have plotted the original estimate (prior) in a very transparent yellow, the radar reading in blue (evidence), and the finale estimate (posterior) in yellow.\n",
    "\n",
    "The posterior retained the same shape and position as the radar measurement, but is smaller. We've seen this with one dimensional Gaussians. Multiplying two Gaussians makes the variance smaller because we are incorporating more information, hence we are less uncertain.  Another point to recognize is that the covariance shape reflects the physical layout of the aircraft and the radar system. The importance of this will become clear in the next step.\n",
    "\n",
    "Now lets say we get a measurement from a second radar, this one to the lower right, which I will plot in blue against the yellow covariance of our current belief. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P3 = [[2, -1.9], [-1.9, 2.2]]\n",
    "plot_covariance_ellipse((10, 10), P2, fc='y', alpha=0.6)\n",
    "plot_covariance_ellipse((10, 10), P3, fc='b', alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To incorporate this new information we will multiply the Gaussians together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P4 = multivariate_multiply((10, 10), P2, (10, 10), P3)[1]\n",
    "plot_covariance_ellipse((10, 10), P2, fc='y', alpha=0.2)\n",
    "plot_covariance_ellipse((10, 10), P3, fc='b', alpha=0.6)\n",
    "plot_covariance_ellipse((10, 10), P4, fc='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only likely place for the aircraft is where the two ellipses intersect. The shapes reflects the geometry of the problem. This allows us to *triangulate* on the aircraft, resulting in a very accurate estimate. We didn't explicitly write any code to perform triangulation; it was a natural outcome of multiplying the Gaussians of each measurement together.\n",
    "\n",
    "Think back to the **g-h Filter** chapter where we displayed the error bars of two weighings on a scale. The estimate must fall somewhere within the region where the error bars overlap. Here the estimate must fall between 161 to 163 pounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gh_internal as gh\n",
    "gh.plot_errorbar2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose the first radar is to the left of the aircraft. I can model the measurement error with\n",
    "$$\\Sigma = \\begin{bmatrix}2&0\\\\0&0.2\\end{bmatrix}$$\n",
    "\n",
    "Here we see the result of multiplying the prior with the measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P1 = [[2, 0], [0, .2]]\n",
    "P2 = multivariate_multiply((10, 10), P0, (10, 10), P1)[1]\n",
    "plot_covariance_ellipse((10, 10), P0, facecolor='y', alpha=0.2)\n",
    "plot_covariance_ellipse((10, 10), P1, facecolor='b', alpha=0.6)\n",
    "plot_covariance_ellipse((10, 10), P2, facecolor='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can incorporate the measurement from the second radar system, which we will leave in the same position as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P3 = [[2, -1.9], [-1.9, 2.2]]\n",
    "P4 = multivariate_multiply((10, 10), P2, (10, 10), P3)[1]\n",
    "plot_covariance_ellipse((10, 10), P2, facecolor='y', alpha=0.2)\n",
    "plot_covariance_ellipse((10, 10), P3, facecolor='b', alpha=0.6)\n",
    "plot_covariance_ellipse((10, 10), P4, facecolor='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our estimate is not as accurate as the previous example.  The two radar stations are no longer orthogonal to each other relative to the aircraft's position so the triangulation is not optimal.\n",
    "\n",
    "For a final example, imagine taking two measurements from the same radar a short time apart. The covariance ellipses will nearly overlap, leaving a very large error in our new estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P5 = multivariate_multiply((10,10), P2, (10.1, 9.97), P2)\n",
    "plot_covariance_ellipse((10, 10), P2, fc='y', alpha=0.2)\n",
    "plot_covariance_ellipse((10.1, 9.97), P2, fc='b', alpha=0.6)\n",
    "plot_covariance_ellipse(P5[0], P5[1], fc='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can already see why a multivariate Kalman filter can perform better than a univariate one. The last section demonstrated how we can use correlations between variables to significantly improve our estimates. We can take this much further. This section contains the key insight to this chapter, so read carefully.\n",
    "\n",
    "Let's say we are tracking an aircraft and we get the following data for the $x$ and $y$ coordinates at time $t$=1,2, and 3 seconds. What does your intuition tell you the value of $x$ will be at time $t$=4 seconds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mkf_internal\n",
    "mkf_internal.show_position_chart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the aircraft is flying in a straight line and we know that aircraft cannot turn on a dime. The most reasonable guess is that at $t$=4 the aircraft is at (4,4). I will depict that with a green arrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mkf_internal.show_position_prediction_chart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You made this inference because you *inferred* a constant velocity for the airplane. The reasonable\n",
    "assumption is that the aircraft is moving one unit each in *x* and *y* per time step.\n",
    "\n",
    "Think back to the **g-h Filter** chapter when we were trying to improve the weight predictions of a noisy scale. We incorporated *weight gain* into the equations because it allowed us to make a better prediction of the weight the next day. The g-h filter uses the $g$ parameter to scale the amount of significance given to the current weight measurement, and the $h$ parameter scaled the amount of significance given to the weight gain.\n",
    "\n",
    "We are going to do the same thing with our Kalman filter. After all, the Kalman filter is a form of a g-h filter. In this case we are tracking an airplane, so instead of weight and weight gain we need to track position and velocity. Weight gain is the *derivative* of weight, and of course velocity is the derivative of position. It's impossible to plot and understand the 4D chart that would be needed to plot *x* and *y* and their respective velocities so let's do it for $x$, knowing that the math generalizes to more dimensions.\n",
    "\n",
    "At time 1 we might be fairly certain about the position (x=0) but have no idea about the velocity. We can plot that with a covariance matrix like this. The narrow width expresses our relative certainty about position, and the tall height expresses our lack of knowledge about velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mkf_internal.show_x_error_chart(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after one second we get a position update of x=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mkf_internal.show_x_error_chart(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies that our velocity is roughly 5 m/s. But of course position and velocity are correlated. If the velocity is 5 m/s the position would be 5, but if the velocity was 10 m/s the position would be 10. So let's draw a covariance matrix in red showing the relationship between the position and velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mkf_internal.show_x_error_chart(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It won't be clear until the next chapter how I calculate this covariance. Ignore the calculation, and think about what this implies. We have no easy way to say where the object really is because we are so uncertain about the velocity. Hence the ellipse stretches very far in the x-axis. Our uncertainty in velocity of course means it is also very spread in the y-axis. But as I said in the last paragraph, position and velocity is correlated. *If* the velocity is 5 m/s the next position would be 5, and *if* the velocity is 10 the next position would be 10. They are extremely correlated, so the ellipse must be very narrow. \n",
    "\n",
    "This superposition of the two covariances is where the magic happens. The only reasonable estimate at time t=1 (where position=5) is roughly the *intersection* between the two covariance matrices! More exactly, we can use the math from the last section and *multiply* the two covariances together. From a Bayesian point of view we multiply the prior with the probability of the evidence (the *likelihood*) to get the posterior. If we multiply the position covariance with the velocity covariance using the Bayesian equations we get the result shown in the next chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mkf_internal.show_x_error_chart(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the new covariance (the posterior) lies at the intersection of the position covariance and the velocity covariance. It is slightly tilted, showing that there is some correlation between the position and velocity. Far more importantly, it is much smaller than either the position or velocity covariances. In the previous chapter our variance would get smaller each time we performed an `update()` because the previous estimate was multiplied by the new measurement. The same thing happens here. However, the amount by which the covariance got smaller by is much larger in this chapter. This is because we are using two different kinds of information which are nevertheless correlated. Knowing the velocity approximately and the position approximately allows us to very quickly hone in on the correct answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a key point in Kalman filters, so read carefully! Our sensor is only detecting the position of the aircraft (how doesn't matter). This is called an *observed variable*. It does not have a sensor that provides velocity. But based on the position estimates we can compute velocity. We call the velocity a *hidden variable*. Hidden means what it sounds like - there is no sensor that is measuring velocity, thus its value is hidden from us. We are able to use the correlation between position and velocity to infer its value very accurately.\n",
    "\n",
    "To round out the terminology there are also *unobserved variables*. For example, the aircraft's state includes things such as as heading, engine RPM, weight, color, the first name of the pilot, and so on. We cannot sense these directly using the position sensor so they are not *observed*. There is no way to *infer* them from the sensor measurements and correlations (red planes don't go faster than white planes), so they are not *hidden*. Instead, they are *unobservable*. If you include an unobserved variable in your filter state the estimate for that variable will be nonsense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes this possible? Imagine for a moment that we superimposed the velocity from a different airplane over the position graph. Clearly the two are not related, and there is no way that combining the two could possibly yield any additional information. In contrast, the velocity of this airplane tells us something very important - the direction and speed of travel. So long as the aircraft does not alter its velocity the velocity allows us to predict where the next position is. After a relatively small amount of error in velocity the probability that it is a good match with the position is very small. Think about it - if you suddenly change direction your position is also going to change a lot. If the measurement of the position is not in the direction of the velocity change it is very unlikely to be true. The two are correlated, so if the velocity changes so must the position, and in a predictable way. \n",
    "\n",
    "It is important to understand that we are taking advantage of the fact that velocity and position are correlated. We get a rough estimate of velocity from the distance and time between two measurement, and use Bayes theorem to and produce very accurate estimates after only a few observations. Please reread this section if you have any doubts. If you do not understand this you will quickly find it impossible to reason about what you will learn in the following chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have taken advantage of the geometry and correlations of the system to produce a very accurate estimate. The math does not care whether we are working with two positions, or a position and a correlated velocity, or if these are spatial dimensions. If floor space is correlated to house price you can write a Kalman filter to track house prices. If age is correlated to disease incidence you can write a Kalman filter to track diseases. If the zombie population is inversely correlated with the number of shotguns then you can write a Kalman filter to track zombie populations. I showed you this in terms of geometry and talked about *triangulation*. That was just to build your intuition. You can write a Kalman filter for state variables that have no geometric representation, such as filters for stock prices or milk production of cows (I received an email from someone tracking milk production!) Get used to thinking of these as Gaussians with correlations. If we can express our uncertainties as a multidimensional Gaussian we can then multiply the prior with the likelihood and get a much more accurate result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [1] http://docs.scipy.org/doc/scipy/reference/tutorial/stats.html\n",
    "\n",
    "- [2] `FilterPy` library. Roger Labbe.\n",
    "https://github.com/rlabbe/filterpy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
