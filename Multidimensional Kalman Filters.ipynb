{
 "metadata": {
  "name": "",
  "signature": "sha256:fd542341131473089289548f12ba7f78ac88663ce51e586ff357c5fb48c3eb91"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Multidimensional Kalman Filters\n",
      "=====\n",
      "\n",
      "The techniques in the last chapter are very powerful, but they only work in one dimension. The gaussians represent a mean and variance that are scalars - real numbers. They provide no way to represent multidimensional data, such as the position of a dog in a field. You may retort that you could use two Kalman filters for that case, one tracks the x coordinate and the other tracks the y coordinate. That does work in some cases, but put that thought aside, because soon you will see some enormous benefits to implementing the multidimensional case.\n",
      "\n",
      "\n",
      "###Multivariate Normal Distributions\n",
      "\n",
      "\n",
      "What might a *multivariate normal distribution* look like? In this context, multivariate just means multiple variables. Our goal is to be able to represent a normal distribution across multiple dimensions. Consider the 2 dimensional case. Let's say we believe that $x = 2$ and $y = 7$. Therefore we can see that for $N$ dimensions, we need $N$ means, like so:\n",
      "$$ \\mu = \\begin{bmatrix}{\\mu}_1\\\\{\\mu}_2\\\\ \\vdots \\\\{\\mu}_n\\end{bmatrix} \n",
      "$$\n",
      "\n",
      "Therefore for this example we would have\n",
      "$$\n",
      "\\mu = \\begin{bmatrix}2\\\\7\\end{bmatrix} \n",
      "$$\n",
      "\n",
      "The next step is representing our variances. At first blush we might think we would also need N variances for N dimensions. We might want to say the variance for x is 10 and the variance for y is 8, like so. \n",
      "\n",
      "$$\\sigma^2 = \\begin{bmatrix}10\\\\8\\end{bmatrix}$$ \n",
      "\n",
      "While this is possible, it does not consider the more general case. For example, suppose we were tracking house prices vs total $m^2$ of the floor plan. These numbers are *correlated*. It is not an exact correlation, but in general houses in the same neighborhood are more expensive if they have a larger floor plan. We want a way to express not only what we think the variance is in the price and the $m^2$, but also the degree to which they are correlated. It turns out that we use the following matrix to denote *covariances* with multivariate normal distributions. You might guess, correctly, that *covariance* is short for *correlated variances*.\n",
      "\n",
      "$$\n",
      "\\Sigma = \\begin{pmatrix}\n",
      "  {{\\sigma}_{1}}^2 & p{\\sigma}_{1}{\\sigma}_{2} & \\cdots & p{\\sigma}_{1}{\\sigma}_{n} \\\\\n",
      "  p{\\sigma}_{2}{\\sigma}_{1} &{{\\sigma}_{2}}^2 & \\cdots & p{\\sigma}_{2}{\\sigma}_{n} \\\\\n",
      "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
      "  p{\\sigma}_{n}{\\sigma}_{1} & p{\\sigma}_{n}{\\sigma}_{2} & \\cdots & {{\\sigma}_{n}}^2\n",
      " \\end{pmatrix}\n",
      "$$\n",
      "\n",
      "If you haven't seen this before it is probably a bit confusing at the moment. Rather than explain the math right now, we will take our usual tactic of building our intuition first with various physical models. At this point, note that the diagonal contains the variance for each state variable, and that all off-diagonal elements are a product of the $\\sigma$ corresponding to the $i$th (row) and $j$th (column) state variable multiplied by a constant $p$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, without explanation, here is the full equation for the multivarate normal distribution.\n",
      "\n",
      "$$\\mathcal{N}(\\mu,\\,\\Sigma) = (2\\pi)^{-\\frac{n}{2}}|\\Sigma|^{-\\frac{1}{2}}\\, e^{ -\\frac{1}{2}(\\mathbf{x}-\\mu)'\\Sigma^{-1}(\\mathbf{x}-\\mu) }$$\n",
      "\n",
      "I urge you to not try to remember this function. We will program it once in a function and then call it when we need to compute a specific value. However, if you look at it briefly you will note that it looks quite similar to the *univarate normal distribution*  except it uses matrices instead of scalar values, and the root of $\\pi$ is scaled by $N$.\n",
      "\n",
      "$$ \n",
      "f(x, \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{{-\\frac{1}{2}}{(x-\\mu)^2}/\\sigma^2 }\n",
      "$$\n",
      "\n",
      "If you are reasonably well-versed in linear algebra this equation should look quite managable; if not, don't worry! If you want to learn the math we will cover it in detail in the next optional chapter. If you choose to skip that chapter the rest of this book should still be managable for you\n",
      "\n",
      "I have programmed it and saved it in the file gaussian.py with the function name multivariate_gaussian. I am not showing the code here because I have taken advantage of the linear algebra solving apparatus of numpy to efficiently compute a solution - the code does not correspond to the equation in a one to one manner. If you wish to view the code, I urge you to either load it in an editor, or load it into this worksheet by putting '%load gaussian.py' without the quotes in the next cell and executing it with ctrl-enter. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> **Note**: As of version 0.14 scipy has implemented the multivariate normal equation with the function **scipy.stats.multivariate_normal()**. It is superior to my function in several ways. First, it is implemented in Fortran, and is therefore faster than mine. Second, it implements a 'frozen' form where you set the mean and covariance once, and then calculate the probability for any number of values for x over any arbitrary number of calls. This is much more efficient then recomputing everything in each call. So, if you have version 0.14 or later you may want to substitute my function for the built in version. Use **scipy.version.version** to get the version number. Note that I deliberately named my function **multivariate_gaussian()** to ensure it is never confused with the built in version.\n",
      "\n",
      "> If you intend to use Python for Kalman filters, you will want to read the tutorial for the stats module, which explains 'freezing' distributions and other very useful features. As of this date, it includes an example of using the multivariate_normal function, which does work a bit differently from my function.\n",
      "http://docs.scipy.org/doc/scipy/reference/tutorial/stats.html</div>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from stats import gaussian, multivariate_gaussian"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's use it to compute a few values just to make sure we know how to call and use the function, and then move on to more interesting things.\n",
      "\n",
      "First, let's find the probability for our dog being at (2.5, 7.3) if we believe he is at (2,7) with a variance of 8 for x and a variance of 10 for y. This function requires us to pass everything in as numpy arrays (we will soon provide a more robust version that works with numpy matrices, numpy arrays, and/or scalars in any combinations. That code contains a lot of boilerplate which obscures the algorithm).\n",
      "\n",
      "So we set x to (2.5,7.3)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "x = np.array([2.5, 7.3])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we set the mean of our belief:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mu = np.array([2,7])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we have to define our covariance matrix. In the problem statement we did not mention any correlation between x and y, and we will assume there is none. This makes sense; a dog can choose to independently wander in either the x direction or y direction without affecting the other. If there is no correlation between the values you just fill in the diagonal of the covariance matrix with the variances:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cov = np.array([[8.,0],[0,10.]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now just call the function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(multivariate_gaussian(x,mu,cov))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's check the probability for the dog being at exactly (2,7)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function\n",
      "\n",
      "x = np.array([2,7])\n",
      "print(multivariate_gaussian(x,mu,cov))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These numbers are not easy to interpret. Let's plot this in 3D, with the z (up) coordinate being the probability."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.pylab as pylab\n",
      "from matplotlib import cm\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "import numpy as np\n",
      "\n",
      "pylab.rcParams['figure.figsize'] = 12,6\n",
      "cov = np.array([[8.,0],[0,10.]])\n",
      "\n",
      "mu = np.array([2,7])\n",
      "\n",
      "xs, ys = np.arange(-8, 13, .75), np.arange(-8, 20, .75)\n",
      "xv, yv = np.meshgrid (xs, ys)\n",
      "\n",
      "zs = np.array([100.* multivariate_gaussian(np.array([x,y]),mu,cov) \\\n",
      "               for x,y in zip(np.ravel(xv), np.ravel(yv))])\n",
      "zv = zs.reshape(xv.shape)\n",
      "\n",
      "ax = plt.figure().add_subplot(111, projection='3d')\n",
      "ax.plot_surface(xv, yv, zv)\n",
      "\n",
      "ax.set_xlabel('X')\n",
      "ax.set_ylabel('Y')\n",
      "\n",
      "ax.contour(xv, yv, zv, zdir='x', offset=-9, cmap=cm.autumn)\n",
      "ax.contour(xv, yv, zv, zdir='y', offset=20, cmap=cm.BuGn)\n",
      "plt.xlim((-10,20))       \n",
      "plt.ylim((-10,20))       \n",
      "\n",
      "plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The result is clearly a 3D bell shaped curve. We can see that the gaussian is centered around (2,7), and that the probability quickly drops away in all directions. On the sides of the plot I have drawn the Gaussians for x in greens and for y in orange.\n",
      "\n",
      "As beautiful as this is, it is perhaps a bit hard to get useful information. For example, it is not easy to tell if x and y both have the same variance or not. So for most of the rest of this book we will display multidimensional gaussian using contour plots. I will use some helper functions in gaussian.py to plot them. If you are interested in linear algebra go ahead and look at the code used to produce these contours, otherwise feel free to ignore it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import stats\n",
      "\n",
      "cov = np.array([[2,0],[0,2]])\n",
      "e = stats.sigma_ellipse (cov, 2, 7)\n",
      "plt.subplot(131)\n",
      "stats.plot_sigma_ellipse(e, '|2 0|\\n|0 2|')\n",
      "\n",
      "\n",
      "cov = np.array([[2,0],[0,9]])\n",
      "e = stats.sigma_ellipse (cov, 2, 7)\n",
      "plt.subplot(132)\n",
      "stats.plot_sigma_ellipse(e, '|2 0|\\n|0 9|')\n",
      "\n",
      "plt.subplot(133)\n",
      "cov = np.array([[2,1.2],[1.2,3]])\n",
      "e = stats.sigma_ellipse (cov, 2, 7)\n",
      "stats.plot_sigma_ellipse(e,'|2 1.2|\\n|1.2 2|')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From a mathematical perspective these display the values that the multivariate gaussian takes for a specific sigma (in this case $\\sigma^2=1$. Think of it as taking a horizontal slice through the 3D surface plot we did above. However, thinking about the physical interpretation of these plots clarifies their meaning.\n",
      "\n",
      "The first plot uses mean and the covariance matrices $\n",
      "\\mu =\\begin{bmatrix}2\\\\7\\end{bmatrix}, cov = \\begin{bmatrix}2&0\\\\0&2\\end{bmatrix}$. Let this be our current belief about the position of our dog in a field. In other words, we believe that he is positioned at (2,7) with a variance of $\\sigma^2=2$ for both x and y. The contour plot shows where we believe the dog is located with the '+' in the center of the ellipse. The ellipse shows the boundary for the $1\\sigma^2$ probability - points where the dog is quite likely to be based on our current knowledge. Of course, the dog might be very far from this point, as Gaussians allow the mean to be any value. For example, the dog could be at (3234.76,189989.62), but that has vanishing low probability of being true. Generally speaking displaying  the $1\\sigma^2$ to $2\\sigma^2$ contour captures the most likely values for the distribution. An equivelent way of thinking about this is the circle/ellipse shows us the amount of error in our belief. A tiny circle would indicate that we have a very small error, and a very large circle indicates a lot of error in our belief. We will use this throughout the rest of the book to display and evaluate the accuracy of our filters at any point in time. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The second plot uses mean and the covariance matrices $\n",
      "\\mu =\\begin{bmatrix}2\\\\7\\end{bmatrix}, cov = \\begin{bmatrix}2&0\\\\0&9\\end{bmatrix}$. This time we use a different variance for x (2) vs y (9). The result is an ellipse. When we look at it we can immediately tell that we have a lot more uncertainty in the y value vs the x value. Our belief that the value is (2,7) is the same in both cases, but errors are different. This sort of thing happens naturally as we track objects in the world - one sensor has a better view of the object, or is closer, than another sensor, and so we end up with different error rates in the different axis."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The third plot uses mean and the covariance matrices $\n",
      "\\mu =\\begin{bmatrix}2\\\\7\\end{bmatrix}, cov = \\begin{bmatrix}2&1.2\\\\1.2&2\\end{bmatrix}$. This is the first contour that has values in the off-diagonal elements of $cov$, and this is the first contour plot with a slanted ellipse. This is not a coincidence. The two facts are telling use the same thing. A slanted ellipse tells us that the x and y values are somehow **correlated**. We denote that in the covariance matrix with values off the diagonal. What does this mean in physical terms? Think of trying to park your car in a parking spot. You can not pull up beside the spot and then move sideways into the space because most cars cannot go purely sideways. $x$ and $y$ are not independent. This is a consequence of the steering system in a car. When your tires are turned the car rotates around its rear axle while moving forward. Or think of a horse attached to a pivoting exercise bar in a corral. The horse can only walk in circles, he cannot vary $x$ and $y$ independently, which means he cannot walk straight forward to to the side. If $x$ changes, $y$ must also change in a defined way. \n",
      "\n",
      "So when we see this ellipse we know that $x$ and $y$ are correlated, and that the correlation is \"strong\". I will not prove it here, but a 45 $^{\\circ}$ angle denotes complete correlation between $x$ and $y$, whereas $0$ and $90$ denote no correlation at all. Those who are familiar with this math will be objecting quite strongly, as this is actually quite sloppy language that does not adress all of the mathematical issues. They are right, but for now this is a good first approximation to understanding these ellipses from a physical interpretation point of view. The size of the ellipse shows how much error we have in each axis, and the slant shows how strongly correlated the values are.\n",
      "**IS THIS TRUE???**\n",
      "\n",
      "A word about **correlation** and **independence**. If variables are **independent** they can vary separately. If you walk in an open field, you can move in the $x$ direction (east-west), the $y$ direction(north-south), or any combination thereof. Independent variables are always also **uncorrelated**. Except in special cases, the reverse does not hold true. Variables can be uncorrelated, but dependent. For example, consider the pair$(x,y)$ where $y=x^2$. Correlation is a linear measurement, so $x$ and $y$ are uncorrelated. However, they are obviously dependent on each other. ** wikipedia article 'correlation and dependence' claims multivariate normals are a special case, where the correlation coeff $p$ completely defines the dependence. FIGURE THIS OUT!**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Kalman Filter Basics\n",
      "\n",
      "Let's say we are tracking an aircraft and we get the following data for the $x$ coordinate at time $t$=1,2, and 3 seconds. What does your intuition tell you the value of $x$ will be at time $t$=4 seconds?\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter ([1,2,3],[1,2,3])\n",
      "plt.xlim([0,4])\n",
      "plt.ylim([0,4])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It appears that the aircraft is flying in a straight line because we can draw a line between the three points, and we know that aircraft cannot turn on a dime. The most reasonable guess is that $x$=4 at $t$=4. I will depict that below with a green square to depict the predictions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter ([1,2,3],[1,2,3])\n",
      "plt.xlim([0,5]); plt.ylim([0,5])\n",
      "plt.plot([0,5],[0,5],'r')\n",
      "plt.scatter ([4], [4], c='g', marker='s',s=200)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If this is data from a Kalman filter, then each point has both a mean and variance. Let's try to show that by showing the approximate error for each point. Don't worry about why I am using a covariance matrix to depict the variance at this point, it will become clear in a few paragraphs. The intent at this point is to show that while we have$x$=1,2,3 that there is a lot of error associated with each measurement."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cov = np.array([[0.003,0], [0,12]])\n",
      "sigma=[0.5,1.,1.5,2]\n",
      "e1 = stats.sigma_ellipses(cov, x=1, y=1, sigma=sigma)\n",
      "e2 = stats.sigma_ellipses(cov, x=2, y=2, sigma=sigma)\n",
      "e3 = stats.sigma_ellipses(cov, x=3, y=3, sigma=sigma)\n",
      "stats.plot_sigma_ellipses([e1, e2, e3], axis_equal=True,x_lim=[0,4],y_lim=[0,15])\n",
      "plt.ylim([0,11])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see that there is a lot of error associated with each value of $x$. We could write a 1D Kalman filter as we did in the last chapter, but suppose this is the output of that filter, and not just raw sensor measurements. Are we out of luck?\n",
      "\n",
      "Let us think about how we predicted that $x$=4 at $t$=4. In one sense we just drew a straight line between the points and saw where it lay at $t$=4. My constant refrain: what is the physical interpretation of that? What is the difference in $x$ over time? What is $\\frac{\\partial x}{\\partial t}$? The derivative, or difference in distance over time is *velocity*. \n",
      "\n",
      "This is the **key point** in Kalman filters, so read carefully! Our sensor is only detecting the position of the aircraft (how doesn't matter). It does not have any kind of sensor that provides velocity to us. But based on the position estimates we can compute velocity. In Kalman filters we would call the velocity an **unobserved variable**. Unobserved means what it sounds like - there is no sensor that is measuring velocity directly. Since the velocity is based on the position, and the position has error, the velocity will have error as well. What happens if we draw the velocity errors over the positions errors?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from matplotlib.patches import Ellipse\n",
      "\n",
      "cov = np.array([[1,1],[1,1.1]])\n",
      "ev = stats.sigma_ellipses(cov, x=2, y=2, sigma=sigma)\n",
      "\n",
      "isct = Ellipse(xy=(2,2), width=.2, height=1.2, edgecolor='r', fc='None', lw=4)\n",
      "plt.figure().gca().add_artist(isct)\n",
      "stats.plot_sigma_ellipses([e1, e2, e3, ev], axis_equal=True,x_lim=[0,4],y_lim=[0,15])\n",
      "plt.ylim([0,11])\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Think about what this plot means. We have a lot of error in our position estimates. We therefore have a lot of error in our velocity estimates. But look at the intersections between the velocity and the positions. Take the intersection at $t$=2. The intersection between the velocity and the position is where our aircraft is most likely to be, which I have roughly depicted with a red ellipse ('roughly' because I set the size via eyeball, not via math). The size of the error is much smaller than the error of the positions, despite the fact that velocity was derived from position. \n",
      "\n",
      "What makes this possible? Imagine for a moment that we superimposed the velocity from a *different* airplane over the position graph. Cleary the two are not related, and there is no way that combining the two could possibly yield any additional information. In contrast, the velocity of the this airplane tells us something very important - the direction and speed of travel. So long as the aircraft does not alter its velocity the velocity allows us to predict where the next position is. After a relatively small amount of error in velocity the probability that it is a good match with the position is very small. Think about it - if you suddenly change direction your position is also going to change a lot. If the position measurement is not in the direction of the assumed velocity change it is very unlikely to be true. The two are correlated, so if the velocity changes so must the position, and in a predictable way.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Kalman Filter Algorithm\n",
      "So in general terms we can show how a multidimensional Kalman filter works. In the example above, we compute velocity from the previous position measurements using something called the **measurement function**. Then we predict the next position by using the current estimate and something called the **state transition function**. In our example above, *new_position = old_position + velocity&ast;time*. Next, we take the measurement from the sensor, and compare it to the prediction we just made. In a world with perfect sensors and perfect airplanes the prediction will always match the measured value. In the real world they will always be at least slightly different. We call the difference between the two the **residual**. Finally, we use something called the **Kalman gain** to update our estimate to be somewhere between the measured position and the predicted position. I will not describe how the gain is set, but suppose we had perfect confidence in our measurement - no error is possible. Then, clearly, we would set the gain so that 100% of the position came from the measurement, and 0% from the prediction. At the other extreme, if he have no confidence at all in the sensor (maybe it reported a hardware fault), we would set the gain so that 100% of the position came from the prediction, and 0% from the measurement. In normal cases, we will take a ratio of the two: maybe 53% of the measurement, and 47% of the prediction. The gain is updated on every cycle based on the variance of the variables (in a way yet to be explained). It should be clear that if the variance of the measurement is low, and the variance of the prediction is high we will favor the measurement, and vice versa. \n",
      "\n",
      "The chart shows a prior estimate of $x=1$ and $\\dot{x}=1$ ($\\dot{x}$ is the shorthand for the derivative of x, which is velocity). Therefore we predict $\\hat{x}=2$. However, the new measurement $x^{'}=1.3$, giving a residual $r=0.7$. Finally, Kalman filter gain $k$ gives us a new estimate of $\\hat{x^{'}}=1.8$.\n",
      "\n",
      "** CHECK SYMBOLOGY!!!!**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from mkf_internal import *\n",
      "show_residual_chart()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cov = np.array([[7,4],[4,7.]])\n",
      "mu = np.array([0,0])\n",
      "x = np.array([0,0])\n",
      "print(multivariate_gaussian(x,mu,cov))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#format the book\n",
      "from IPython.core.display import HTML\n",
      "def css_styling():\n",
      "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
      "    return HTML(styles)\n",
      "css_styling()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}