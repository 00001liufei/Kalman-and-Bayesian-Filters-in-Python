{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Table of Contents](http://nbviewer.ipython.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/table_of_contents.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kalman Filter Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('http://fonts.googleapis.com/css?family=Source+Code+Pro');\n",
       "@import url('http://fonts.googleapis.com/css?family=Vollkorn');\n",
       "@import url('http://fonts.googleapis.com/css?family=Arimo');\n",
       "@import url('http://fonts.googleapis.com/css?family=Fira_sans');\n",
       "\n",
       ".CodeMirror pre {\n",
       "    font-family: 'Source Code Pro', Consolas, monocco, monospace;\n",
       "}\n",
       "    div.cell{\n",
       "        width: 900px;\n",
       "        margin-left: 0% !important;\n",
       "        margin-right: auto;\n",
       "    }\n",
       "    div.text_cell code {\n",
       "        background: transparent;\n",
       "        color: #000000;\n",
       "        font-weight: 600;\n",
       "        font-size: 11pt;\n",
       "        font-style: bold;\n",
       "        font-family:  'Source Code Pro', Consolas, monocco, monospace;\n",
       "   }\n",
       "    h1 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "\t}\n",
       "\t\n",
       "    div.input_area {\n",
       "        background: #F6F6F9;\n",
       "        border: 1px solid #586e75;      \n",
       "    }\n",
       "\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 30pt;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h2 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "        text-align: left;\n",
       "    }\n",
       "    .text_cell_render h2 {\n",
       "        font-weight: 200;\n",
       "        font-size: 16pt;\n",
       "        font-style: italic;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1.5em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h3 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h3 {\n",
       "        font-weight: 200;\n",
       "        font-size: 14pt;\n",
       "        line-height: 100%;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 2em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    }\n",
       "    h4 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h4 {\n",
       "        font-weight: 100;\n",
       "        font-size: 14pt;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    h5 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 200;\n",
       "        font-style: normal;\n",
       "        color: #1d3b84;\n",
       "        font-size: 16pt;\n",
       "        margin-bottom: 0em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    div.text_cell_render{\n",
       "        font-family: 'Fira sans', verdana,arial,sans-serif;\n",
       "        line-height: 150%;\n",
       "        font-size: 110%;\n",
       "        font-weight: 400;\n",
       "        text-align:justify;\n",
       "        text-justify:inter-word;\n",
       "    }\n",
       "    div.output_subarea.output_text.output_pyout {\n",
       "        overflow-x: auto;\n",
       "        overflow-y: scroll;\n",
       "        max-height: 50000px;\n",
       "    }\n",
       "    div.output_subarea.output_stream.output_stdout.output_text {\n",
       "        overflow-x: auto;\n",
       "        overflow-y: scroll;\n",
       "        max-height: 50000px;\n",
       "    }\n",
       "    div.output_wrapper{\n",
       "        margin-top:0.2em;\n",
       "        margin-bottom:0.2em;\n",
       "}\n",
       "\n",
       "    code{\n",
       "        font-size: 6pt;\n",
       "\n",
       "    }\n",
       "    .rendered_html code{\n",
       "    background-color: transparent;\n",
       "    }\n",
       "    ul{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li li{\n",
       "        padding-left: 0.2em; \n",
       "        margin-bottom: 0.2em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    ol{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ol li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    a:link{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:visited{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:hover{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:focus{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:active{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    .rendered_html :link {\n",
       "       text-decoration: underline; \n",
       "    }\n",
       "    .rendered_html :hover {\n",
       "       text-decoration: none; \n",
       "    }\n",
       "    .rendered_html :visited {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :focus {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :active {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "    } \n",
       "    hr {\n",
       "      color: #f3f3f3;\n",
       "      background-color: #f3f3f3;\n",
       "      height: 1px;\n",
       "    }\n",
       "    blockquote{\n",
       "      display:block;\n",
       "      background: #fcfcfc;\n",
       "      border-left: 5px solid #c76c0c;\n",
       "      font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "      width:680px;\n",
       "      padding: 10px 10px 10px 10px;\n",
       "      text-align:justify;\n",
       "      text-justify:inter-word;\n",
       "      }\n",
       "      blockquote p {\n",
       "        margin-bottom: 0;\n",
       "        line-height: 125%;\n",
       "        font-size: 100%;\n",
       "      }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"],\n",
       "                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    scale:95,\n",
       "                        availableFonts: [],\n",
       "                        preferredFont:null,\n",
       "                        webFont: \"TeX\",\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#format the book\n",
    "%matplotlib inline\n",
    "from __future__ import division, print_function\n",
    "from book_format import load_style\n",
    "load_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've gotten this far I hope that you are thinking that the Kalman filter's fearsome reputation is somewhat undeserved. Sure, I hand waved some equations away, but I hope implementation has been fairly straightforward for you. The underlying concept is quite straightforward - take two measurements, or a measurement and a prediction, and choose the output to be somewhere between the two. If you believe the measurement more your guess will be closer to the measurement, and if you believe the prediction is more accurate your guess will lie closer it it. That's not rocket science (little joke - it is exactly this math that got Apollo to the moon and back!). \n",
    "\n",
    "To be honest I have been choosing my problems carefully. For an arbitrary problem designing the Kalman filter matrices can be extremely difficult. I haven't been *too tricky*, though. Equations like Newton's equations of motion can be trivially computed for Kalman filter applications, and they make up the bulk of the kind of problems that we want to solve. \n",
    "\n",
    "I have illustrated the concepts with code and reasoning, not math. But there are topics that do require more mathematics than I have used so far. This chapter presents the math that you will need for the rest of the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling a Dynamic System\n",
    "\n",
    "A *dynamic system* is a physical system whose state (position, temperature, etc) evolves over time. Calculus is the math of changing values, so we use differential equations to model dynamic systems. Some systems cannot be modeled with differential equations, but we will not encounter those in this book.\n",
    "\n",
    "Modeling dynamic systems is properly the topic of several college courses. To an extent there is no substitute for a few semesters of ordinary and partial differential equations followed by a graduate course in control system theory. If you are a hobbyist, or trying to solve one very specific filtering problem at work you probably do not have the time and/or inclination to devote a year or more to that education.\n",
    "\n",
    "Fortunately, I can present enough of the theory to allow us to create the system equations for many different Kalman filters. My goal is to get you to the stage where you can read a publication and understand it well enough to implement the algorithms. The background math is deep, but in practice we end up using a few simple techniques over and over again.\n",
    "\n",
    "We need to start by understanding the underlying equations and assumptions that the Kalman filter uses. We are trying to model real world phenomena, so what do we have to consider?\n",
    "\n",
    "Each physical system has a process. For example, a car traveling at a certain velocity goes so far in a fixed amount of time, and its velocity varies as a function of its acceleration. We describe that behavior with the well known Newtonian equations that we learned in high school.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v&=at\\\\\n",
    "x &= \\frac{1}{2}at^2 + v_0t + x_0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Once we learned calculus we saw them in this form:\n",
    "\n",
    "$$ \\mathbf v = \\frac{d \\mathbf x}{d t}, \n",
    "\\quad \\mathbf a = \\frac{d \\mathbf v}{d t} = \\frac{d^2 \\mathbf x}{d t^2}\n",
    "$$\n",
    " \n",
    "Perfectly modeling a system is impossible except for the most trivial problems. A typical automobile tracking problem would have you compute the distance traveled given a constant velocity or acceleration. But, of course we know this is not all that is happening. No car travels on a perfect road. There are bumps that cause the car to slow down, there is wind drag, there are hills that raise and lower the speed. The suspension is a mechanical system with friction and imperfect springs. Gusts of wind alter the car's state.\n",
    "\n",
    "So control theory is forced to make a simplification. At any time $t$ we say that the true state (such as the position of our car) is the predicted value from the imperfect model plus some unknown *process noise*:\n",
    "\n",
    "$$\n",
    "x(t) = x_{pred}(t) + noise(t)\n",
    "$$\n",
    "\n",
    "This is not meant to imply that $noise(t)$ is a function that we can derive analytically. It is merely a statement of fact - we can always describe the true value as the predicted value  plus the process noise. \"Noise\" does not imply random events. If we are tracking a thrown ball in the atmosphere, and our model assumes the ball is in a vacuum, then the effect of air drag is process noise in this context.\n",
    "\n",
    "In the next section we will learn techniques to convert a set of differential equations into a set of first-order differential equations.  Assuming that, we can say that our model of the system without noise is:\n",
    "\n",
    "$$ \\dot{\\mathbf x} = \\mathbf{Ax}$$\n",
    "\n",
    "That is, we have a set of linear equations that describe the dynamics of the system. Now we need to model the noise. We will call that $\\mathbf w$, and add it to the equation. \n",
    "\n",
    "$$ \\dot{\\mathbf x} = \\mathbf{Ax} + \\mathbf w$$\n",
    "\n",
    "$\\mathbf w$ may strike you as a poor choice for the name, but you will soon see that the Kalman filter assumes *white* noise.\n",
    "\n",
    "Finally, we need to consider any inputs into the system. We assume an input $\\mathbf u$, and that there exists a linear model that defines how that input changes the system. For example, pressing the accelerator in your car makes it accelerate, and gravity causes balls to fall. Both are contol inputs. We will need a matrix $\\mathbf B$ to convert $u$ into the effect on the system. We add that into our equation:\n",
    "\n",
    "$$ \\dot{\\mathbf x} = \\mathbf{Ax} + \\mathbf{Bu} + \\mathbf{w}$$\n",
    "\n",
    "And that's it. That is one of the equations that Dr. Kalman set out to solve, and he found an optimal esitmator if we assume certain properties of $\\mathbf w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State-Space Representation of Dynamic Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last section we derived the equation\n",
    "\n",
    "$$ \\dot{\\mathbf x} = \\mathbf{Ax}+ \\mathbf{Bu} + \\mathbf{w}$$.\n",
    "\n",
    "However, for our filters we are not interested in the derivative of $\\mathbf x$, but in $\\mathbf x$ itself. Ignoring the noise for a moment, we want an equation that recusively finds the value of $\\mathbf x$ at time $t_k$ in terms of $\\mathbf x$ at time $t_{k-1}$:\n",
    "\n",
    "$$\\mathbf x(t_k) = \\mathbf F(\\Delta t)\\mathbf x(t_{k-1}) + \\mathbf B(t_k) + \\mathbf u (t_k)$$\n",
    "\n",
    "Convention allows us to write $\\mathbf x(t_k)$ as $\\mathbf x_k$, which means the \n",
    "the value of $\\mathbf x$ at the k$^{th}$ value of $t$.\n",
    "\n",
    "$$\\mathbf x_k = \\mathbf{Fx}_{k-1} + \\mathbf B_k\\mathbf u_k$$\n",
    "\n",
    "$\\mathbf F$ is the familiar *state transition matrix*, named due to its ability to transition the state from the previous time step to the current time step.\n",
    "\n",
    "Normally finding this equation is quite difficult. The equation $\\dot x = v$ is the simplest possible differential equation and we trivially integrate it as:\n",
    "\n",
    "$$ \\int\\limits_{x_{k-1}}^{x_k}  \\mathrm{d}x = \\int\\limits_{0}^{\\Delta t} v\\, \\mathrm{d}t \\\\\n",
    "x_k-x_0 = v \\Delta t \\\\\n",
    "x_k = v \\Delta t + x_0$$\n",
    "\n",
    "This equation is *recursive*: we compute the value of $x$ at time $t$ based on its value at time $t-1$. This recursive form enables us to represent the system (process model) in the form required by the Kalman filter:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf x_k &= \\mathbf{Fx}_{k-1}  \\\\\n",
    "&= \\begin{bmatrix} 1 & \\Delta t \\\\ 0 & 1\\end{bmatrix}\n",
    "\\begin{bmatrix}x_{k-1} \\\\ \\dot x_{k-1}\\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "\n",
    "We can do that only because $\\dot x = v$ is simplest differential equation possible. Almost all other in physical systems result in more complicated differential equation which do not yield to this approach. \n",
    "\n",
    "*State-space* methods became popular around the time of the Apollo missions, largely due to the work of Dr. Kalman. The idea is simple. Model a system with a set of  $n^{th}$-order differential equations. Convert them into an equivalent set of first-order differential equations. Put them into the vector-matrix form used in the previous section: $ \\dot{\\mathbf x} = \\mathbf{Ax} + \\mathbf{Bu}$. Once in this form we use of of several techniques to convert these linear differential equations into the recursive equation:\n",
    "\n",
    "$$ \\mathbf x_k = \\mathbf{Fx}_{k-1} + \\mathbf B_k\\mathbf u_k$$\n",
    "\n",
    "Some books call the state transition matrix the *fundamental matrix*. Many use $\\mathbf \\Phi$ instead of $\\mathbf F$. Sources based heavily on control theory tend to use these forms.\n",
    "\n",
    "These are called *state-space* methods because we are expressing the solution of the differential equations in terms of the system state. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forming First Order Equations from Higher Order Equations\n",
    "\n",
    "Many models of physical systems require second or higher order differential equations with control input $u$:\n",
    "\n",
    "$$a_n \\frac{d^ny}{dt^n} + a_{n-1} \\frac{d^{n-1}y}{dt^{n-1}} +  \\dots + a_2 \\frac{d^2y}{dt^2} + a_1 \\frac{dy}{dt} + a_0 = u$$\n",
    "\n",
    "State-space methods require first-order equations. Any higher order system of equations can be reduced to first-order by defining extra variables for the derivatives and then solving. \n",
    "\n",
    "\n",
    "Let's do an example. Given the system $\\ddot{x} - 6\\dot x + 9x = t$ find the equivalent first order equations. I've used the dot notation for the time derivatives for clarity.\n",
    "\n",
    "The first step is to isolate the highest order term onto one side of the equation.\n",
    "\n",
    "$$\\ddot{x} = 6\\dot x - 9x + t$$\n",
    "\n",
    "We define two new variables:\n",
    "\n",
    "$$ x_1(t) = x \\\\\n",
    "x_2(t) = \\dot x\n",
    "$$\n",
    "\n",
    "Now we will substitute these into the original equation and solve. The solution yields a set of first-order equations in terms of these new variables. It is conventional to drop the $(t)$ for notational convenience.\n",
    "\n",
    "We know that $\\dot x_1 = x_2$ and that $\\dot x_2 = \\ddot{x}$. Therefore\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\dot x_2 &= \\ddot{x} \\\\\n",
    "          &= 6\\dot x - 9x + t\\\\\n",
    "          &= 6x_2-9x_1 + t\n",
    "\\end{aligned}$$\n",
    "\n",
    "Therefore our first-order system of equations is\n",
    "\n",
    "$$\\begin{aligned}\\dot x_1 &= x_2 \\\\\n",
    "\\dot x_2 &= 6x_2-9x_1 + t\\end{aligned}$$\n",
    "\n",
    "If you practice this a bit you will become adept at it. Isolate the highest term, define a new variable and its derivatives, and then substitute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Order Differential Equations In State-Space Form\n",
    "\n",
    "Substituting the newly defined variables from the previous section:\n",
    "\n",
    "$$\\frac{dx_1}{dt} = x_2,\\,  \n",
    "\\frac{dx_2}{dt} = x_3, \\, ..., \\, \n",
    "\\frac{dx_{n-1}}{dt} = x_n$$\n",
    "\n",
    "into the first order equations yields: \n",
    "\n",
    "$$\\frac{dx_n}{dt} = \\frac{1}{a_n}\\sum\\limits_{i=0}^{n-1}a_ix_{i+1} + \\frac{1}{a_n}u\n",
    "$$\n",
    "\n",
    "\n",
    "Using vector-matrix notation we have:\n",
    "\n",
    "$$\\begin{bmatrix}\\frac{dx_1}{dt} \\\\ \\frac{dx_2}{dt} \\\\ \\vdots \\\\ \\frac{dx_n}{dt}\\end{bmatrix} = \n",
    "\\begin{bmatrix}\\dot x_1 \\\\ \\dot x_2 \\\\ \\vdots \\\\ \\dot x_n\\end{bmatrix}=\n",
    "\\begin{bmatrix}0 & 1 & 0 &\\cdots & 0 \\\\\n",
    "0 & 0 & 1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "-\\frac{a_0}{a_n} & -\\frac{a_1}{a_n} & -\\frac{a_2}{a_n} & \\cdots & -\\frac{a_{n-1}}{a_n}\\end{bmatrix}\n",
    "\\begin{bmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\\end{bmatrix} + \n",
    "\\begin{bmatrix}0 \\\\ 0 \\\\ \\vdots \\\\ \\frac{1}{a_n}\\end{bmatrix}u$$\n",
    "\n",
    "which we then write as $\\dot{\\mathbf x} = \\mathbf{Ax} + \\mathbf{B}u$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Fundamental Matrix for Time Invariant Systems\n",
    "\n",
    "We express the system equations in state-space form with\n",
    "\n",
    "$$ \\dot{\\mathbf x} = \\mathbf{Ax}$$\n",
    "\n",
    "and want to find the *fundamental matrix* $\\mathbf F$ that propagates the state $\\mathbf x$ with the equation\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf x(t_k) = \\mathbf F(\\Delta t)\\mathbf x(t_{k-1})\\end{aligned}$$\n",
    "\n",
    "It is conventional to drop the $t_k$ and use the notation\n",
    "\n",
    "$$\\mathbf x_k = \\Phi(\\Delta t)\\mathbf x_{k-1}$$\n",
    "\n",
    "$\\mathbf x_k$ does not mean the k$^{th}$ value of $\\mathbf x$, but the value of $\\mathbf x$ at the k$^{th}$ value of $t$.\n",
    "\n",
    "Broadly speaking there are three common ways to find this matrix for Kalman filters. The technique most often used with Kalman filters is to use a Taylor-series expansion. Linear Time Invariant Theory, also known as LTI System Theory, is a second technique. Finally, there are numerical techniques. You may know of others, but these three are what you will most likely encounter in the Kalman filter literature and praxis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Matrix Exponential\n",
    "\n",
    "The solution to the equation $\\frac{dx}{dt} = kx$ can be found by:\n",
    "\n",
    "$$\\frac{dx}{dt} = kx \\\\\n",
    "\\frac{dx}{x} = k\\, dt \\\\\n",
    "\\int \\frac{1}{x}\\, dx = \\int k\\, dt \\\\\n",
    "\\log x = kt + c \\\\\n",
    "x = e^{kt+c} \\\\\n",
    "x = e^ce^{kt} \\\\\n",
    "x = c_0e^{kt}$$\n",
    "\n",
    "Using similar math, the solution to the first-order equation \n",
    "\n",
    "$$\\dot{\\mathbf x} = \\mathbf{Ax} ,\\, \\, \\, \\mathbf x(0) = \\mathbf x_0$$\n",
    "\n",
    "where $\\mathbf A$ is a constant matrix, is\n",
    "\n",
    "$$\\mathbf x = e^{\\mathbf At}\\mathbf x_0$$\n",
    "\n",
    "Substituting $F = e^{\\mathbf At}$, we can write \n",
    "\n",
    "$$\\mathbf x_k = \\mathbf F\\mathbf x_{k-1}$$\n",
    "\n",
    "which is the form we are looking for! We have reduced the problem of finding the fundamental matrix to one of finding the value for $e^{\\mathbf At}$.\n",
    "\n",
    "$e^{\\mathbf At}$ is known as the matrix exponential. It can be computed with this power series:\n",
    "\n",
    "$$e^{\\mathbf At} = \\mathbf{I} + \\mathbf{A}t  + \\frac{(\\mathbf{A}t)^2}{2!} + \\frac{(\\mathbf{A}t)^3}{3!} + ... $$\n",
    "\n",
    "That series is found by doing a Taylor series expansion of $e^{\\mathbf At}$, which I will not cover here.\n",
    "\n",
    "Let's use this to find the solution to Newton's equations. Using $v$ as an substitution for $\\dot x$, and assuming constant velocity we get the linear matrix-vector form \n",
    "\n",
    "$$\\begin{bmatrix}\\dot x \\\\ \\dot v\\end{bmatrix} =\\begin{bmatrix}0&1\\\\0&0\\end{bmatrix} \\begin{bmatrix}x \\\\ v\\end{bmatrix}$$\n",
    "\n",
    "This is a first order differential equation, so we can set $\\mathbf{A}=\\begin{bmatrix}0&1\\\\0&0\\end{bmatrix}$ and solve the following equation.\n",
    "\n",
    "$$\\mathbf F e^{\\mathbf At} = \\mathbf{I} + \\mathbf At  + \\frac{(\\mathbf At)^2}{2!} + \\frac{(\\mathbf At)^3}{3!} + ... $$\n",
    "\n",
    "If you perform the multiplication you will find that $\\mathbf{A}^2=\\begin{bmatrix}0&0\\\\0&0\\end{bmatrix}$, which means that all higher powers of $\\mathbf{A}$ are also $\\mathbf{0}$. Thus we get an exact answer without an infinite number of terms:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf F &=\\mathbf{I} + \\mathbf At + \\mathbf{0} \\\\\n",
    "&= \\begin{bmatrix}1&0\\\\0&1\\end{bmatrix} + \\begin{bmatrix}0&1\\\\0&0\\end{bmatrix}t\\\\\n",
    "&= \\begin{bmatrix}1&t\\\\0&1\\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "\n",
    "We plug this into $\\mathbf x_k= \\mathbf{Fx}_{k-1}$ to get\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_k &=\\begin{bmatrix}1&\\Delta t\\\\0&1\\end{bmatrix}x_{k-1}\n",
    "\\end{aligned}$$\n",
    "\n",
    "You will recognize this as the matrix we derived analytically for the constant velocity Kalman filter in the **Multivariate Kalman Filter** chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Invariance\n",
    "\n",
    "If the behavior of the system depends on time we can say that a dynamic system is described by the first-order differential equation\n",
    "\n",
    "$$ g(t) = \\dot x$$\n",
    "\n",
    "However, if the system is *time invariant* the equation is of the form:\n",
    "\n",
    "$$ f(x) = \\dot x$$\n",
    "\n",
    "What does *time invariant* mean? Consider a home stereo. If you input a signal $x$ into it at time $t$, it will output some signal $f(x)$. If you instead perform the input at time $t + \\Delta t$ the output signal will be the same $f(x)$, shifted in time.\n",
    "\n",
    "A counter-example is $x(t) = \\sin(t)$, with the system  $f(x) = t\\,  x(t) = t \\sin(t)$. This is not time invariant; the value will be different at different times due to the multiplication by t.  An aircraft is not time invariant. If you make a control input to the aircraft at a later time its behavior will be different because it will have burned fuel and thus lost weight. Lower weight results in different behavior.\n",
    "\n",
    "We can solve these equations by integrating each side. I demonstrated integrating the time invariants system $v = \\dot x$ above. However, integrating the time invariant equation $\\dot x = f(x)$ is not so straightforward. Using the *separation of variables* techniques we divide by $f(x)$ and move the $dt$ term to the right so we can integrate each side:\n",
    "\n",
    "$$\n",
    "\\frac{dx}{dt} = f(x) \\\\\n",
    "\\int^x_{x_0} \\frac{1}{f(x)} dx = \\int^t_{t_0} dt\\\\\n",
    "$$\n",
    "\n",
    "If we let $F(x) = \\int \\frac{1}{f(x)} dx$ we get\n",
    "\n",
    "$$F(x) - F(x_0) = t-t_0$$\n",
    "\n",
    "We then solve for x with\n",
    "\n",
    "$$F(x) = t - t_0 + F(x_0) \\\\\n",
    "x = F^{-1}[t-t_0 + F(x_0)]$$\n",
    "\n",
    "In other words, we need to find the inverse of $F$. This is not trivial, and a significant amount of coursework in a STEM education is devoted to finding tricky, analytic solutions to this problem. \n",
    "\n",
    "However, they are tricks, and many simple forms of $f(x)$ either have no closed form solution or pose extreme difficulties. Instead, the practicing engineer turns to state-space methods to find approximate solutions.\n",
    "\n",
    "The advantage of the matrix exponential is that we can use it for any arbitrary set of differential equations which are *time invariant*. However, we often use this technique even when the equations are not time invariant. As an aircraft flies it burns fuel and loses weight. However, the weight loss over one second is negligible, and so the system is nearly linear over that time step. Our answers will still be reasonably accurate so long as the time step is short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Mass-Spring-Damper Model\n",
    "\n",
    "Suppose we wanted to track the motion of a weight on a spring and connected to a damper, such as an automobile's suspension. The equation for the motion with $m$ being the mass, $k$ the spring constant, and $c$ the damping force, under some input $u$ is \n",
    "\n",
    "$$m\\frac{d^2x}{dt^2} + c\\frac{dx}{dt} +kx = u$$\n",
    "\n",
    "For notational convenience I will write that as\n",
    "\n",
    "$$m\\ddot x + c\\dot x + kx = u$$\n",
    "\n",
    "I can turn this into a system of first order equations by setting $x_1(t)=x(t)$, and then substituting as follows:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "x_1 &= x \\\\\n",
    "x_2 &= \\dot x_1 \\\\\n",
    "\\dot x_2 &= \\dot x_1 = \\ddot x\n",
    "\\end{aligned}$$\n",
    "\n",
    "As is common I dropped the $(t)$ for notational convenience. This gives the equation\n",
    "\n",
    "$$m\\dot x_2 + c x_2 +kx_1 = u$$\n",
    "\n",
    "Solving for $\\dot x_2$ we get a first order equation:\n",
    "\n",
    "$$\\dot x_2 = -\\frac{c}{m}x_2 - \\frac{k}{m}x_1 + \\frac{1}{m}u$$\n",
    "\n",
    "We put this into matrix form:\n",
    "\n",
    "$$\\begin{bmatrix} \\dot x_1 \\\\ \\dot x_2 \\end{bmatrix} = \n",
    "\\begin{bmatrix}0 & 1 \\\\ -k/m & -c/m \\end{bmatrix}\n",
    "\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} + \n",
    "\\begin{bmatrix} 0 \\\\ 1/m \\end{bmatrix}u$$\n",
    "\n",
    "Now we use the matrix exponential to find the state transition matrix:\n",
    "\n",
    "$$\\Phi(t) = e^{\\mathbf At} = \\mathbf{I} + \\mathbf At  + \\frac{(\\mathbf At)^2}{2!} + \\frac{(\\mathbf At)^3}{3!} + ... $$\n",
    "\n",
    "The first two terms give us\n",
    "\n",
    "$$\\mathbf F = \\begin{bmatrix}1 & t \\\\ -(k/m) t & 1-(c/m) t \\end{bmatrix}$$\n",
    "\n",
    "This may or may not give you enough precision. You can easily check this by computing $\\frac{(\\mathbf At)^2}{2!}$ for your constants and seeing how much this matrix contributes to the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Time Invariant Theory\n",
    "\n",
    "*Linear Time Invariant Theory*, also known as LTI System Theory, gives us a way to find $\\Phi$ using the inverse Laplace transform. You are either nodding your head now, or completely lost. Don't worry, I will not be using the Laplace transform in this book except in this paragraph, as the computation can be quite difficult. LTI system theory tells us that \n",
    "\n",
    "$$ \\Phi(t) = \\mathcal{L}^{-1}[(s\\mathbf{I} - \\mathbf{F})^{-1}]$$\n",
    "\n",
    "I have no intention of going into this other than to say that the Laplace transform $\\mathcal{L}$ converts a signal into a space $s$ that excludes time, but finding a solution to the equation above is non-trivial. If you are interested, the Wikipedia article on LTI system theory provides an introduction [2]. I mention LTI because you will find some literature using it to design the Kalman filter matrices for difficult problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Solutions\n",
    "\n",
    "Finally, there are numerous numerical techniques to find $\\mathbf F$. As filters get larger finding analytical solutions becomes very tedious (though packages like SymPy make it easier). C. F. van Loan [3] has developed a technique that finds both $\\Phi$ and $\\mathbf Q$ numerically. Given the continuous model\n",
    "\n",
    "$$ \\dot x = Ax + Gw$$\n",
    "\n",
    "where $w$ is the unity white noise, van Loan's method computes both $\\mathbf F_k$ and $\\mathbf Q_k$.\n",
    "    \n",
    "I have implemented van Loan's method in `FilterPy`. You may use it as follows:\n",
    "\n",
    "```python\n",
    "from filterpy.common import van_loan_discretization\n",
    "\n",
    "A = np.array([[0., 1.], [-1., 0.]])\n",
    "G = np.array([[0.], [2.]]) # white noise scaling\n",
    "F, Q = van_loan_discretization(A, G, dt=0.1)\n",
    "```\n",
    "    \n",
    "In the section *Numeric Integration of Differential Equations* I present alternative methods which are very commonly used in Kalman filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design of the Process Noise Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general the design of the $\\mathbf Q$ matrix is among the most difficult aspects of Kalman filter design. This is due to several factors. First, the math requires a good foundation in signal theory. Second, we are trying to model the noise in something for which we have little information. Consider trying to model the process noise for a thrown baseball. We can model it as a sphere moving through the air, but that leave many unknown factors - the wind, ball rotation and spin decay, the coefficient of drag of a ball with stitches, the effects of wind and air density, and so on. We develop the equations for an exact mathematical solution for a given process model, but since the process model is incomplete the result for $\\mathbf Q$ will also be incomplete. This has a lot of ramifications for the behavior of the Kalman filter. If $\\mathbf Q$ is too small then the filter will be overconfident in its prediction model and will diverge from the actual solution. If $\\mathbf Q$ is too large than the filter will be unduly influenced by the noise in the measurements and perform sub-optimally. In practice we spend a lot of time running simulations and evaluating collected data to try to select an appropriate value for $\\mathbf Q$. But let's start by looking at the math.\n",
    "\n",
    "\n",
    "Let's assume a kinematic system - some system that can be modeled using Newton's equations of motion. We can make a few different assumptions about this process. \n",
    "\n",
    "We have been using a process model of\n",
    "\n",
    "$$ \\dot{\\mathbf x} = \\mathbf{Ax} + \\mathbf{Bu} + \\mathbf{w}$$\n",
    "\n",
    "where $\\mathbf{w}$ is the process noise. Kinematic systems are *continuous* - their inputs and outputs can vary at any arbitrary point in time. However, our Kalman filters are *discrete*. We sample the system at regular intervals. Therefore we must find the discrete representation for the noise term in the equation above. This depends on what assumptions we make about the behavior of the noise. We will consider two different models for the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous White Noise Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model kinematic systems using Newton's equations. We have either used position and velocity, or position, velocity, and acceleration as the models for our systems. There is nothing stopping us from going further - we can model jerk, jounce, snap, and so on. We don't do that normally because adding terms beyond the dynamics of the real system degrades the estimate. \n",
    "\n",
    "Let's say that we need to model the position, velocity, and acceleration. We can then assume that acceleration is constant for each discrete time step. Of course, there is process noise in the system and so the acceleration is not actually constant. The tracked object will alter the acceleration over time due to external, unmodeled forces. In this section we will assume that the acceleration changes by a continuous time zero-mean white noise $w(t)$. In other words, we are assuming that velocity is acceleration changing by small amounts that over time average to 0 (zero-mean). \n",
    "\n",
    "Since the noise is changing continuously we will need to integrate to get the discrete noise for the discretization interval that we have chosen. We will not prove it here, but the equation for the discretization of the noise is\n",
    "\n",
    "$$\\mathbf Q = \\int_0^{\\Delta t} \\mathbf F(t)\\mathbf{Q_c}\\mathbf F^\\mathsf{T}(t) dt$$\n",
    "\n",
    "where $\\mathbf{Q_c}$ is the continuous noise. This gives us\n",
    "\n",
    "$$\\Phi = \\begin{bmatrix}1 & \\Delta t & {\\Delta t}^2/2 \\\\ 0 & 1 & \\Delta t\\\\ 0& 0& 1\\end{bmatrix}$$\n",
    "\n",
    "for the fundamental matrix, and\n",
    "\n",
    "$$\\mathbf{Q_c} = \\begin{bmatrix}0&0&0\\\\0&0&0\\\\0&0&1\\end{bmatrix} \\Phi_s$$\n",
    "\n",
    "for the continuous process noise matrix, where $\\Phi_s$ is the spectral density of the white noise.\n",
    "\n",
    "We could carry out these computations ourselves, but I prefer using SymPy to solve the equation.\n",
    "\n",
    "$$\\mathbf{Q_c} = \\begin{bmatrix}0&0&0\\\\0&0&0\\\\0&0&1\\end{bmatrix} \\Phi_s$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}\\frac{\\Delta{t}^{5}}{20} & \\frac{\\Delta{t}^{4}}{8} & \\frac{\\Delta{t}^{3}}{6}\\\\\\frac{\\Delta{t}^{4}}{8} & \\frac{\\Delta{t}^{3}}{3} & \\frac{\\Delta{t}^{2}}{2}\\\\\\frac{\\Delta{t}^{3}}{6} & \\frac{\\Delta{t}^{2}}{2} & \\Delta{t}\\end{matrix}\\right] \\Phi_{s}$$"
      ],
      "text/plain": [
       "⎡         5           4           3⎤       \n",
       "⎢\\Delta{t}   \\Delta{t}   \\Delta{t} ⎥       \n",
       "⎢──────────  ──────────  ──────────⎥⋅\\Phi_s\n",
       "⎢    20          8           6     ⎥       \n",
       "⎢                                  ⎥       \n",
       "⎢         4           3           2⎥       \n",
       "⎢\\Delta{t}   \\Delta{t}   \\Delta{t} ⎥       \n",
       "⎢──────────  ──────────  ──────────⎥       \n",
       "⎢    8           3           2     ⎥       \n",
       "⎢                                  ⎥       \n",
       "⎢         3           2            ⎥       \n",
       "⎢\\Delta{t}   \\Delta{t}             ⎥       \n",
       "⎢──────────  ──────────  \\Delta{t} ⎥       \n",
       "⎣    6           2                 ⎦       "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy\n",
    "from sympy import (init_printing, Matrix,MatMul, \n",
    "                   integrate, symbols)\n",
    "\n",
    "init_printing(use_latex='mathjax')\n",
    "dt, phi = symbols('\\Delta{t} \\Phi_s')\n",
    "F_k = Matrix([[1, dt, dt**2/2],\n",
    "              [0,  1,      dt],\n",
    "              [0,  0,       1]])\n",
    "Q_c = Matrix([[0, 0, 0],\n",
    "              [0, 0, 0],\n",
    "              [0, 0, 1]])*phi\n",
    "\n",
    "Q=sympy.integrate(F_k * Q_c * F_k.T, (dt, 0, dt))\n",
    "\n",
    "# factor phi out of the matrix to make it more readable\n",
    "Q = Q / phi\n",
    "sympy.MatMul(Q, phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness, let us compute the equations for the 0th order and 1st order equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th order discrete process noise\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}\\Delta{t} \\Phi_{s}\\end{matrix}\\right]$$"
      ],
      "text/plain": [
       "[\\Delta{t}⋅\\Phi_s]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_k = sympy.Matrix([[1]])\n",
    "Q_c = sympy.Matrix([[phi]])\n",
    "\n",
    "print('0th order discrete process noise')\n",
    "sympy.integrate(F_k*Q_c*F_k.T,(dt, 0, dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st order discrete process noise\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}\\frac{\\Delta{t}^{3}}{3} & \\frac{\\Delta{t}^{2}}{2}\\\\\\frac{\\Delta{t}^{2}}{2} & \\Delta{t}\\end{matrix}\\right] \\Phi_{s}$$"
      ],
      "text/plain": [
       "⎡         3           2⎤       \n",
       "⎢\\Delta{t}   \\Delta{t} ⎥       \n",
       "⎢──────────  ──────────⎥⋅\\Phi_s\n",
       "⎢    3           2     ⎥       \n",
       "⎢                      ⎥       \n",
       "⎢         2            ⎥       \n",
       "⎢\\Delta{t}             ⎥       \n",
       "⎢──────────  \\Delta{t} ⎥       \n",
       "⎣    2                 ⎦       "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_k = sympy.Matrix([[1, dt],\n",
    "                    [0, 1]])\n",
    "Q_c = sympy.Matrix([[0, 0],\n",
    "                    [0, 1]])*phi\n",
    "\n",
    "Q = sympy.integrate(F_k * Q_c * F_k.T, (dt, 0, dt))\n",
    "\n",
    "print('1st order discrete process noise')\n",
    "# factor phi out of the matrix to make it more readable\n",
    "Q = Q / phi\n",
    "sympy.MatMul(Q, phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Piecewise White Noise Model\n",
    "\n",
    "Another model for the noise assumes that the that highest order term (say, acceleration) is constant for the duration of each time period, but differs for each time period, and each of these is uncorrelated between time periods. In other words there is a discontinuous jump in acceleration at each time step. This is subtly different than the model above, where we assumed that the last term had a continuously varying noisy signal applied to it.  \n",
    "\n",
    "We will model this as\n",
    "\n",
    "$$f(x)=Fx+\\Gamma w$$\n",
    "\n",
    "where $\\Gamma$ is the *noise gain* of the system, and $w$ is the constant piecewise acceleration (or velocity, or jerk, etc). \n",
    "\n",
    "Lets start by looking at a first order system. In this case we have the state transition function\n",
    "\n",
    "$$\\mathbf{F} = \\begin{bmatrix}1&\\Delta t \\\\ 0& 1\\end{bmatrix}$$\n",
    "\n",
    "In one time period, the change in velocity will be $w(t)\\Delta t$, and the change in position will be $w(t)\\Delta t^2/2$, giving us\n",
    "\n",
    "$$\\Gamma = \\begin{bmatrix}\\frac{1}{2}\\Delta t^2 \\\\ \\Delta t\\end{bmatrix}$$\n",
    "\n",
    "The covariance of the process noise is then\n",
    "\n",
    "$$Q = \\mathbb E[\\Gamma w(t) w(t) \\Gamma^\\mathsf{T}] = \\Gamma\\sigma^2_v\\Gamma^\\mathsf{T}$$.\n",
    "\n",
    "We can compute that with SymPy as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}\\frac{\\Delta{t}^{4}}{4} & \\frac{\\Delta{t}^{3}}{2}\\\\\\frac{\\Delta{t}^{3}}{2} & \\Delta{t}^{2}\\end{matrix}\\right] \\sigma^{2}_{v}$$"
      ],
      "text/plain": [
       "⎡         4           3⎤    \n",
       "⎢\\Delta{t}   \\Delta{t} ⎥    \n",
       "⎢──────────  ──────────⎥⋅σ²ᵥ\n",
       "⎢    4           2     ⎥    \n",
       "⎢                      ⎥    \n",
       "⎢         3            ⎥    \n",
       "⎢\\Delta{t}            2⎥    \n",
       "⎢──────────  \\Delta{t} ⎥    \n",
       "⎣    2                 ⎦    "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var=symbols('sigma^2_v')\n",
    "v = Matrix([[dt**2 / 2], [dt]])\n",
    "\n",
    "Q = v * var * v.T\n",
    "\n",
    "# factor variance out of the matrix to make it more readable\n",
    "Q = Q / var\n",
    "sympy.MatMul(Q, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second order system proceeds with the same math.\n",
    "\n",
    "\n",
    "$$\\mathbf{F} = \\begin{bmatrix}1 & \\Delta t & {\\Delta t}^2/2 \\\\ 0 & 1 & \\Delta t\\\\ 0& 0& 1\\end{bmatrix}$$\n",
    "\n",
    "Here we will assume that the white noise is a discrete time Wiener process. This gives us\n",
    "\n",
    "$$\\Gamma = \\begin{bmatrix}\\frac{1}{2}\\Delta t^2 \\\\ \\Delta t\\\\ 1\\end{bmatrix}$$\n",
    "\n",
    "There is no 'truth' to this model, it is just convenient and provides good results. For example, we could assume that the noise is applied to the jerk at the cost of a more complicated equation. \n",
    "\n",
    "The covariance of the process noise is then\n",
    "\n",
    "$$Q = \\mathbb E[\\Gamma w(t) w(t) \\Gamma^\\mathsf{T}] = \\Gamma\\sigma^2_v\\Gamma^\\mathsf{T}$$.\n",
    "\n",
    "We can compute that with SymPy as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\\left[\\begin{matrix}\\frac{\\Delta{t}^{4}}{4} & \\frac{\\Delta{t}^{3}}{2} & \\frac{\\Delta{t}^{2}}{2}\\\\\\frac{\\Delta{t}^{3}}{2} & \\Delta{t}^{2} & \\Delta{t}\\\\\\frac{\\Delta{t}^{2}}{2} & \\Delta{t} & 1\\end{matrix}\\right] \\sigma^{2}_{v}$$"
      ],
      "text/plain": [
       "⎡         4           3           2⎤    \n",
       "⎢\\Delta{t}   \\Delta{t}   \\Delta{t} ⎥    \n",
       "⎢──────────  ──────────  ──────────⎥⋅σ²ᵥ\n",
       "⎢    4           2           2     ⎥    \n",
       "⎢                                  ⎥    \n",
       "⎢         3                        ⎥    \n",
       "⎢\\Delta{t}            2            ⎥    \n",
       "⎢──────────  \\Delta{t}   \\Delta{t} ⎥    \n",
       "⎢    2                             ⎥    \n",
       "⎢                                  ⎥    \n",
       "⎢         2                        ⎥    \n",
       "⎢\\Delta{t}                         ⎥    \n",
       "⎢──────────  \\Delta{t}       1     ⎥    \n",
       "⎣    2                             ⎦    "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var=symbols('sigma^2_v')\n",
    "v = Matrix([[dt**2 / 2], [dt], [1]])\n",
    "\n",
    "Q = v * var * v.T\n",
    "\n",
    "# factor variance out of the matrix to make it more readable\n",
    "Q = Q / var\n",
    "sympy.MatMul(Q, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot say that this model is more or less correct than the continuous model - both are approximations to what is happening to the actual object. Only experience and experiments can guide you to the appropriate model. In practice you will usually find that either model provides reasonable results, but typically one will perform better than the other.\n",
    "\n",
    "The advantage of the second model is that we can model the noise in terms of $\\sigma^2$ which we can describe in terms of the motion and the amount of error we expect. The first model requires us to specify the spectral density, which is not very intuitive, but it handles varying time samples much more easily since the noise is integrated across the time period. However, these are not fixed rules - use whichever model (or a model of your own devising) based on testing how the filter performs and/or your knowledge of the behavior of the physical model.\n",
    "\n",
    "A good rule of thumb is to set $\\sigma$ somewhere from $\\frac{1}{2}\\Delta a$ to $\\Delta a$, where $\\Delta a$ is the maximum amount that the acceleration will change between sample periods. In practice we pick a number, run simulations on data, and choose a value that works well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using FilterPy to Compute Q\n",
    "\n",
    "FilterPy offers several routines to compute the $\\mathbf Q$ matrix. The function `Q_continuous_white_noise()` computes $\\mathbf Q$ for a given value for $\\Delta t$ and the spectral density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.333  0.5  ]\n",
      " [ 0.5    1.   ]]\n"
     ]
    }
   ],
   "source": [
    "from filterpy.common import Q_continuous_white_noise\n",
    "from filterpy.common import Q_discrete_white_noise\n",
    "\n",
    "Q = Q_continuous_white_noise(dim=2, dt=1, spectral_density=1)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.05   0.125  0.167]\n",
      " [ 0.125  0.333  0.5  ]\n",
      " [ 0.167  0.5    1.   ]]\n"
     ]
    }
   ],
   "source": [
    "Q = Q_continuous_white_noise(dim=3, dt=1, spectral_density=1)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `Q_discrete_white_noise()` computes $\\mathbf Q$ assuming a piecewise model for the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25  0.5 ]\n",
      " [ 0.5   1.  ]]\n"
     ]
    }
   ],
   "source": [
    "Q = Q_discrete_white_noise(2, var=1.)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25  0.5   0.5 ]\n",
      " [ 0.5   1.    1.  ]\n",
      " [ 0.5   1.    1.  ]]\n"
     ]
    }
   ],
   "source": [
    "Q = Q_discrete_white_noise(3, var=1.)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplification of Q\n",
    "\n",
    "Many treatments use a much simpler form for $\\mathbf Q$, setting it to zero except for a noise term in the lower rightmost element. Is this justified? Well, consider the value of $\\mathbf Q$ for a small $\\Delta t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000002  0.00000078  0.00002083]\n",
      " [ 0.00000078  0.00004167  0.00125   ]\n",
      " [ 0.00002083  0.00125     0.05      ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=8)\n",
    "Q = Q_continuous_white_noise(\n",
    "    dim=3, dt=0.05, spectral_density=1)\n",
    "print(Q)\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most of the terms are very small. Recall that the only Kalman filter using this matrix is\n",
    "\n",
    "$$ \\mathbf P=\\mathbf{FPF}^\\mathsf{T} + \\mathbf Q$$\n",
    "\n",
    "If the values for $\\mathbf Q$ are small relative to $\\mathbf P$\n",
    "than it will be contributing almost nothing to the computation of $\\mathbf P$. Setting $\\mathbf Q$ to the zero matrix except for the lower right term\n",
    "\n",
    "$$\\mathbf Q=\\begin{bmatrix}0&0&0\\\\0&0&0\\\\0&0&\\sigma^2\\end{bmatrix}$$\n",
    "\n",
    "while not correct, is often a useful approximation. If you do this you will have to perform quite a few studies to guarantee that your filter works in a variety of situations. \n",
    "\n",
    "If you do this, 'lower right term' means the most rapidly changing term for each variable. If the state is $x=\\begin{bmatrix}x & \\dot x & \\ddot{x} & y & \\dot{y} & \\ddot{y}\\end{bmatrix}^\\mathsf{T}$ Then Q will be 6x6; the elements for both $\\ddot{x}$ and $\\ddot{y}$ will have to be set to non-zero in $\\mathbf Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Integration of Differential Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've been exposed to several numerical techniques to solve linear differential equations. These include state-space methods, the Laplace transform, and van Loan's method. \n",
    "\n",
    "These work well for linear ordinary differential equations (ODEs), but do not work well for nonlinear equations. For example, consider trying to predict the position of a rapidly turning car. Cars maneuver by turning the front wheels. This makes them pivot around their rear axle as it moves forward. Therefore the path will be continuously varying and a linear prediction will necessarily produce an incorrect value. If the change in the system is small enough relative to $\\Delta t$ this can often produce adequate results, but that will rarely be the case with the nonlinear Kalman filters we will be studying in subsequent chapters. \n",
    "\n",
    "For these reasons we need to know how to numerically integrate ODEs. This can be a vast topic that requires several books. If you need to explore this topic in depth *Computational Physics in Python* by Dr. Eric Ayars is excellent, and available for free here:\n",
    "\n",
    "http://phys.csuchico.edu/ayars/312/Handouts/comp-phys-python.pdf\n",
    "\n",
    "However, I will cover a few simple techniques which will work for a majority of the problems you encounter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euler's Method\n",
    "\n",
    "Let's say we have the initial condition problem of \n",
    "\n",
    "$$ y' = y, \\\\ y(0) = 1$$\n",
    "\n",
    "We happen to know the exact answer is $y=e^t$ because we solved it earlier, but for an arbitrary ODE we will not know the exact solution. In general all we know is the derivative of the equation, which is equal to the slope. We also know the initial value: at $t=0$, $y=1$. If we know these two pieces of information we can predict the value at $y(t=1)$ using the slope at $t=0$ and the value of $y(0)$. I've plotted this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "t = np.linspace(-1, 1, 10)\n",
    "plt.plot(t, np.exp(t))\n",
    "t = np.linspace(-1, 1, 2)\n",
    "plt.plot(t,t+1, ls='--', c='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the slope is very close to the curve at $t=0.1$, but far from it\n",
    "at $t=1$. But let's continue with a step size of 1 for a moment. We can see that at $t=1$ the estimated value of $y$ is 2. Now we can compute the value at $t=2$ by taking the slope of the curve at $t=1$ and adding it to our initial estimate. The slope is computed with $y'=y$, so the slope is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import book_plots\n",
    "t = np.linspace(-1, 2, 20)\n",
    "plt.plot(t, np.exp(t))\n",
    "t = np.linspace(0, 1, 2)\n",
    "plt.plot([1, 2, 4], ls='--', c='k')\n",
    "book_plots.set_labels(x='x', y='y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the next estimate for y is 4. The errors are getting large quickly, and you might be unimpressed. But 1 is a very large step size. Let's put this algorithm in code, and verify that it works by using a small step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euler(t, tmax, y, dx, step=1.):\n",
    "    ys = []\n",
    "    while t < tmax:\n",
    "        y = y + step*dx(t, y)\n",
    "        ys.append(y)\n",
    "        t +=step        \n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dx(t, y): return y\n",
    "\n",
    "print(euler(0, 1, 1, dx, step=1.)[-1])\n",
    "print(euler(0, 2, 1, dx, step=1.)[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks correct. So now lets plot the result of a much smaller step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ys = euler(0, 4, 1, dx, step=0.00001)\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Computed')\n",
    "plt.plot(np.linspace(0, 4, len(ys)),ys)\n",
    "plt.subplot(1,2,2)\n",
    "t = np.linspace(0, 4, 20)\n",
    "plt.title('Exact')\n",
    "plt.plot(t, np.exp(t));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('exact answer=', np.exp(4))\n",
    "print('euler answer=', ys[-1])\n",
    "print('difference =', np.exp(4) - ys[-1])\n",
    "print('iterations =', len(ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the error is reasonably small, but it took a very large number of iterations to get three digits of precision. In practice Euler's method is too slow for most problems, and we use more sophisticated methods.\n",
    "\n",
    "Before we go on, let's formally derive Euler's method, as it is the basis for the more advanced Runge Kutta methods used in the next section. In fact, Euler's method is the simplest form of Runge Kutta.\n",
    "\n",
    "\n",
    "Here are the first 3 terms of the Euler expansion of $y$. An infinite expansion would give an exact answer, so $O(h^4)$ denotes the error due to the finite expansion.\n",
    "\n",
    "$$y(t_0 + h) = y(t_0) + h y'(t_0) + \\frac{1}{2!}h^2 y''(t_0) + \\frac{1}{3!}h^3 y'''(t_0) +  O(h^4)$$\n",
    "\n",
    "Here we can see that Euler's method is using the first two terms of the Taylor expansion. Each subsequent term is smaller than the previous terms, so we are assured that the estimate will not be too far off from the correct value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runge Kutta Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Runge Kutta integration is the workhorse of numerical integration. There are a vast number of methods in the literature. In practice, using the Runge Kutta algorithm that I present here will solve most any problem you will face. It offers a very good balance of speed, precision, and stability, and it is the 'go to' numerical integration method unless you have a very good reason to choose something different.\n",
    "\n",
    "Let's dive in. We start with some differential equation\n",
    "\n",
    "$$\\ddot{y} = \\frac{d}{dt}\\dot{y}$$.\n",
    "\n",
    "We can substitute the derivative of y with a function f, like so\n",
    "\n",
    "$$\\ddot{y} = \\frac{d}{dt}f(y,t)$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deriving these equations is outside the scope of this book, but the Runge Kutta RK4 method is defined with these equations.\n",
    "\n",
    "$$y(t+\\Delta t) = y(t) + \\frac{1}{6}(k_1 + 2k_2 + 2k_3 + k_4) + O(\\Delta t^4)$$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "k_1 &= f(y,t)\\Delta t \\\\\n",
    "k_2 &= f(y+\\frac{1}{2}k_1, t+\\frac{1}{2}\\Delta t)\\Delta t \\\\\n",
    "k_3 &= f(y+\\frac{1}{2}k_2, t+\\frac{1}{2}\\Delta t)\\Delta t \\\\\n",
    "k_4 &= f(y+k_3, t+\\Delta t)\\Delta t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here is the corresponding code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runge_kutta4(y, x, dx, f):\n",
    "    \"\"\"computes 4th order Runge-Kutta for dy/dx.\n",
    "    y is the initial value for y\n",
    "    x is the initial value for x\n",
    "    dx is the difference in x (e.g. the time step)\n",
    "    f is a callable function (y, x) that you supply \n",
    "    to compute dy/dx for the specified values.\n",
    "    \"\"\"\n",
    "    \n",
    "    k1 = dx * f(y, x)\n",
    "    k2 = dx * f(y + 0.5*k1, x + 0.5*dx)\n",
    "    k3 = dx * f(y + 0.5*k2, x + 0.5*dx)\n",
    "    k4 = dx * f(y + k3, x + dx)\n",
    "    \n",
    "    return y + (k1 + 2*k2 + 2*k3 + k4) / 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this for a simple example. Let\n",
    "\n",
    "$$\\dot{y} = t\\sqrt{y(t)}$$\n",
    "\n",
    "with the initial values\n",
    "\n",
    "$$\\begin{aligned}t_0 &= 0\\\\y_0 &= y(t_0) = 1\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "t = 0.\n",
    "y = 1.\n",
    "dt = .1\n",
    "\n",
    "ys, ts = [], []\n",
    "\n",
    "def func(y,t):\n",
    "    return t*math.sqrt(y)\n",
    "\n",
    "while t <= 10:\n",
    "    y = runge_kutta4(y, t, dt, func)\n",
    "    t += dt\n",
    "    ys.append(y)\n",
    "    ts.append(t)\n",
    "\n",
    "exact = [(t**2 + 4)**2 / 16. for t in ts]\n",
    "plt.plot(ts, ys)\n",
    "plt.plot(ts, exact)\n",
    "\n",
    "error = np.array(exact) - np.array(ys)\n",
    "print(\"max error {}\".format(max(error)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearizing with the Taylor Series\n",
    "\n",
    "Taylor series represents a function as an infinite sum of terms. The terms are linear, even for a nonlinear function, so we can express any arbitrary nonlinear function using linear algebra. The cost of this choice is that unless we use an infinite number of terms the value we compute will be approximate rather than exact.\n",
    "\n",
    "Before applying it to a matrix lets do the Taylor expansion of a real function since this is much easier to visualize. I choose sin(x). The Taylor series for a real or complex function f(x) at x=a is the infinite series\n",
    "\n",
    "$$f(x) = f(a) + f'(a)(x-a) + \\frac{f''(a)}{2!}(x-a)^2 + \\, ...\\,  + \\frac{f^{(n)}(a)}{n!}(x-a)^n + \\, ...$$\n",
    "\n",
    "where $f^{n}$ is the nth derivative of f. To compute the Taylor series for $f(x)=sin(x)$ at $x=0$ Let's first work out the terms for f.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "f^{0}(x) &= sin(x) ,\\ \\  &f^{0}(0) &= 0 \\\\\n",
    "f^{1}(x) &= cos(x),\\ \\  &f^{1}(0) &= 1 \\\\\n",
    "f^{2}(x) &= -sin(x),\\ \\  &f^{2}(0) &= 0 \\\\\n",
    "f^{3}(x) &= -cos(x),\\ \\  &f^{3}(0) &= -1 \\\\\n",
    "f^{4}(x) &= sin(x),\\ \\  &f^{4}(0) &= 0 \\\\\n",
    "f^{5}(x) &= cos(x),\\ \\  &f^{5}(0) &= 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now we can substitute these values into the equation.\n",
    "\n",
    "$$\\sin(x) = \\frac{0}{0!}(x)^0 + \\frac{1}{1!}(x)^1 + \\frac{0}{2!}(x)^2 + \\frac{-1}{3!}(x)^3 + \\frac{0}{4!}(x)^4 + \\frac{-1}{5!}(x)^5 + ... $$\n",
    "\n",
    "And let's test this with some code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate of sin(.3) is 0.30452025\n",
      "exact value of sin(.3) is 0.295520206661\n"
     ]
    }
   ],
   "source": [
    "x = .3\n",
    "estimate = x + x**3/6 + x**5/120\n",
    "exact = np.sin(.3)\n",
    "\n",
    "print('estimate of sin(.3) is', estimate)\n",
    "print('exact value of sin(.3) is', exact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not bad for only three terms. If you are curious, go ahead and implement this as a Python function to compute the series for an arbitrary number of terms.\n",
    "\n",
    "Now we can consider how to linearize a nonlinear "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Filtering\n",
    "\n",
    "Starting in the Discrete Bayes chapter I used a Bayesian formulation for filtering. Traditionally, Kalman filters use a least-squares formulation. **blah**\n",
    "\n",
    "Suppose we are tracking an object. We define its *state* at a specific time as its position, velocity, and so on. For example, we might write the state at time $t$ as $\\mathbf x_t = \\begin{bmatrix}x_t &\\dot x_t \\end{bmatrix}^\\mathsf T$. \n",
    "\n",
    "When we take a measurement of the object we are measuring the state. Sensors are noisy, so the measurement is corrupted with noise. Clearly though, the measurement is determined by the state. That is, the object's state determines the measurement, the measurement does not determine the state. The state is the *input*, the measurement is the *output*.\n",
    "\n",
    "In filtering we are presented a set of measurements from time 0 to time $t$, $\\mathbf z_{0:t}$,  and want to compute the corresponding states for those times, $\\mathbf x_{0:t}$. This is called *statistical inversion* because we are trying to compute the input from the output. \n",
    "\n",
    "Recall Bayes Theorem:\n",
    "\n",
    "$$P(x \\mid z) = \\frac{P(z \\mid x)P(x)}{P(z)}$$\n",
    "\n",
    "where $P(z \\mid x)$ is the *likelihood* of the measurement $z$, $P(x)$ is the *prior* based on our process model, and $P(z)$ is a normalization constant. $P(x \\mid z)$ is the *posterior*, or the distribution after incorporating the measurement $z$, also called the *evidence*.\n",
    "\n",
    "This is a *statistical inversion* as it goes from $P(z \\mid x)$ to $P(x \\mid z)$. The solution to our filtering problem can be expressed as:\n",
    "\n",
    "$$P(\\mathbf x_{0:t} \\mid \\mathbf z_{0:t}) = \\frac{P(\\mathbf z_{0:t} \\mid \\mathbf x_{0:t})P(\\mathbf x_{0:t})}{P(\\mathbf z_{0:t})}$$\n",
    "\n",
    "That is all well and good until the next measurement $\\mathbf z_{t+1}$ comes in, at which point we need to recompute the entire expression for the range $0:t+1$. \n",
    "\n",
    "\n",
    "In practice this is intractable because we are trying to compute the posterior distribution $P(\\mathbf x_{0:t} \\mid \\mathbf z_{0:t})$ for the state over the full range of time steps. But do we really care about the probability distribution at the third step (say) when we just received the tenth measurement? Not usually. So we relax our requirements and only compute the distributions for the current time step.\n",
    "\n",
    "The first simplification is we describe our process (e.g., the motion model for a moving object) as a *Markov chain*. That is, we say that the current state is solely dependent on the previous state and a transition probability $P(\\mathbf x_k \\mid \\mathbf x_{k-1})$, which is just the probability of going from the last state to the current one. We write:\n",
    "\n",
    "$$\\mathbf x_k \\sim P(\\mathbf x_k \\mid \\mathbf x_{k-1})$$\n",
    "\n",
    "The next simplification we make is do define the *measurement model* as depending on the current state $\\mathbf x_k$ with the conditional probability of the measurement given the current state: $P(\\mathbf z_t \\mid \\mathbf x_x)$. We write:\n",
    "\n",
    "$$\\mathbf z_k \\sim P(\\mathbf z_t \\mid \\mathbf x_x)$$\n",
    "\n",
    "We have a recurrance now, so we need an initial condition to terminate it. Therefore we say that the initial distribution is the probablity of the state $\\mathbf x_0$:\n",
    "\n",
    "$$\\mathbf x_0 \\sim P(\\mathbf x_0)$$\n",
    "\n",
    "\n",
    "These terms are plugged into Bayes equation. If we have the state $\\mathbf x_0$ and the first measurement we can estimate $P(\\mathbf x_1 | \\mathbf z_1)$. The motion model creates the prior $P(\\mathbf x_2 \\mid \\mathbf x_1)$. We feed this back into Bayes theorem to compute $P(\\mathbf x_2 | \\mathbf z_2)$. We continue this predictor-corrector algorithm, recursively computing the state and distribution at time $t$ based solely on the state and distribution at time $t-1$ and the measurement at time $t$.\n",
    "\n",
    "The details of the mathematics for this computation varies based on the problem. The **Discrete Bayes** and **Univariate Kalman Filter** chapters gave two different formulations which you should have been able to reason through. The univariate Kalman filter assumes that for a scalar state both the noise and process are linear model are affected by zero-mean, uncorrelated Gaussian noise. \n",
    "\n",
    "The Multivariate Kalman filter make the same assumption but for states and measurements that are vectors, not scalars. Dr. Kalman was able to prove that if these assumptions hold true then the Kalman filter is *optimal*. Colloquially this means there is no way to derive more information from the noise. In the remainder of the book I will present filters that relax the constraints on linearity and Gaussian noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the Multivariate Equations to the Univariate Case\n",
    "\n",
    "The multivariate Kalman filter equations do not resemble the equations for the univariate filter. However, if we use one dimensional states and measurements the equations do reduce to the univariate equations. This section will provide you with a strong intuition into what the Kalman filter equations are actually doing. While reading this section is not required to understand the rest of the book, I recommend reading this section carefully as it should make the rest of the material easier to understand.\n",
    "\n",
    "Here are the multivariate equations for the prediction. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{\\bar{x}} &= \\mathbf{F x} + \\mathbf{B u} \\\\\n",
    "\\mathbf{\\bar{P}} &= \\mathbf{FPF}^\\mathsf{T} + \\mathbf Q\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For a univariate problem the state $\\mathbf x$ only has one variable, so it is a $1\\times 1$ matrix. Our motion $\\mathbf{u}$ is also a $1\\times 1$ matrix. Therefore, $\\mathbf{F}$ and $\\mathbf B$ must also be $1\\times 1$ matrices. That means that they are all scalars, and we can write\n",
    "\n",
    "$$\\bar{x} = Fx + Bu$$\n",
    "\n",
    "Here the variables are not bold, denoting that they are not matrices or vectors. \n",
    "\n",
    "Our state transition is simple - the next state is the same as this state, so $F=1$. The same holds for the motion transition, so, $B=1$. Thus we have\n",
    "\n",
    "$$x = x + u$$\n",
    "\n",
    "which is equivalent to the Gaussian equation from the last chapter\n",
    "\n",
    "$$ \\mu = \\mu_1+\\mu_2$$\n",
    "\n",
    "Hopefully the general process is clear, so now I will go a bit faster on the rest. We have\n",
    "\n",
    "$$\\mathbf{\\bar{P}} = \\mathbf{FPF}^\\mathsf{T} + \\mathbf Q$$\n",
    "\n",
    "Again, since our state only has one variable $\\mathbf P$ and $\\mathbf Q$ must also be $1\\times 1$ matrix, which we can treat as scalars, yielding  \n",
    "\n",
    "$$\\bar{P} = FPF^\\mathsf{T} + Q$$\n",
    "\n",
    "We already know $F=1$. The transpose of a scalar is the scalar, so $F^\\mathsf{T} = 1$. This yields\n",
    "\n",
    "$$\\bar{P} = P + Q$$\n",
    "\n",
    "which is equivalent to the Gaussian equation of \n",
    "\n",
    "$$\\sigma^2 = \\sigma_1^2 + \\sigma_2^2$$\n",
    "\n",
    "This proves that the multivariate prediction equations are performing the same math as the univariate equations for the case of the dimension being 1.\n",
    "\n",
    "These are the equations for the update step:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{K}&= \\mathbf{\\bar{P}H}^\\mathsf{T} (\\mathbf{H\\bar{P}H}^\\mathsf{T} + \\mathbf R)^{-1} \\\\\n",
    "\\textbf{y} &= \\mathbf z - \\mathbf{H \\bar{x}}\\\\\n",
    "\\mathbf x&=\\mathbf{\\bar{x}} +\\mathbf{K\\textbf{y}} \\\\\n",
    "\\mathbf P&= (\\mathbf{I}-\\mathbf{KH})\\mathbf{\\bar{P}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "As above, all of the matrices become scalars. $H$ defines how we convert from a position to a measurement. Both are positions, so there is no conversion, and thus $H=1$. Let's substitute in our known values and convert to scalar in one step. The inverse of a 1x1 matrix is the reciprocal of the value so we will convert the matrix inversion to division.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "K &=\\frac{\\bar{P}}{\\bar{P} + R} \\\\\n",
    "y &= z - \\bar{x}\\\\\n",
    "x &=\\bar{x}+Ky \\\\\n",
    "P &= (1-K)\\bar{P}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Before we continue with the proof, I want you to look at those equations to recognize what a simple concept these equations implement. The residual $y$ is nothing more than the measurement minus the prediction. The gain $K$ is scaled based on how certain we are about the last prediction vs how certain we are about the measurement. We choose a new state $x$ based on the old value of $x$ plus the scaled value of the residual. Finally, we update the uncertainty based on how certain we are about the measurement. Algorithmically this should sound exactly like what we did in the last chapter.\n",
    "\n",
    "Let's finish off the algebra to prove this. Recall that the univariate equations for the update step are:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu &=\\frac{\\sigma_1^2 \\mu_2 + \\sigma_2^2 \\mu_1} {\\sigma_1^2 + \\sigma_2^2}, \\\\\n",
    "\\sigma^2 &= \\frac{1}{\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here we will say that $\\mu_1$ is the state $x$, and $\\mu_2$ is the measurement $z$. Thus it follows that that $\\sigma_1^2$ is the state uncertainty $P$, and $\\sigma_2^2$ is the measurement noise $R$. Let's substitute those in.\n",
    "\n",
    "$$ \\mu = \\frac{Pz + Rx}{P+R} \\\\\n",
    "\\sigma^2 = \\frac{1}{\\frac{1}{P} + \\frac{1}{R}}\n",
    "$$\n",
    "\n",
    "I will handle $\\mu$ first. The corresponding equation in the multivariate case is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x &= x + Ky \\\\\n",
    "&= x + \\frac{P}{P+R}(z-x) \\\\\n",
    "&= \\frac{P+R}{P+R}x + \\frac{Pz - Px}{P+R} \\\\\n",
    "&= \\frac{Px + Rx + Pz - Px}{P+R} \\\\\n",
    "&= \\frac{Pz + Rx}{P+R}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now let's look at $\\sigma^2$. The corresponding equation in the multivariate case is\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "P &= (1-K)P \\\\\n",
    "&= (1-\\frac{P}{P+R})P \\\\\n",
    "&= (\\frac{P+R}{P+R}-\\frac{P}{P+R})P \\\\\n",
    "&= (\\frac{P+R-P}{P+R})P \\\\\n",
    "&= \\frac{RP}{P+R}\\\\\n",
    "&= \\frac{1}{\\frac{P+R}{RP}}\\\\\n",
    "&= \\frac{1}{\\frac{R}{RP} + \\frac{P}{RP}} \\\\\n",
    "&= \\frac{1}{\\frac{1}{P} + \\frac{1}{R}}\n",
    "\\quad\\blacksquare\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We have proven that the multivariate equations are equivalent to the univariate equations when we only have one state variable. I'll close this section by recognizing one quibble - I hand waved my assertion that $H=1$ and $F=1$. In general we know this is not true. For example, a digital thermometer may provide measurement in volts, and we need to convert that to temperature, and we use $H$ to do that conversion. I left that issue out to keep the explanation as simple and streamlined as possible. It is very straightforward to add that generalization to the equations above, redo the algebra, and still have the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Kalman Filter to a g-h Filter\n",
    "\n",
    "I've stated that the Kalman filter is a form of the g-h filter. It just takes some algebra to prove it. It's more straightforward to do with the one dimensional case, so I will do that. Recall \n",
    "\n",
    "$$\n",
    "\\mu_{x}=\\frac{\\sigma_1^2 \\mu_2 + \\sigma_2^2 \\mu_1} {\\sigma_1^2 + \\sigma_2^2}\n",
    "$$\n",
    "\n",
    "which I will make more friendly for our eyes as:\n",
    "\n",
    "$$\n",
    "\\mu_{x}=\\frac{ya + xb} {a+b}\n",
    "$$\n",
    "\n",
    "We can easily put this into the g-h form with the following algebra\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu_{x}&=(x-x) + \\frac{ya + xb} {a+b} \\\\\n",
    "\\mu_{x}&=x-\\frac{a+b}{a+b}x  + \\frac{ya + xb} {a+b} \\\\ \n",
    "\\mu_{x}&=x +\\frac{-x(a+b) + xb+ya}{a+b} \\\\\n",
    "\\mu_{x}&=x+ \\frac{-xa+ya}{a+b}  \\\\\n",
    "\\mu_{x}&=x+ \\frac{a}{a+b}(y-x)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We are almost done, but recall that the variance of estimate is given by \n",
    "\n",
    "$${\\sigma_{x}^2} = \\frac{1}{ \\frac{1}{\\sigma_1^2} +  \\frac{1}{\\sigma_2^2}}\\\\\n",
    "= \\frac{1}{ \\frac{1}{a} +  \\frac{1}{b}}\n",
    "$$\n",
    "\n",
    "We can incorporate that term into our equation above by observing that\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "\\frac{a}{a+b} &= \\frac{a/a}{(a+b)/a} = \\frac{1}{(a+b)/a}  \\\\\n",
    " &= \\frac{1}{1 + \\frac{b}{a}} = \\frac{1}{\\frac{b}{b} + \\frac{b}{a}}  \\\\\n",
    " &= \\frac{1}{b}\\frac{1}{\\frac{1}{b} + \\frac{1}{a}} \\\\\n",
    " &= \\frac{\\sigma^2_{x'}}{b}\n",
    " \\end{aligned}\n",
    "$$\n",
    "\n",
    "We can tie all of this together with\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu_{x}&=x+ \\frac{a}{a+b}(y-x) \\\\\n",
    "&= x + \\frac{\\sigma^2_{x'}}{b}(y-x) \\\\\n",
    "&= x + g_n(y-x)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$g_n = \\frac{\\sigma^2_{x}}{\\sigma^2_{y}}$$\n",
    "\n",
    "The end result is multiplying the residual of the two measurements by a constant and adding to our previous value, which is the $g$ equation for the g-h filter. $g$ is the variance of the new estimate divided by the variance of the measurement. Of course in this case $g$ is not a constant as it varies with each time step as the variance changes. We can also derive the formula for $h$ in the same way. It is not a particularly illuminating derivation and I will skip it. The end result is\n",
    "\n",
    "$$h_n = \\frac{COV (x,\\dot x)}{\\sigma^2_{y}}$$\n",
    "\n",
    "The takeaway point is that $g$ and $h$ are specified fully by the variance and covariances of the measurement and predictions at time $n$. In other words, we are picking a point between the measurement and prediction by a scale factor determined by the quality of each of those two inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Least Squares for Sensor Fusion (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A broad category of use for the Kalman filter is *sensor fusion*. For example, we might have a position sensor and a velocity sensor, and we want to combine the data from both to find an optimal estimate of state. In this section we will discuss a different case, where we have multiple sensors providing the same type of measurement. \n",
    "\n",
    " The Global Positioning System (GPS) is designed so that at least 6 satellites are in view at any time at any point on the globe. The GPS receiver knows the location of the satellites in the sky relative to the Earth. At each epoch (instant in time) the receiver gets a signal from each satellite from which it can derive the *pseudorange* to the satellite. In more detail, the GPS receiver gets a signal identifying the satellite along with the time stamp of when the signal was transmitted. The GPS satellite has an atomic clock on board so this time stamp is extremely accurate. The signal travels at the speed of light, which is constant in a vacuum, so in theory the GPS should be able to produce an extremely accurate distance measurement to the measurement by measuring how long the signal took to reach the receiver. There are several problems with that. First, the signal is not traveling through a vacuum, but through the atmosphere. The atmosphere causes the signal to bend, so it is not traveling in a straight line. This causes the signal to take longer to reach the receiver than theory suggests. Second, the on board clock on the GPS *receiver* is not very accurate, so deriving an exact time duration is nontrivial. Third, in many environments the signal can bounce off of buildings, trees, and other objects, causing either a longer path or *multipaths*, in which case the receive receives both the original signal from space and the reflected signals. \n",
    "\n",
    "Let's look at this graphically. I will do this in 2D to make it easier to graph and see, but of course this will generalize to three dimensions. We know the position of each satellite and the range to each (the range is called the *pseudorange*; we will discuss why later). We cannot measure the range exactly, so there is noise associated with the measurement, which I have depicted with the thickness of the lines. Here is an example of four pseudorange readings from four satellites. I positioned them in a configuration which is unlikely for the actual GPS constellation merely to make the intersections easy to visualize. Also, the amount of error shown is not to scale with the distances, again to make it easier to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from book_format import set_figsize, figsize\n",
    "\n",
    "import ukf_internal\n",
    "with figsize(10, 6):\n",
    "    ukf_internal.show_four_gps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2D two measurements are sometimes enough to determine a unique solution. There are two intersections of the range circles, but often the second intersection is not physically realizable (it is in space, or under ground). However, with GPS we also need to solve for time, so we would need a third measurement to get a 2D position.\n",
    "\n",
    "However, since GPS is a 3D system we need to solve for the 3 dimensions of space, and 1 dimension of time. That is 4 unknowns, so in theory with 4 satellites we have all the information we need. However, we normally have at least 6 satellites in view, and often more than 6. This means the system is *overdetermined*. Finally, because of the noise in the measurements none of pseudoranges intersect exactly.\n",
    "\n",
    "If you are well versed in linear algebra you know that this an extremely common problem in scientific computing, and that there are various techniques for solving overdetermined systems. Probably the most common approach used by GPS receivers to find the position is the *iterative least squares* algorithm, commonly abbreviated ILS. As you know, if the errors are Gaussian then the least squares algorithm finds the optimal solution. In other words, we want to minimize the square of the residuals for an overdetermined system.\n",
    "\n",
    "Let's start with some definitions which should be familiar to you. First, we define the innovation as \n",
    "\n",
    "$$\\delta \\mathbf{\\bar{z}}= \\mathbf z - h(\\mathbf{\\bar{x}})$$\n",
    "\n",
    "where $\\mathbf z$ is the measurement, $h(\\bullet)$ is the measurement function, and $\\delta \\mathbf{\\bar{z}}$ is the innovation, which we abbreviate as $y$ in FilterPy. In other words, this is the equation $\\mathbf{y} = \\mathbf z - \\mathbf{H\\bar{x}}$ in the linear Kalman filter's update step.\n",
    "\n",
    "Next, the *measurement residual* is\n",
    "\n",
    "$$\\delta \\mathbf z^+ = \\mathbf z - h(\\mathbf x^+)$$\n",
    "\n",
    "I don't use the plus superscript much because I find it quickly makes the equations unreadable, but $\\mathbf x^+$ is the *a posteriori* state estimate, which is the predicted or unknown future state. In other words, the predict step of the linear Kalman filter computes this value. Here it is stands for the value of x which the ILS algorithm will compute on each iteration.\n",
    "\n",
    "These equations give us the following linear algebra equation:\n",
    "\n",
    "$$\\delta \\mathbf z^- = \\mathbf H\\delta \\mathbf x + \\delta \\mathbf z^+$$\n",
    "\n",
    "$\\mathbf H$ is our measurement function, defined as\n",
    "\n",
    "$$\\mathbf H = \\frac{d\\mathbf H}{d\\mathbf x} = \\frac{d\\mathbf z}{d\\mathbf x}$$\n",
    "\n",
    "We find the minimum of an equation by taking the derivative and setting it to zero. In this case we want to minimize the square of the residuals, so our equation is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial}{\\partial \\mathbf x}({\\delta \\mathbf z^+}^\\mathsf{T}\\delta \\mathbf z^+) = 0,$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\delta \\mathbf z^+=\\delta \\mathbf z^- - \\mathbf H\\delta \\mathbf x.$$\n",
    "\n",
    "Here I have switched to using the matrix $\\mathbf H$ as the measurement function. We want to use linear algebra to peform the ILS, so for each step we will have to compute the matrix $\\mathbf H$ which corresponds to $h(\\mathbf{x^-})$ during each iteration.  $h(\\bullet)$ is usually nonlinear for these types of problems so you will have to linearize it at each step (more about this soon).\n",
    "\n",
    "For various reasons you may want to weigh some measurement more than others. For example, the geometry of the problem might favor orthogonal measurements, or some measurements may be more noisy than others. We can do that with the equation\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\mathbf x}({\\delta \\mathbf z^+}^\\mathsf{T}\\mathbf{W}\\delta \\mathbf z^+) = 0$$\n",
    "\n",
    "If we solve the first equation for ${\\delta \\mathbf x}$ (the derivation is shown in the next section) we get\n",
    "\n",
    "$${\\delta \\mathbf x} = {{(\\mathbf H^\\mathsf{T}\\mathbf H)^{-1}}\\mathbf H^\\mathsf{T} \\delta \\mathbf z^-}\n",
    "$$\n",
    "\n",
    "And the second equation yields\n",
    "\n",
    "$${\\delta \\mathbf x} = {{(\\mathbf H^\\mathsf{T}\\mathbf{WH})^{-1}}\\mathbf H^\\mathsf{T}\\mathbf{W} \\delta \\mathbf z^-}\n",
    "$$\n",
    "\n",
    "Since the equations are overdetermined we cannot solve these equations exactly so we use an iterative approach. An initial guess for the position is made, and this guess is used to compute  for $\\delta \\mathbf x$ via the equation above. $\\delta \\mathbf x$ is added to the intial guess, and this new state is fed back into the equation to produce another $\\delta \\mathbf x$. We iterate in this manner until the difference in the measurement residuals is suitably small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation of ILS Equations (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will implement the ILS in code, but first let's derive the equation for $\\delta \\mathbf x$. You can skip the derivation if you want, but it is somewhat instructive and not too hard if you know basic linear algebra and partial differential equations.\n",
    "\n",
    "Substituting $\\delta \\mathbf z^+=\\delta \\mathbf z^- - \\mathbf H\\delta \\mathbf x$ into the partial differential equation we get\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\mathbf x}(\\delta \\mathbf z^- -\\mathbf H \\delta \\mathbf x)^\\mathsf{T}(\\delta \\mathbf z^- - \\mathbf H \\delta \\mathbf x)=0$$\n",
    "\n",
    "which expands to\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\mathbf x}({\\delta \\mathbf x}^\\mathsf{T}\\mathbf H^\\mathsf{T}\\mathbf H\\delta \\mathbf x - \n",
    "{\\delta \\mathbf x}^\\mathsf{T}\\mathbf H^\\mathsf{T}\\delta \\mathbf z^- - \n",
    "{\\delta \\mathbf z^-}^\\mathsf{T}\\mathbf H\\delta \\mathbf x +\n",
    "{\\delta \\mathbf z^-}^\\mathsf{T}\\delta \\mathbf z^-)=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that \n",
    "\n",
    "$$\\frac{\\partial \\mathbf{A}^\\mathsf{T}\\mathbf B}{\\partial \\mathbf B} = \\frac{\\partial \\mathbf B^\\mathsf{T}\\mathbf{A}}{\\partial \\mathbf B} = \\mathbf{A}^\\mathsf{T}$$\n",
    "\n",
    "Therefore the third term can be computed as\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mathbf x}{\\delta \\mathbf z^-}^\\mathsf{T}\\mathbf H\\delta \\mathbf x = {\\delta \\mathbf z^-}^\\mathsf{T}\\mathbf H$$\n",
    "\n",
    "and the second term as\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mathbf x}{\\delta \\mathbf x}^\\mathsf{T}\\mathbf H^\\mathsf{T}\\delta \\mathbf z^-={\\delta \\mathbf z^-}^\\mathsf{T}\\mathbf H$$\n",
    "\n",
    "We also know that\n",
    "$$\\frac{\\partial \\mathbf B^\\mathsf{T}\\mathbf{AB}}{\\partial \\mathbf B} = \\mathbf B^\\mathsf{T}(\\mathbf{A} + \\mathbf{A}^\\mathsf{T})$$\n",
    "\n",
    "Therefore the first term becomes\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\mathbf x}{\\delta \\mathbf x}^\\mathsf{T}\\mathbf H^\\mathsf{T}\\mathbf H\\delta \\mathbf x &= {\\delta \\mathbf x}^\\mathsf{T}(\\mathbf H^\\mathsf{T}\\mathbf H + {\\mathbf H^\\mathsf{T}\\mathbf H}^\\mathsf{T})\\\\\n",
    "&= {\\delta \\mathbf x}^\\mathsf{T}(\\mathbf H^\\mathsf{T}\\mathbf H + \\mathbf H^\\mathsf{T}\\mathbf H) \\\\\n",
    "&= 2{\\delta \\mathbf x}^\\mathsf{T}\\mathbf H^\\mathsf{T}\\mathbf H\n",
    "\\end{aligned}$$\n",
    "\n",
    "Finally, the fourth term is\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\mathbf x}\n",
    "{\\delta \\mathbf z^-}^\\mathsf{T}\\delta \\mathbf z^-=0$$\n",
    "\n",
    "Replacing the terms in the expanded partial differential equation gives us\n",
    "\n",
    "$$\n",
    " 2{\\delta \\mathbf x}^\\mathsf{T}\\mathbf H^\\mathsf{T}\\mathbf H -\n",
    " {\\delta \\mathbf z^-}^\\mathsf{T}\\mathbf H - {\\delta \\mathbf z^-}^\\mathsf{T}\\mathbf H\n",
    " =0\n",
    "$$\n",
    "\n",
    "$${\\delta \\mathbf x}^\\mathsf{T}\\mathbf H^\\mathsf{T}\\mathbf H -\n",
    " {\\delta \\mathbf z^-}^\\mathsf{T}\\mathbf H = 0$$\n",
    " \n",
    "$${\\delta \\mathbf x}^\\mathsf{T}\\mathbf H^\\mathsf{T}\\mathbf H =\n",
    " {\\delta \\mathbf z^-}^\\mathsf{T}\\mathbf H$$\n",
    "\n",
    "Multiplying each side by $(\\mathbf H^\\mathsf{T}\\mathbf H)^{-1}$ yields\n",
    "\n",
    "$${\\delta \\mathbf x}^\\mathsf{T} =\n",
    "{\\delta \\mathbf z^-}^\\mathsf{T}\\mathbf H(\\mathbf H^\\mathsf{T}\\mathbf H)^{-1}$$\n",
    "\n",
    "Taking the transpose of each side gives\n",
    "\n",
    "$${\\delta \\mathbf x} = ({{\\delta \\mathbf z^-}^\\mathsf{T}\\mathbf H(\\mathbf H^\\mathsf{T}\\mathbf H)^{-1}})^\\mathsf{T} \\\\\n",
    "={{(\\mathbf H^\\mathsf{T}\\mathbf H)^{-1}}^T\\mathbf H^\\mathsf{T} \\delta \\mathbf z^-} \\\\\n",
    "={{(\\mathbf H^\\mathsf{T}\\mathbf H)^{-1}}\\mathbf H^\\mathsf{T} \\delta \\mathbf z^-}\n",
    "$$\n",
    "\n",
    "For various reasons you may want to weigh some measurement more than others. We can do that with the equation\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\mathbf x}({\\delta \\mathbf z}^\\mathsf{T}\\mathbf{W}\\delta \\mathbf z) = 0$$\n",
    "\n",
    "Replicating the math above with the added $\\mathbf{W}$ term results in\n",
    "\n",
    "$${\\delta \\mathbf x} = {{(\\mathbf H^\\mathsf{T}\\mathbf{WH})^{-1}}\\mathbf H^\\mathsf{T}\\mathbf{W} \\delta \\mathbf z^-}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Iterative Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to implement an iterative solution to \n",
    "$${\\delta \\mathbf x} = {{(\\mathbf H^\\mathsf{T}\\mathbf H)^{-1}}\\mathbf H^\\mathsf{T} \\delta \\mathbf z^-}\n",
    "$$\n",
    "\n",
    "First, we have to compute $\\mathbf H$, where $\\mathbf H =  d\\mathbf z/d\\mathbf x$. To keep the example small so the results are easier to interpret we will do this in 2D. Therefore for $n$ satellites $\\mathbf H$ expands to\n",
    "\n",
    "$$\\mathbf H = \\begin{bmatrix}\n",
    "\\frac{\\partial p_1}{\\partial x_1} & \\frac{\\partial p_1}{\\partial y_1} \\\\\n",
    "\\frac{\\partial p_2}{\\partial x_2} & \\frac{\\partial p_2}{\\partial y_2} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "\\frac{\\partial p_n}{\\partial x_n} & \\frac{\\partial p_n}{\\partial y_n}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "We will linearize $\\mathbf H$ by computing the partial for $x$ as\n",
    "\n",
    "$$ \\frac{estimated\\_x\\_position - satellite\\_x\\_position}{estimated\\_range\\_to\\_satellite}$$\n",
    "\n",
    "The equation for $y$ just substitutes $y$ for $x$.\n",
    "\n",
    "Then the algorithm is as follows.\n",
    "\n",
    "    def ILS:\n",
    "        guess position\n",
    "        while not converged:\n",
    "            compute range to satellites for current estimated position\n",
    "            compute H linearized at estimated position\n",
    "            compute new estimate delta from (H^T H)'H^T dz\n",
    "            new estimate = current estimate + estimate delta\n",
    "            check for convergence\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm, inv\n",
    "from numpy.random import randn\n",
    "from numpy import dot\n",
    "import book_format\n",
    "\n",
    "np.random.seed(1234)\n",
    "user_pos = np.array([800, 200])\n",
    "\n",
    "\n",
    "sat_pos = np.asarray(\n",
    "    [[0, 1000],\n",
    "     [0, -1000],\n",
    "     [500, 500]], dtype=float)\n",
    "\n",
    "def satellite_range(pos, sat_pos):\n",
    "    \"\"\" Compute distance between position 'pos' and \n",
    "    the list of positions in sat_pos\"\"\"\n",
    "\n",
    "    N = len(sat_pos)\n",
    "    rng = np.zeros(N)\n",
    "\n",
    "    diff = np.asarray(pos) - sat_pos\n",
    "\n",
    "    for i in range(N):\n",
    "        rng[i] = norm(diff[i])\n",
    "\n",
    "    return norm(diff, axis=1)\n",
    "\n",
    "\n",
    "def hx_ils(pos, sat_pos, range_est):\n",
    "    \"\"\" compute measurement function where\n",
    "    pos : array_like \n",
    "        2D current estimated position. e.g. (23, 45)\n",
    "        \n",
    "    sat_pos : array_like of 2D positions\n",
    "        position of each satellite e.g. [(0,100), (100,0)]\n",
    "        \n",
    "    range_est : array_like of floats\n",
    "        range to each satellite\n",
    "    \"\"\"\n",
    "    \n",
    "    N = len(sat_pos)\n",
    "    H = np.zeros((N, 2))\n",
    "    for j in range(N):\n",
    "        H[j, 0] = (pos[0] - sat_pos[j, 0]) / range_est[j]\n",
    "        H[j, 1] = (pos[1] - sat_pos[j, 1]) / range_est[j]\n",
    "    return H\n",
    "\n",
    "\n",
    "def lop_ils(zs, sat_pos, pos_est, hx, eps=1.e-6):\n",
    "    \"\"\" iteratively estimates the solution to a set of\n",
    "    measurement, given known transmitter locations\"\"\"\n",
    "    pos = np.array(pos_est)\n",
    "\n",
    "    with book_format.numpy_precision(precision=4):\n",
    "        converged = False\n",
    "        for i in range(20):\n",
    "            r_est = satellite_range(pos, sat_pos)\n",
    "            print('iteration:', i+1)\n",
    "\n",
    "            H = hx(pos, sat_pos, r_est)        \n",
    "            Hinv = inv(dot(H.T, H)).dot(H.T)\n",
    "\n",
    "            # update position estimate\n",
    "            y = zs - r_est\n",
    "            print('innovation', y)\n",
    "\n",
    "            Hy = np.dot(Hinv, y)\n",
    "            pos = pos + Hy\n",
    "            print('pos       {}\\n\\n'.format(pos))\n",
    "\n",
    "            if max(abs(Hy)) < eps:\n",
    "                converged = True\n",
    "                break\n",
    "\n",
    "    return pos, converged\n",
    "\n",
    "# compute range to each sensor\n",
    "rz = satellite_range(user_pos, sat_pos)\n",
    "\n",
    "pos, converted = lop_ils(rz, sat_pos, (900, 90), hx=hx_ils)\n",
    "print('Iterated solution: ', pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's think about this. The first iteration is essentially performing the computation that the linear Kalman filter computes during the update step:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf z - \\mathbf{Hx}\\\\\n",
    "\\mathbf x = \\mathbf x + \\mathbf{Ky}$$\n",
    "\n",
    "where the Kalman gain equals one. You can see that despite the very inaccurate initial guess (900, 90) the computed value for $\\mathbf x$, (805.4, 205.3), was very close to the actual value of (800, 200). However, it was not perfect. But after three iterations the ILS algorithm was able to find the exact answer. So hopefully it is clear why we use ILS instead of doing the sensor fusion with the Kalman filter - it gives a better result. Of course, we started with a very inaccurate guess; what if the guess was better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos, converted = lop_ils(rz, sat_pos, (801, 201), hx=hx_ils)\n",
    "print('Iterated solution: ', pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first iteration produced a better estimate, but it still could be improved upon by iterating.\n",
    "\n",
    "I injected no noise in the measurement to test and display the theoretical performance of the filter. Now let's see how it performs when we inject noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add some noise\n",
    "nrz = []\n",
    "for z in rz:\n",
    "    nrz.append(z + randn())\n",
    "pos, converted = lop_ils(nrz, sat_pos, (601,198.3), hx=hx_ils)\n",
    "print('Iterated solution: ', pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the noise means that we no longer find the exact solution but we are still able to quickly converge onto a more accurate solution than the first iteration provides.\n",
    "\n",
    "This is far from a complete coverage of the iterated least squares algorithm, let alone methods used in GNSS to compute positions from GPS pseudoranges. You will find a number of approaches in the literature, including QR decomposition, SVD, and other techniques to solve the overdetermined system. For a nontrivial task you will have to survey the literature and perhaps design your algorithm depending on your specific sensor configuration, the amounts of noise, your accuracy requirements, and the amount of computation you can afford to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * [1] *Matrix Exponential* http://en.wikipedia.org/wiki/Matrix_exponential \n",
    "\n",
    " * [2] *LTI System Theory* http://en.wikipedia.org/wiki/LTI_system_theory\n",
    " \n",
    " * [3] C.F. van Loan, \"Computing Integrals Involving the Matrix Exponential,\" IEEE Transactions Automatic Control, June 1978."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
